[
  {
    "content": "ACM/ASM Workshop Welcome to the ACM/ASM Workshop!\nNote It’s not an official Google’s workshop.\nThis ACM/ASM Workshop allows to illustrate an advanced and secure setup of a platform generated by Kubernetes resources, via GitOps.\nThis workshop leverages 3 main services:\nGoogle Kubernertes Engine (GKE) Anthos Config Management (ACM): Config Sync, Config Controller, Policy Controller and Config Connector Anthos Service Mesh (ASM) It’s a step-by-step guided hands-on lab.\nAfter the workshop, you will be able to:\nBetter understand the different services included in ACM: Config Sync, Config Controller, Policy Controller and Config Connector Secure a GKE cluster Get experience with GitOps flow to deploy Kubernetes manifests Deploy Infrastructure, Configs and Applications via Kubernetes manifests, via GitOps Define clear roles and responsabilities between Org Admin, Platform Admin and Apps Operator Set up a Managed ASM on GKE with a secure Ingress Gateway behind a HTTPS GCLB and Cloud Armor Deploy sample apps such as Whereami, Online Boutique and Bank of Anthos with security best practices including Pod Security Admission (PSA), NetworkPolicies, Sidecars and AuthorizationPolicies. Use external managed databases such as Memorystore (Redis) and Spanner for Online Boutique With this workshop, here is what you will accomplish, from scratch:\nRecently tested with:\nGKE 1.25.5-gke.1500 ASM MCP 1.15.4-asm.2 + MDP 1.15.4-asm.2 ACM 1.14.1 Whereami 1.2.14 Online Boutique 0.5.0 Bank of Anthos 0.5.10 ",
    "description": "",
    "tags": null,
    "title": "ACM/ASM Workshop",
    "uri": "/acm-workshop/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Org Admin\nIn this section, you will enable and grant the appropriate APIs in the Tenant project and the IAM role for the Tenant project’s service account. This will allow later this service account to provision the Artifact Registry to have your private container images. You will also the containers analysis and scanning features of Artifact Registry.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Define role Define the artifactregistry.admin role with an IAMPolicyMember for the Tenant project’s service account:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/artifactregistry-admin.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: artifactregistry-admin-${TENANT_PROJECT_ID} namespace: config-control annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/config-control/IAMServiceAccount/${TENANT_PROJECT_ID},resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} spec: memberFrom: serviceAccountRef: name: ${TENANT_PROJECT_ID} role: roles/artifactregistry.admin resourceRef: kind: Project external: projects/${TENANT_PROJECT_ID} EOF Define APIs Define the Artifact Registry API Service resource for the Tenant project:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/artifactregistry-service.yaml apiVersion: serviceusage.cnrm.cloud.google.com/v1beta1 kind: Service metadata: annotations: cnrm.cloud.google.com/deletion-policy: \"abandon\" cnrm.cloud.google.com/disable-dependent-services: \"false\" config.kubernetes.io/depends-on: resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} name: ${TENANT_PROJECT_ID}-artifactregistry namespace: config-control spec: projectRef: name: ${TENANT_PROJECT_ID} resourceID: artifactregistry.googleapis.com EOF Info We are enabling the GCP services APIs from the Org Admin, it allows more control and governance over which GCP services APIs the Platform Admin could use or not. If you want to give more autonomy to the Platform Admin, you could grant the serviceusage.serviceUsageAdmin role to the associated service account.\nDefine the Container scanning APIs Service resource for the Tenant project:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/containeranalysis-service.yaml apiVersion: serviceusage.cnrm.cloud.google.com/v1beta1 kind: Service metadata: annotations: cnrm.cloud.google.com/deletion-policy: \"abandon\" cnrm.cloud.google.com/disable-dependent-services: \"false\" config.kubernetes.io/depends-on: resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} name: ${TENANT_PROJECT_ID}-containeranalysis namespace: config-control spec: projectRef: name: ${TENANT_PROJECT_ID} resourceID: containeranalysis.googleapis.com EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/containerscanning-service.yaml apiVersion: serviceusage.cnrm.cloud.google.com/v1beta1 kind: Service metadata: annotations: cnrm.cloud.google.com/deletion-policy: \"abandon\" cnrm.cloud.google.com/disable-dependent-services: \"false\" config.kubernetes.io/depends-on: resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} name: ${TENANT_PROJECT_ID}-containerscanning namespace: config-control spec: projectRef: name: ${TENANT_PROJECT_ID} resourceID: containerscanning.googleapis.com EOF Info Container Analysis performs vulnerability scans on container images in Artifact Registry and Container Registry, and it monitors the vulnerability information to keep it up to date. This process comprises two main tasks: scanning and continuous analysis.\nDefine the On-demand scanning APIs Service resource for the Tenant project:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/ondemandscanning-service.yaml apiVersion: serviceusage.cnrm.cloud.google.com/v1beta1 kind: Service metadata: annotations: cnrm.cloud.google.com/deletion-policy: \"abandon\" cnrm.cloud.google.com/disable-dependent-services: \"false\" config.kubernetes.io/depends-on: resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} name: ${TENANT_PROJECT_ID}-ondemandscanning namespace: config-control spec: projectRef: name: ${TENANT_PROJECT_ID} resourceID: ondemandscanning.googleapis.com EOF Info On-demand scanning lets you scan container images locally on your computer or in your registry, using the gcloud CLI. This gives you the flexibility to customize your CI/CD pipeline, depending on when you need to access the vulnerability results. In this workshop, you will try out this feature with its associated gcloud command.\nDeploy Kubernetes manifests cd ${WORK_DIR}$HOST_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Allow Artifact Registry for Tenant project\" \u0026\u0026 git push origin main Check deployments graph TD; IAMPolicyMember-.-\u003eIAMServiceAccount IAMPolicyMember-.-\u003eProject Service-.-\u003eProject Service-.-\u003eProject Service-.-\u003eProject Service-.-\u003eProject List the Kubernetes resources managed by Config Sync in Config Controller for the Host project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Host project configs repository:\ncd ${WORK_DIR}$HOST_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud services list \\ --enabled \\ --project ${TENANT_PROJECT_ID} \\ | grep -E 'containerscanning|containeranalysis|artifactregistry|ondemandscanning' gcloud projects get-iam-policy $TENANT_PROJECT_ID \\ --filter=\"bindings.members:${TENANT_PROJECT_SA_EMAIL}\" \\ --flatten=\"bindings[].members\" \\ --format=\"table(bindings.role)\" \\ | grep artifactregistry Wait and re-run this command above until you see the resources created.\n",
    "description": "Duration: 5 min | Persona: Org Admin",
    "tags": [
      "kcc",
      "org-admin",
      "security-tips"
    ],
    "title": "Allow Artifact Registry",
    "uri": "/acm-workshop/artifact-registry/allow-artifact-registry/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Org Admin\nIn this section, you will enable and grant the appropriate APIs in the Tenant project and the IAM role for the Tenant project’s service account to allow later this service account configure a Service Mesh for your GKE cluster. You will also enable the Anthos API in order to leverage the Service Mesh feature from within the Google Cloud console.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Define APIs Define the Mesh API Service resource in the Tenant project:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/mesh-service.yaml apiVersion: serviceusage.cnrm.cloud.google.com/v1beta1 kind: Service metadata: annotations: cnrm.cloud.google.com/deletion-policy: \"abandon\" cnrm.cloud.google.com/disable-dependent-services: \"false\" config.kubernetes.io/depends-on: resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} name: ${TENANT_PROJECT_ID}-mesh namespace: config-control spec: projectRef: name: ${TENANT_PROJECT_ID} resourceID: mesh.googleapis.com EOF Define the Cloud Trace API Service resource in the Tenant project:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/cloudtrace-service.yaml apiVersion: serviceusage.cnrm.cloud.google.com/v1beta1 kind: Service metadata: annotations: cnrm.cloud.google.com/deletion-policy: \"abandon\" cnrm.cloud.google.com/disable-dependent-services: \"false\" config.kubernetes.io/depends-on: resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} name: ${TENANT_PROJECT_ID}-cloudtrace namespace: config-control spec: projectRef: name: ${TENANT_PROJECT_ID} resourceID: cloudtrace.googleapis.com EOF Deploy Kubernetes manifests cd ${WORK_DIR}$HOST_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Allow ASM for Tenant project\" \u0026\u0026 git push origin main Check deployments graph TD; Service-.-\u003eProject Service-.-\u003eProject List the Kubernetes resources managed by Config Sync in Config Controller for the Host project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Host project configs repository:\ncd ${WORK_DIR}$HOST_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud services list \\ --enabled \\ --project ${TENANT_PROJECT_ID} \\ | grep -E 'mesh|cloudtrace' Wait and re-run this command above until you see the resources created.\n",
    "description": "Duration: 5 min | Persona: Org Admin",
    "tags": [
      "asm",
      "kcc",
      "org-admin"
    ],
    "title": "Allow ASM",
    "uri": "/acm-workshop/service-mesh/allow-asm/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Org Admin\nIn this section, you will enable and grant the appropriate APIs in the Tenant project and the IAM role for the Tenant project’s service account. This will allow later this service account to provision the GKE cluster.\nDefine variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Define roles Define the container.admin, iam.serviceAccountAdmin, resourcemanager.projectIamAdmin, iam.serviceAccountUser and serviceusage.serviceUsageConsumer roles with an IAMPolicyMember resource for the Tenant project’s service account:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/container-admin.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: container-admin-${TENANT_PROJECT_ID} namespace: config-control annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/config-control/IAMServiceAccount/${TENANT_PROJECT_ID},resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} spec: memberFrom: serviceAccountRef: name: ${TENANT_PROJECT_ID} role: roles/container.admin resourceRef: kind: Project external: projects/${TENANT_PROJECT_ID} EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/service-account-admin.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: service-account-admin-${TENANT_PROJECT_ID} namespace: config-control annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/config-control/IAMServiceAccount/${TENANT_PROJECT_ID},resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} spec: memberFrom: serviceAccountRef: name: ${TENANT_PROJECT_ID} role: roles/iam.serviceAccountAdmin resourceRef: kind: Project external: projects/${TENANT_PROJECT_ID} EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/iam-admin.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: iam-admin-${TENANT_PROJECT_ID} namespace: config-control annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/config-control/IAMServiceAccount/${TENANT_PROJECT_ID},resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} spec: memberFrom: serviceAccountRef: name: ${TENANT_PROJECT_ID} role: roles/resourcemanager.projectIamAdmin resourceRef: kind: Project external: projects/${TENANT_PROJECT_ID} EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/service-account-user.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: service-account-user-${TENANT_PROJECT_ID} namespace: config-control annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/config-control/IAMServiceAccount/${TENANT_PROJECT_ID},resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} spec: memberFrom: serviceAccountRef: name: ${TENANT_PROJECT_ID} role: roles/iam.serviceAccountUser resourceRef: kind: Project external: projects/${TENANT_PROJECT_ID} EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/serviceusage-consumer.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: serviceusage-consumer-${TENANT_PROJECT_ID} namespace: config-control annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/config-control/IAMServiceAccount/${TENANT_PROJECT_ID},resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} spec: memberFrom: serviceAccountRef: name: ${TENANT_PROJECT_ID} role: roles/serviceusage.serviceUsageConsumer resourceRef: kind: Project external: projects/${TENANT_PROJECT_ID} EOF Define APIs Define the Cloud Resources Manager API Service resource for the Tenant project:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/cloudresourcemanager-service.yaml apiVersion: serviceusage.cnrm.cloud.google.com/v1beta1 kind: Service metadata: annotations: cnrm.cloud.google.com/deletion-policy: \"abandon\" cnrm.cloud.google.com/disable-dependent-services: \"false\" config.kubernetes.io/depends-on: resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} name: ${TENANT_PROJECT_ID}-cloudresourcemanager namespace: config-control spec: projectRef: name: ${TENANT_PROJECT_ID} resourceID: cloudresourcemanager.googleapis.com EOF Define the GKE API Service resource for the Tenant project:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/container-service.yaml apiVersion: serviceusage.cnrm.cloud.google.com/v1beta1 kind: Service metadata: annotations: cnrm.cloud.google.com/deletion-policy: \"abandon\" cnrm.cloud.google.com/disable-dependent-services: \"false\" config.kubernetes.io/depends-on: resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} name: ${TENANT_PROJECT_ID}-container namespace: config-control spec: projectRef: name: ${TENANT_PROJECT_ID} resourceID: container.googleapis.com EOF Define the DNS API Service resource for the Tenant project in order to leverage the Cloud DNS feature of GKE:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/dns-service.yaml apiVersion: serviceusage.cnrm.cloud.google.com/v1beta1 kind: Service metadata: annotations: cnrm.cloud.google.com/deletion-policy: \"abandon\" cnrm.cloud.google.com/disable-dependent-services: \"false\" config.kubernetes.io/depends-on: resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} name: ${TENANT_PROJECT_ID}-dns namespace: config-control spec: projectRef: name: ${TENANT_PROJECT_ID} resourceID: dns.googleapis.com EOF Later in the workshop, you can optionally leverage the GKE Security Posture features (Scan workloads configurations and Scan container images for known vulnerabilities) in order to scan the configurations of your GKE workloads. Define the GKE Security Posture API Service resource for the Tenant project:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/containersecurity-service.yaml apiVersion: serviceusage.cnrm.cloud.google.com/v1beta1 kind: Service metadata: annotations: cnrm.cloud.google.com/deletion-policy: \"abandon\" cnrm.cloud.google.com/disable-dependent-services: \"false\" config.kubernetes.io/depends-on: resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} name: ${TENANT_PROJECT_ID}-containersecurity namespace: config-control spec: projectRef: name: ${TENANT_PROJECT_ID} resourceID: containersecurity.googleapis.com EOF Deploy Kubernetes manifests cd ${WORK_DIR}$HOST_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Allow GKE and GKE Security Posture for Tenant project\" \u0026\u0026 git push origin main Check deployments graph TD; IAMPolicyMember-.-\u003eIAMServiceAccount IAMPolicyMember-.-\u003eProject IAMPolicyMember-.-\u003eIAMServiceAccount IAMPolicyMember-.-\u003eProject IAMPolicyMember-.-\u003eIAMServiceAccount IAMPolicyMember-.-\u003eProject IAMPolicyMember-.-\u003eIAMServiceAccount IAMPolicyMember-.-\u003eProject IAMPolicyMember-.-\u003eIAMServiceAccount IAMPolicyMember-.-\u003eProject Service-.-\u003eProject Service-.-\u003eProject Service-.-\u003eProject List the Kubernetes resources managed by Config Sync in Config Controller for the Host project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Host project configs repository:\ncd ${WORK_DIR}$HOST_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud services list \\ --enabled \\ --project ${TENANT_PROJECT_ID} \\ | grep -E 'container' gcloud projects get-iam-policy $TENANT_PROJECT_ID \\ --filter=\"bindings.members:${TENANT_PROJECT_SA_EMAIL}\" \\ --flatten=\"bindings[].members\" \\ --format=\"table(bindings.role)\" Wait and re-run this command above until you see the resources created.\n",
    "description": "Duration: 5 min | Persona: Org Admin",
    "tags": [
      "gke",
      "kcc",
      "org-admin"
    ],
    "title": "Allow GKE",
    "uri": "/acm-workshop/gke-cluster/allow-gke/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Org Admin\nIn this section, you will enable and grant the appropriate APIs in the Tenant project and the IAM role for the Tenant project’s service account. This will allow later this service account to provision Memorystore (Redis) instances.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Define API Define the Memorystore (Redis) API Service resource for the Tenant project:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/redis-service.yaml apiVersion: serviceusage.cnrm.cloud.google.com/v1beta1 kind: Service metadata: annotations: cnrm.cloud.google.com/deletion-policy: \"abandon\" cnrm.cloud.google.com/disable-dependent-services: \"false\" config.kubernetes.io/depends-on: resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} name: ${TENANT_PROJECT_ID}-redis namespace: config-control spec: projectRef: name: ${TENANT_PROJECT_ID} resourceID: redis.googleapis.com EOF Define role Define the redis.admin role with an IAMPolicyMember for the Tenant project’s service account:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/redis-admin.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: redis-admin-${TENANT_PROJECT_ID} namespace: config-control annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/config-control/IAMServiceAccount/${TENANT_PROJECT_ID},resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} spec: memberFrom: serviceAccountRef: name: ${TENANT_PROJECT_ID} role: roles/redis.admin resourceRef: kind: Project external: projects/${TENANT_PROJECT_ID} EOF Deploy Kubernetes manifests cd ${WORK_DIR}$HOST_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Allow Memorystore (Redis) for Tenant project\" \u0026\u0026 git push origin main Check deployments graph TD; Service-.-\u003eProject IAMPolicyMember-.-\u003eProject IAMPolicyMember-.-\u003eIAMServiceAccount List the Kubernetes resources managed by Config Sync in Config Controller for the Host project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Host project configs repository:\ncd ${WORK_DIR}$HOST_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud services list \\ --enabled \\ --project ${TENANT_PROJECT_ID} \\ | grep -E 'redis' gcloud projects get-iam-policy $TENANT_PROJECT_ID \\ --filter=\"bindings.members:${TENANT_PROJECT_SA_EMAIL}\" \\ --flatten=\"bindings[].members\" \\ --format=\"table(bindings.role)\" \\ | grep redis ",
    "description": "Duration: 5 min | Persona: Org Admin",
    "tags": [
      "kcc",
      "org-admin"
    ],
    "title": "Allow Memorystore",
    "uri": "/acm-workshop/onlineboutique/memorystore/allow-memorystore/index.html"
  },
  {
    "content": " Duration: 2 min | Persona: Org Admin\nIn this section, you will enable and grant the appropriate APIs in the Tenant project and the IAM role for the Tenant project’s service account. This will allow later this service account to provision the networking services.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Define role Define the compute.networkAdmin role with an IAMPolicyMember for the Tenant project’s service account:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/network-admin.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: network-admin-${TENANT_PROJECT_ID} namespace: config-control annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/config-control/IAMServiceAccount/${TENANT_PROJECT_ID},resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} spec: memberFrom: serviceAccountRef: name: ${TENANT_PROJECT_ID} role: roles/compute.networkAdmin resourceRef: kind: Project external: projects/${TENANT_PROJECT_ID} EOF Define API Define the Compute Engine API Service resource for the Tenant project:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/compute-service.yaml apiVersion: serviceusage.cnrm.cloud.google.com/v1beta1 kind: Service metadata: annotations: cnrm.cloud.google.com/deletion-policy: \"abandon\" cnrm.cloud.google.com/disable-dependent-services: \"false\" config.kubernetes.io/depends-on: resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} name: ${TENANT_PROJECT_ID}-compute namespace: config-control spec: projectRef: name: ${TENANT_PROJECT_ID} resourceID: compute.googleapis.com EOF Info Throughout this workshop, we are enabling the Google Cloud services APIs from the Org Admin, it allows more control and governance over which Google Cloud services APIs the Platform Admin could use or not.\nDeploy Kubernetes manifests cd ${WORK_DIR}$HOST_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Allow Networking for Tenant project\" \u0026\u0026 git push origin main Check deployments graph TD; IAMPolicyMember-.-\u003eProject IAMPolicyMember-.-\u003eService List the Kubernetes resources managed by Config Sync in Config Controller for the Host project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Host project configs repository:\ncd ${WORK_DIR}$HOST_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud services list \\ --enabled \\ --project ${TENANT_PROJECT_ID} \\ | grep compute gcloud projects get-iam-policy $TENANT_PROJECT_ID \\ --filter=\"bindings.members:${TENANT_PROJECT_SA_EMAIL}\" \\ --flatten=\"bindings[].members\" \\ --format=\"table(bindings.role)\" \\ | grep networkAdmin Wait and re-run this command above until you see the resources created.\n",
    "description": "Duration: 2 min | Persona: Org Admin",
    "tags": [
      "kcc",
      "org-admin"
    ],
    "title": "Allow Networking",
    "uri": "/acm-workshop/networking/allow-networking/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Org Admin\nIn this section, you will enable and grant the appropriate APIs in the Tenant project and the IAM role for the Tenant project’s service account. This will allow later this service account to provision a Spanner instance.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Define API Define the Spanner API Service resource for the Tenant project:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/spanner-service.yaml apiVersion: serviceusage.cnrm.cloud.google.com/v1beta1 kind: Service metadata: annotations: cnrm.cloud.google.com/deletion-policy: \"abandon\" cnrm.cloud.google.com/disable-dependent-services: \"false\" config.kubernetes.io/depends-on: resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} name: ${TENANT_PROJECT_ID}-spanner namespace: config-control spec: projectRef: name: ${TENANT_PROJECT_ID} resourceID: spanner.googleapis.com EOF Define role Define the spanner.admin role with an IAMPolicyMember for the Tenant project’s service account:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/spanner-admin.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: spanner-admin-${TENANT_PROJECT_ID} namespace: config-control annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/config-control/IAMServiceAccount/${TENANT_PROJECT_ID},resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} spec: memberFrom: serviceAccountRef: name: ${TENANT_PROJECT_ID} role: roles/spanner.admin resourceRef: kind: Project external: projects/${TENANT_PROJECT_ID} EOF Deploy Kubernetes manifests cd ${WORK_DIR}$HOST_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Allow Spanner for Tenant project\" \u0026\u0026 git push origin main Check deployments graph TD; Service-.-\u003eProject IAMPolicyMember-.-\u003eProject IAMPolicyMember-.-\u003eIAMServiceAccount List the Kubernetes resources managed by Config Sync in Config Controller for the Host project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Host project configs repository:\ncd ${WORK_DIR}$HOST_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud services list \\ --enabled \\ --project ${TENANT_PROJECT_ID} \\ | grep -E 'spanner' gcloud projects get-iam-policy $TENANT_PROJECT_ID \\ --filter=\"bindings.members:${TENANT_PROJECT_SA_EMAIL}\" \\ --flatten=\"bindings[].members\" \\ --format=\"table(bindings.role)\" \\ | grep spanner ",
    "description": "Duration: 5 min | Persona: Org Admin",
    "tags": [
      "kcc",
      "org-admin"
    ],
    "title": "Allow Spanner",
    "uri": "/acm-workshop/onlineboutique/spanner/allow-spanner/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Org Admin\nIn this section, you will create the Host project. This Google Cloud project will host the Config Controller instance later.\nTwo Google Cloud Projects will be created during this workshop, we will assign them your Billing Account Id. Set your Billing Account Id for the rest of the workshop:\nBILLING_ACCOUNT_ID=FIXME These two Google Cloud Projects will be created either at the Folder level (recommended) or at the Organization level. Set your Folder Id or Organization Id for the rest of the workshop:\nFOLDER_OR_ORG_ID=FIXME Define the acm-workshop-variables.sh file which will contain all the reusable environment variables leveraged throughout this workshop.\nWORK_DIR=~/ touch ${WORK_DIR}acm-workshop-variables.sh chmod +x ${WORK_DIR}acm-workshop-variables.sh RANDOM_SUFFIX=$(shuf -i 100-999 -n 1) echo \"export RANDOM_SUFFIX=${RANDOM_SUFFIX}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh echo \"export HOST_PROJECT_ID=acm-workshop-${RANDOM_SUFFIX}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh echo \"export BILLING_ACCOUNT_ID=${BILLING_ACCOUNT_ID}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh echo \"export FOLDER_OR_ORG_ID=${FOLDER_OR_ORG_ID}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Info This source ${WORK_DIR}acm-workshop-variables.sh command will be particularly helpful if you are running this workshop from within Cloud Shell. You could run it every time your session timed out to reinitialize yoru environment variables.\nCreate Host project Create the Config Controller’s GCP project either at the Folder level or the Organization level: ​ Folder level Folder level Org level Org level Create this resource at a Folder level:\ngcloud projects create $HOST_PROJECT_ID \\ --folder $FOLDER_OR_ORG_ID \\ --name $HOST_PROJECT_ID Alternatively, you could also create this resource at the Organization level:\ngcloud projects create $HOST_PROJECT_ID \\ --organization $FOLDER_OR_ORG_ID \\ --name $HOST_PROJECT_ID Set the Billing account on this GCP project:\ngcloud beta billing projects link $HOST_PROJECT_ID \\ --billing-account $BILLING_ACCOUNT_ID Set this project as the default project for following gcloud commands:\ngcloud config set project $HOST_PROJECT_ID Check deployments List the Google Cloud resources created:\ngcloud projects describe $HOST_PROJECT_ID ",
    "description": "Duration: 5 min | Persona: Org Admin",
    "tags": [
      "org-admin"
    ],
    "title": "Create Host project",
    "uri": "/acm-workshop/host-project/create-host-project/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Platform Admin\nIn this section, you will create a public IP address in order to expose all your applications in your Service Mesh thanks to an Ingress Gateway you will configure.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export INGRESS_GATEWAY_PUBLIC_IP_NAME=${GKE_NAME}-asm-ingressgateway\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Define IP address Define the Ingress Gateway’s public static IP address resource:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/public-ip-address.yaml apiVersion: compute.cnrm.cloud.google.com/v1beta1 kind: ComputeAddress metadata: annotations: cnrm.cloud.google.com/project-id: ${TENANT_PROJECT_ID} name: ${INGRESS_GATEWAY_PUBLIC_IP_NAME} namespace: ${TENANT_PROJECT_ID} spec: location: global EOF Deploy Kubernetes manifests cd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Ingress Gateway's public static IP address\" \u0026\u0026 git push origin main Check deployments graph TD; ComputeAddress-.-\u003eProject List the Kubernetes resources managed by Config Sync in Config Controller for the Tenant project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $TENANT_PROJECT_ID Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Tenant project configs repository:\ncd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud compute addresses list \\ --project $TENANT_PROJECT_ID \\ | grep ${INGRESS_GATEWAY_PUBLIC_IP_NAME} Wait and re-run this command above until you see the resources created.\nGet the provisioned IP address INGRESS_GATEWAY_PUBLIC_IP=$(gcloud compute addresses describe $INGRESS_GATEWAY_PUBLIC_IP_NAME --global --project ${TENANT_PROJECT_ID} --format \"value(address)\") echo ${INGRESS_GATEWAY_PUBLIC_IP} echo \"export INGRESS_GATEWAY_PUBLIC_IP=${INGRESS_GATEWAY_PUBLIC_IP}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh ",
    "description": "Duration: 5 min | Persona: Platform Admin",
    "tags": [
      "kcc",
      "platform-admin"
    ],
    "title": "Create IP address",
    "uri": "/acm-workshop/ingress-gateway/create-ip-address/index.html"
  },
  {
    "content": " Duration: 10 min | Persona: Org Admin\nIn this section, you will create the Tenant project. The Tenant project will contain all the Google Cloud resources needed in this workshop.\nDefine variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh TENANT_PROJECT_ID=acm-workshop-${RANDOM_SUFFIX}-tenant echo \"export TENANT_PROJECT_ID=${TENANT_PROJECT_ID}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh echo \"export TENANT_PROJECT_SA_EMAIL=${TENANT_PROJECT_ID}@${HOST_PROJECT_ID}.iam.gserviceaccount.com\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Create a dedicated folder for this Tenant project resources:\nmkdir ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects mkdir ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID Define GCP project Define the GCP project either at the Folder level or the Organization level: ​ Folder level Folder level Org level Org level At the Folder level:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/project.yaml apiVersion: resourcemanager.cnrm.cloud.google.com/v1beta1 kind: Project metadata: annotations: cnrm.cloud.google.com/auto-create-network: \"false\" name: ${TENANT_PROJECT_ID} namespace: config-control spec: name: ${TENANT_PROJECT_ID} billingAccountRef: external: \"${BILLING_ACCOUNT_ID}\" folderRef: external: \"${FOLDER_OR_ORG_ID}\" resourceID: ${TENANT_PROJECT_ID} EOF At the Organization level:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/project.yaml apiVersion: resourcemanager.cnrm.cloud.google.com/v1beta1 kind: Project metadata: annotations: cnrm.cloud.google.com/auto-create-network: \"false\" name: ${TENANT_PROJECT_ID} namespace: config-control spec: name: ${TENANT_PROJECT_ID} billingAccountRef: external: \"${BILLING_ACCOUNT_ID}\" organizationRef: external: \"${FOLDER_OR_ORG_ID}\" resourceID: ${TENANT_PROJECT_ID} EOF Define Tenant project service account cat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/service-account.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMServiceAccount metadata: name: ${TENANT_PROJECT_ID} namespace: config-control annotations: config.kubernetes.io/depends-on: resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} spec: displayName: ${TENANT_PROJECT_ID} EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/workload-identity-user.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPartialPolicy metadata: name: ${TENANT_PROJECT_ID}-sa-wi-user namespace: config-control annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/config-control/IAMServiceAccount/${TENANT_PROJECT_ID} spec: resourceRef: name: ${TENANT_PROJECT_ID} kind: IAMServiceAccount bindings: - role: roles/iam.workloadIdentityUser members: - member: serviceAccount:${HOST_PROJECT_ID}.svc.id.goog[cnrm-system/cnrm-controller-manager-${TENANT_PROJECT_ID}] EOF Tip You could see that we use the annotation config.kubernetes.io/depends-on, since the version 1.11 of Config Management we could declare resource dependencies between resource objects. KCC already handles dependencies with a retry loop with backoff, which can make things with long reconcile time even longer and generate warnings or errors on these resources. With that annotation we are optimizing these behaviors. We will use this annotation as much as we can throughout this workshop.\nDefine Tenant project namespace and ConfigConnectorContext cat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: ${TENANT_PROJECT_ID} EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/config-connector-context.yaml apiVersion: core.cnrm.cloud.google.com/v1beta1 kind: ConfigConnectorContext metadata: name: configconnectorcontext.core.cnrm.cloud.google.com namespace: ${TENANT_PROJECT_ID} annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/config-control/IAMServiceAccount/${TENANT_PROJECT_ID} spec: billingProject: ${TENANT_PROJECT_ID} googleServiceAccount: ${TENANT_PROJECT_SA_EMAIL} requestProjectPolicy: BILLING_PROJECT EOF Deploy Kubernetes manifests cd ${WORK_DIR}$HOST_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Setting up Tenant namespace/project\" \u0026\u0026 git push origin main Check deployments graph TD; IAMServiceAccount--\u003eProject IAMPartialPolicy--\u003eIAMServiceAccount ConfigConnectorContext--\u003eIAMServiceAccount List the Kubernetes resources managed by Config Sync in Config Controller for the Host project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Host project configs repository:\ncd ${WORK_DIR}$HOST_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud projects describe $TENANT_PROJECT_ID gcloud iam service-accounts describe $TENANT_PROJECT_SA_EMAIL \\ --project $HOST_PROJECT_ID Resolve Tenant project creation issue Let’s make sure that the Tenant project has been successfully created.\nRun this command below:\nkubectl get gcpproject -n config-control If the output is similar to this below (STATUS UpToDate), you are good and you could move forward to the next page:\nNAME AGE READY STATUS STATUS AGE acm-workshop-742-tenant 50m True UpToDate 47m But if you have this output below (STATUS UpdateFailed), that’s where you will need to take actions:\nNAME AGE READY STATUS STATUS AGE acm-workshop-742-tenant 50m True UpdateFailed 47m Run this command below to have a closer look at the details of the error:\nkubectl describe gcpproject -n config-control The error you may have could be similar to:\nUpdate call failed: error applying desired state: summary: failed pre-requisites: missing permission on \"billingAccounts/XXX\": billing.resourceAssociations.create We will resolve this issue by redeploying the Project resource by removing the billingAccountRef part.\nUpdate the GCP project either at the Folder level or the Organization level: ​ Folder level Folder level Org level Org level At the Folder level:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/project.yaml apiVersion: resourcemanager.cnrm.cloud.google.com/v1beta1 kind: Project metadata: annotations: cnrm.cloud.google.com/auto-create-network: \"false\" name: ${TENANT_PROJECT_ID} namespace: config-control spec: name: ${TENANT_PROJECT_ID} folderRef: external: \"${FOLDER_OR_ORG_ID}\" resourceID: ${TENANT_PROJECT_ID} EOF At the Organization level:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/project.yaml apiVersion: resourcemanager.cnrm.cloud.google.com/v1beta1 kind: Project metadata: annotations: cnrm.cloud.google.com/auto-create-network: \"false\" name: ${TENANT_PROJECT_ID} namespace: config-control spec: name: ${TENANT_PROJECT_ID} organizationRef: external: \"${FOLDER_OR_ORG_ID}\" resourceID: ${TENANT_PROJECT_ID} EOF Re-deploy the Project resource:\ncd ${WORK_DIR}$HOST_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Remove the billingAccountRef in order to create Tenant Project\" \u0026\u0026 git push origin main Wait for status SYNCED with this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system And then, check that the Google Cloud project is created:\ngcloud projects describe $TENANT_PROJECT_ID Then what you have to do is to manually assign the Billing Account to this project by running by yourself this command below:\ngcloud beta billing projects link $TENANT_PROJECT_ID \\ --billing-account $BILLING_ACCOUNT_ID If you can’t run the command above, the alternative is having someone in your organization (Billing Account or Organization admins) running it for you.\n",
    "description": "Duration: 10 min | Persona: Org Admin",
    "tags": [
      "kcc",
      "org-admin"
    ],
    "title": "Create Tenant project",
    "uri": "/acm-workshop/tenant-project/create-tenant-project/index.html"
  },
  {
    "content": " Personas Agenda Before you begin Credits Costs ",
    "description": "",
    "tags": null,
    "title": "Overview",
    "uri": "/acm-workshop/overview/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Platform Admin\nIn this section, you will set up a dedicated DNS with Cloud Endpoints you will use later for the Bank of Anthos app.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export BANK_OF_ANTHOS_INGRESS_GATEWAY_HOST_NAME='bankofanthos.endpoints.${TENANT_PROJECT_ID}.cloud.goog'\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Create FQDN Create an FQDN with Cloud Endpoints for Bank of Anthos:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}dns-spec.yaml swagger: \"2.0\" info: description: \"Bank of Anthos Cloud Endpoints DNS\" title: \"Bank of Anthos Cloud Endpoints DNS\" version: \"1.0.0\" paths: {} host: \"${BANK_OF_ANTHOS_INGRESS_GATEWAY_HOST_NAME}\" x-google-endpoints: - name: \"${BANK_OF_ANTHOS_INGRESS_GATEWAY_HOST_NAME}\" target: \"${INGRESS_GATEWAY_PUBLIC_IP}\" EOF gcloud endpoints services deploy ${WORK_DIR}dns-spec.yaml \\ --project ${TENANT_PROJECT_ID} rm ${WORK_DIR}dns-spec.yaml Define ManagedCertificate resource Define the ManagedCertificate for Bank of Anthos in the Ingress Gateway namespace:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE/managedcertificate-bankofanthos.yaml apiVersion: networking.gke.io/v1 kind: ManagedCertificate metadata: name: bankofanthos namespace: ${INGRESS_GATEWAY_NAMESPACE} spec: domains: - \"${BANK_OF_ANTHOS_INGRESS_GATEWAY_HOST_NAME}\" EOF Update Ingress Configure Bank of Anthos ManagedCertificate on the Ingress Gateway’s Ingress resource:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE kpt fn eval . \\ -i set-annotations:v0.1 \\ --match-kind Ingress \\ -- networking.gke.io/managed-certificates=whereami,onlineboutique,bankofanthos Note The networking.gke.io/managed-certificates annotation has 3 values, whereami and onlineboutique configured previously and the new bankofanthos we are configuring with this page. Very important to keep the three here.\nDeploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Bank of Anthos ManagedCertificate\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the GKE cluster configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud endpoints services list \\ --project $TENANT_PROJECT_ID gcloud compute ssl-certificates list \\ --project $TENANT_PROJECT_ID Note Wait for the ManagedCertificate to be provisioned. This usually takes about 30 minutes.\n",
    "description": "Duration: 5 min | Persona: Platform Admin",
    "tags": [
      "platform-admin"
    ],
    "title": "Set up DNS",
    "uri": "/acm-workshop/bankofanthos/set-up-dns/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Platform Admin\nIn this section, you will set up a dedicated DNS with Cloud Endpoints you will use later for the Online Boutique app.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME='onlineboutique.endpoints.${TENANT_PROJECT_ID}.cloud.goog'\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Create FQDN Create an FQDN with Cloud Endpoints for Online Boutique:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}dns-spec.yaml swagger: \"2.0\" info: description: \"Online Boutique Cloud Endpoints DNS\" title: \"Online Boutique Cloud Endpoints DNS\" version: \"1.0.0\" paths: {} host: \"${ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME}\" x-google-endpoints: - name: \"${ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME}\" target: \"${INGRESS_GATEWAY_PUBLIC_IP}\" EOF gcloud endpoints services deploy ${WORK_DIR}dns-spec.yaml \\ --project ${TENANT_PROJECT_ID} rm ${WORK_DIR}dns-spec.yaml Define ManagedCertificate resource Define the ManagedCertificate for Online Boutique in the Ingress Gateway namespace:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE/managedcertificate-onlineboutique.yaml apiVersion: networking.gke.io/v1 kind: ManagedCertificate metadata: name: onlineboutique namespace: ${INGRESS_GATEWAY_NAMESPACE} spec: domains: - \"${ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME}\" EOF Update Ingress Configure Online Boutique ManagedCertificate on the Ingress Gateway’s Ingress resource:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE kpt fn eval . \\ -i set-annotations:v0.1 \\ --match-kind Ingress \\ -- networking.gke.io/managed-certificates=whereami,onlineboutique Note The networking.gke.io/managed-certificates annotation has 2 values, whereami configured previously and the new onlineboutique we are configuring with this page. Very important to keep both here.\nDeploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Online Boutique ManagedCertificate\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the GKE cluster configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud endpoints services list \\ --project $TENANT_PROJECT_ID gcloud compute ssl-certificates list \\ --project $TENANT_PROJECT_ID Note Wait for the ManagedCertificate to be provisioned. This usually takes about 30 minutes.\n",
    "description": "Duration: 5 min | Persona: Platform Admin",
    "tags": [
      "platform-admin"
    ],
    "title": "Set up DNS",
    "uri": "/acm-workshop/onlineboutique/set-up-dns/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Platform Admin\nIn this section, you will set up a dedicated DNS with Cloud Endpoints you will use later for the Whereami app.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export WHERE_AMI_INGRESS_GATEWAY_HOST_NAME='whereami.endpoints.${TENANT_PROJECT_ID}.cloud.goog'\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Create FQDN Create an FQDN with Cloud Endpoints for Whereami:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}dns-spec.yaml swagger: \"2.0\" info: description: \"Whereami Cloud Endpoints DNS\" title: \"Whereami Cloud Endpoints DNS\" version: \"1.0.0\" paths: {} host: \"${WHERE_AMI_INGRESS_GATEWAY_HOST_NAME}\" x-google-endpoints: - name: \"${WHERE_AMI_INGRESS_GATEWAY_HOST_NAME}\" target: \"${INGRESS_GATEWAY_PUBLIC_IP}\" EOF gcloud endpoints services deploy ${WORK_DIR}dns-spec.yaml \\ --project ${TENANT_PROJECT_ID} rm ${WORK_DIR}dns-spec.yaml Define ManagedCertificate Define the ManagedCertificate for Whereami in the Ingress Gateway namespace:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE/managedcertificate-whereami.yaml apiVersion: networking.gke.io/v1 kind: ManagedCertificate metadata: name: whereami namespace: ${INGRESS_GATEWAY_NAMESPACE} spec: domains: - \"${WHERE_AMI_INGRESS_GATEWAY_HOST_NAME}\" EOF Update Ingress Configure Whereami ManagedCertificate on the Ingress Gateway’s Ingress resource:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE kpt fn eval . \\ -i set-annotations:v0.1 \\ --match-kind Ingress \\ -- networking.gke.io/managed-certificates=whereami Deploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Whereami ManagedCertificate\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the GKE cluster configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud endpoints services list \\ --project $TENANT_PROJECT_ID gcloud compute ssl-certificates list \\ --project $TENANT_PROJECT_ID Wait and re-run this command above until you see the resources created. Note Wait for the ManagedCertificate to be provisioned. This usually takes about 30 minutes.\n",
    "description": "Duration: 5 min | Persona: Platform Admin",
    "tags": [
      "platform-admin"
    ],
    "title": "Set up DNS",
    "uri": "/acm-workshop/whereami/set-up-dns/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Platform Admin\nIn this section, you will set up the VPC, subnet and Cloud NAT which will be used by the GKE cluster later.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export GKE_NAME=gke\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Define VPC and Subnet Define the VPC:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/vpc.yaml apiVersion: compute.cnrm.cloud.google.com/v1beta1 kind: ComputeNetwork metadata: annotations: cnrm.cloud.google.com/project-id: ${TENANT_PROJECT_ID} name: ${GKE_NAME} namespace: ${TENANT_PROJECT_ID} spec: routingMode: REGIONAL autoCreateSubnetworks: false EOF Define the Subnet:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/subnet.yaml apiVersion: compute.cnrm.cloud.google.com/v1beta1 kind: ComputeSubnetwork metadata: name: ${GKE_NAME} namespace: ${TENANT_PROJECT_ID} annotations: config.kubernetes.io/depends-on: compute.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/ComputeNetwork/${GKE_NAME} spec: ipCidrRange: 10.2.0.0/20 region: ${GKE_LOCATION} networkRef: name: ${GKE_NAME} secondaryIpRange: - rangeName: servicesrange ipCidrRange: 10.3.0.0/20 - rangeName: clusterrange ipCidrRange: 10.4.0.0/20 EOF Define Cloud NAT cat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/router.yaml apiVersion: compute.cnrm.cloud.google.com/v1beta1 kind: ComputeRouter metadata: name: ${GKE_NAME} namespace: ${TENANT_PROJECT_ID} annotations: config.kubernetes.io/depends-on: compute.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/ComputeNetwork/${GKE_NAME} spec: networkRef: name: ${GKE_NAME} region: ${GKE_LOCATION} EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/router-nat.yaml apiVersion: compute.cnrm.cloud.google.com/v1beta1 kind: ComputeRouterNAT metadata: name: ${GKE_NAME} namespace: ${TENANT_PROJECT_ID} annotations: config.kubernetes.io/depends-on: compute.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/ComputeSubnetwork/${GKE_NAME},compute.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/ComputeRouter/${GKE_NAME} spec: natIpAllocateOption: AUTO_ONLY region: ${GKE_LOCATION} routerRef: name: ${GKE_NAME} sourceSubnetworkIpRangesToNat: LIST_OF_SUBNETWORKS subnetwork: - subnetworkRef: name: ${GKE_NAME} sourceIpRangesToNat: - ALL_IP_RANGES EOF Deploy Kubernetes manifests cd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Network for Tenant project\" \u0026\u0026 git push origin main Check deployments graph TD; ComputeNetwork-.-\u003eProject ComputeSubnetwork--\u003eComputeNetwork ComputeRouterNAT--\u003eComputeSubnetwork ComputeRouterNAT--\u003eComputeRouter ComputeRouter--\u003eComputeNetwork List the Kubernetes resources managed by Config Sync in Config Controller for the Tenant project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $TENANT_PROJECT_ID Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Tenant project configs repository:\ncd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud compute networks list \\ --project $TENANT_PROJECT_ID gcloud compute networks subnets list \\ --project $TENANT_PROJECT_ID gcloud compute routers list \\ --project $TENANT_PROJECT_ID gcloud compute routers nats list \\ --router $GKE_NAME \\ --region $GKE_LOCATION \\ --project $TENANT_PROJECT_ID ",
    "description": "Duration: 5 min | Persona: Platform Admin",
    "tags": [
      "gitops-tips",
      "kcc",
      "platform-admin"
    ],
    "title": "Set up Network",
    "uri": "/acm-workshop/networking/set-up-network/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Platform Admin\nIn this section, you will verify in the Google Cloud console the versions of both: the control plane and data plane of ASM.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Managed Anthos Service Mesh is a Google-managed control plane and data plane that you simply configure. Google handles their reliability, upgrades, scaling and security for you.\nYou can view the versions of the control plane (revision column below) and data plane (proxy-version column below) in Monitoring \u003e Metrics Explorer. Click on the link displayed by the command below:\necho -e \"https://console.cloud.google.com/monitoring/metrics-explorer?pageState=%7B%22xyChart%22:%7B%22dataSets%22:%5B%7B%22timeSeriesFilter%22:%7B%22filter%22:%22metric.type%3D%5C%22istio.io%2Fcontrol%2Fproxy_clients%5C%22%20resource.type%3D%5C%22k8s_container%5C%22%20resource.label.%5C%22container_name%5C%22%3D%5C%22cr-${ASM_VERSION}%5C%22%22,%22minAlignmentPeriod%22:%2260s%22,%22aggregations%22:%5B%7B%22perSeriesAligner%22:%22ALIGN_MEAN%22,%22crossSeriesReducer%22:%22REDUCE_SUM%22,%22alignmentPeriod%22:%2260s%22,%22groupByFields%22:%5B%22metric.label.%5C%22revision%5C%22%22,%22metric.label.%5C%22proxy_version%5C%22%22%5D%7D,%7B%22perSeriesAligner%22:%22ALIGN_NONE%22,%22crossSeriesReducer%22:%22REDUCE_NONE%22,%22alignmentPeriod%22:%2260s%22,%22groupByFields%22:%5B%5D%7D%5D%7D,%22targetAxis%22:%22Y1%22,%22plotType%22:%22LINE%22%7D%5D,%22options%22:%7B%22mode%22:%22COLOR%22%7D,%22constantLines%22:%5B%5D,%22timeshiftDuration%22:%220s%22,%22y1Axis%22:%7B%22label%22:%22y1Axis%22,%22scale%22:%22LINEAR%22%7D%7D,%22isAutoRefresh%22:true,%22timeSelection%22:%7B%22timeRange%22:%221h%22%7D%7D\u0026project=${TENANT_PROJECT_ID}\" ",
    "description": "Duration: 5 min | Persona: Platform Admin",
    "tags": [
      "asm",
      "monitoring",
      "platform-admin"
    ],
    "title": "Verify ASM version",
    "uri": "/acm-workshop/monitoring-and-audit/verify-asm-version/index.html"
  },
  {
    "content": "Set up the Host project with the Config Controller instance.\nCreate Host projectDuration: 5 min | Persona: Org Admin\nCreate Config ControllerDuration: 20 min | Persona: Org Admin\nSet up Host project's Git repoDuration: 10 min | Persona: Org Admin\nEnforce Tenant projects policiesDuration: 5 min | Persona: Org Admin\n",
    "description": "",
    "tags": null,
    "title": "1. Host project",
    "uri": "/acm-workshop/host-project/index.html"
  },
  {
    "content": " Duration: 2 min | Persona: Org Admin\nIn this section, you will grant the appropriate IAM role for the Tenant project’s service account. This will allow later this service account to provision Cloud Armor.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Define role Define the compute.securityAdmin role with an IAMPolicyMember for the Tenant project’s service account:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/security-admin.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: security-admin-${TENANT_PROJECT_ID} namespace: config-control annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/config-control/IAMServiceAccount/${TENANT_PROJECT_ID},resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} spec: memberFrom: serviceAccountRef: name: ${TENANT_PROJECT_ID} role: roles/compute.securityAdmin resourceRef: kind: Project external: projects/${TENANT_PROJECT_ID} EOF Deploy Kubernetes manifests cd ${WORK_DIR}$HOST_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Allow Cloud Armor for Tenant project\" \u0026\u0026 git push origin main Check deployments graph TD; IAMPolicyMember-.-\u003eProject List the Kubernetes resources managed by Config Sync in Config Controller for the Host project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Host project configs repository:\ncd ${WORK_DIR}$HOST_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud projects get-iam-policy $TENANT_PROJECT_ID \\ --filter=\"bindings.members:${TENANT_PROJECT_SA_EMAIL}\" \\ --flatten=\"bindings[].members\" \\ --format=\"table(bindings.role)\" \\ | grep securityAdmin Wait and re-run this command above until you see the resources created.\n",
    "description": "Duration: 2 min | Persona: Org Admin",
    "tags": [
      "kcc",
      "org-admin"
    ],
    "title": "Allow Cloud Armor",
    "uri": "/acm-workshop/ingress-gateway/allow-cloud-armor/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Platform Admin\nIn this section, you will set up your own private Artifact Registry to store both all the container images and the Helm charts required for this workshop. You will also grant viewer access to both: the GKE’s GSA and Config Sync’s GSA.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh CONTAINER_REGISTRY_NAME=containers echo \"export CONTAINER_REGISTRY_NAME=${CONTAINER_REGISTRY_NAME}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh CHART_REGISTRY_NAME=charts echo \"export CHART_REGISTRY_NAME=${CHART_REGISTRY_NAME}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh CONTAINER_REGISTRY_HOST_NAME=${GKE_LOCATION}-docker.pkg.dev echo \"export CONTAINER_REGISTRY_HOST_NAME=${CONTAINER_REGISTRY_HOST_NAME}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh echo \"export CONTAINER_REGISTRY_REPOSITORY=${CONTAINER_REGISTRY_HOST_NAME}/${TENANT_PROJECT_ID}/${CONTAINER_REGISTRY_NAME}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh echo \"export CHART_REGISTRY_REPOSITORY=${CONTAINER_REGISTRY_HOST_NAME}/${TENANT_PROJECT_ID}/${CHART_REGISTRY_NAME}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh echo \"export HELM_CHARTS_READER_GSA=helm-charts-reader\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Define Artifact Registry containers repository Define the Artifact Registry containers repository:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/artifactregistry-containers.yaml apiVersion: artifactregistry.cnrm.cloud.google.com/v1beta1 kind: ArtifactRegistryRepository metadata: annotations: cnrm.cloud.google.com/project-id: ${TENANT_PROJECT_ID} name: ${CONTAINER_REGISTRY_NAME} namespace: ${TENANT_PROJECT_ID} spec: format: DOCKER location: ${GKE_LOCATION} EOF Define Artifact Registry reader role for the GKE’s GSA for the container images cat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/artifactregistry-reader.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: artifactregistry-reader namespace: ${TENANT_PROJECT_ID} annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/IAMServiceAccount/${GKE_SA},artifactregistry.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/ArtifactRegistryRepository/${CONTAINER_REGISTRY_NAME} spec: memberFrom: serviceAccountRef: name: ${GKE_SA} namespace: ${TENANT_PROJECT_ID} resourceRef: kind: ArtifactRegistryRepository name: ${CONTAINER_REGISTRY_NAME} role: roles/artifactregistry.reader EOF Define Artifact Registry charts repository Define the Artifact Registry charts repository:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/artifactregistry-charts.yaml apiVersion: artifactregistry.cnrm.cloud.google.com/v1beta1 kind: ArtifactRegistryRepository metadata: annotations: cnrm.cloud.google.com/project-id: ${TENANT_PROJECT_ID} name: ${CHART_REGISTRY_NAME} namespace: ${TENANT_PROJECT_ID} spec: format: DOCKER location: ${GKE_LOCATION} EOF Define Artifact Registry reader role for the RepoSync’s GSA for the Helm charts Define the Helm charts registry’s Google Service Account:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/repo-syncs-sa.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMServiceAccount metadata: annotations: cnrm.cloud.google.com/project-id: ${TENANT_PROJECT_ID} name: ${HELM_CHARTS_READER_GSA} namespace: ${TENANT_PROJECT_ID} spec: displayName: ${HELM_CHARTS_READER_GSA} EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/artifactregistry-charts-reader.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: artifactregistry-charts-reader namespace: ${TENANT_PROJECT_ID} annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/IAMServiceAccount/${HELM_CHARTS_READER_GSA},artifactregistry.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/ArtifactRegistryRepository/${CHART_REGISTRY_NAME} spec: memberFrom: serviceAccountRef: name: ${HELM_CHARTS_READER_GSA} namespace: ${TENANT_PROJECT_ID} resourceRef: kind: ArtifactRegistryRepository name: ${CHART_REGISTRY_NAME} role: roles/artifactregistry.reader EOF Deploy Kubernetes manifests cd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Artifact Registry for containers and charts for GKE cluster\" \u0026\u0026 git push origin main Check deployments graph TD; ArtifactRegistryRepository-.-\u003eProject IAMPolicyMember--\u003eArtifactRegistryRepository IAMPolicyMember-.-\u003eIAMServiceAccount List the Kubernetes resources managed by Config Sync in Config Controller for the Tenant project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $TENANT_PROJECT_ID Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Tenant project configs repository:\ncd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud artifacts repositories get-iam-policy $CONTAINER_REGISTRY_NAME \\ --project $TENANT_PROJECT_ID \\ --location $GKE_LOCATION \\ --filter=\"bindings.members:${GKE_SA}@${TENANT_PROJECT_ID}.iam.gserviceaccount.com\" \\ --flatten=\"bindings[].members\" \\ --format=\"table(bindings.role)\" gcloud artifacts repositories list \\ --project $TENANT_PROJECT_ID Wait and re-run this command above until you see the resources created.\n",
    "description": "Duration: 5 min | Persona: Platform Admin",
    "tags": [
      "gitops-tips",
      "kcc",
      "platform-admin"
    ],
    "title": "Create Artifact Registry",
    "uri": "/acm-workshop/artifact-registry/create-artifact-registry/index.html"
  },
  {
    "content": " Duration: 20 min | Persona: Org Admin\nIn this section, you will create your Config Controller instance. You will also add the least privilege Google Cloud roles to its associated service account. This Config Controller instance will allow throughout this workshop to deploy any infrastructure via Kubernetes manifests.\nDefine variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export CONFIG_CONTROLLER_NAME=configcontroller\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh echo \"export CONFIG_CONTROLLER_LOCATION=northamerica-northeast1\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh echo \"export CONFIG_CONTROLLER_NETWORK=default\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Info We are creating the Config Controller instance in northamerica-northeast1 because that’s the greenest Google Cloud region (Low CO2) in the regions supported by Config Controller: europe-north1, europe-west1, europe-west3, australia-southeast1, australia-southeast2, us-east1, us-central1, northamerica-northeast1, northamerica-northeast2, asia-northeast1 and asia-northeast2.\nCreate the Config Controller instance Enable the required Google Cloud APIs:\ngcloud services enable \\ compute.googleapis.com \\ krmapihosting.googleapis.com \\ cloudresourcemanager.googleapis.com If you don’t have a default network in your project, create one by running the following command:\ngcloud compute networks create $CONFIG_CONTROLLER_NETWORK \\ --subnet-mode=auto Note If you get an error telling you that the default network already exists, you can ignore it.\nCreate the Config Controller instance:\ngcloud alpha anthos config controller create $CONFIG_CONTROLLER_NAME \\ --location $CONFIG_CONTROLLER_LOCATION \\ --network $CONFIG_CONTROLLER_NETWORK \\ --full-management Tip As a security best practice you could provision your Config Controller instance with the --man-block $(curl -4 ifconfig.co)/32 parameter. We are not doing this in this workshop to avoid any issues with Cloud Shell which is allocating a new IP address as soon as the session expired.\nNote The Config Controller instance provisioning could take around 15-20 min.\nCheck that the Config Controller instance was successfully created:\ngcloud anthos config controller list \\ --location $CONFIG_CONTROLLER_LOCATION gcloud anthos config controller describe $CONFIG_CONTROLLER_NAME \\ --location $CONFIG_CONTROLLER_LOCATION Get the Config Controller instance credentials gcloud anthos config controller get-credentials $CONFIG_CONTROLLER_NAME \\ --location $CONFIG_CONTROLLER_LOCATION Set Config Controller’s service account roles Get the actual the Config Controller’s service account:\nCONFIG_CONTROLLER_SA=\"$(kubectl get ConfigConnectorContext \\ -n config-control \\ -o jsonpath='{.items[0].spec.googleServiceAccount}')\" Set the resourcemanager.projectCreator role either at the Folder level or the Organization level: ​ Folder level Folder level Org level Org level Create this resource at a Folder level:\ngcloud resource-manager folders add-iam-policy-binding ${FOLDER_OR_ORG_ID} \\ --member=\"serviceAccount:${CONFIG_CONTROLLER_SA}\" \\ --role='roles/resourcemanager.projectCreator' Alternatively, you could also create this resource at the Organization level:\ngcloud organizations add-iam-policy-binding ${FOLDER_OR_ORG_ID} \\ --member=\"serviceAccount:${CONFIG_CONTROLLER_SA}\" \\ --role='roles/resourcemanager.projectCreator' Set the serviceusage.serviceUsageAdmin and iam.serviceAccountAdmin roles:\ngcloud projects add-iam-policy-binding ${HOST_PROJECT_ID} \\ --member=\"serviceAccount:${CONFIG_CONTROLLER_SA}\" \\ --role='roles/serviceusage.serviceUsageAdmin' gcloud projects add-iam-policy-binding ${HOST_PROJECT_ID} \\ --member=\"serviceAccount:${CONFIG_CONTROLLER_SA}\" \\ --role='roles/iam.serviceAccountAdmin' Finally, you need to assign the billing.user role too. Later in this workshop, it will be needed to attach a Project to a Billing Account. If you don’t have the proper role you may have an error by running the command below. In this case you need to ask your Billing Account or Organization admins in order to run this command for you.\ngcloud beta billing accounts add-iam-policy-binding ${BILLING_ACCOUNT_ID} \\ --member=\"serviceAccount:${CONFIG_CONTROLLER_SA}\" \\ --role='roles/billing.user' Note In some specific scenario, you may not be able to accomplish this step. You could skip it for now, another way to assign the Billing Account to a Project will be provided later in this workshop, when you will need it.\nCheck deployments List the Google Cloud resources created:\ngcloud anthos config controller list \\ --project $HOST_PROJECT_ID gcloud beta billing accounts get-iam-policy ${BILLING_ACCOUNT_ID} \\ --filter=\"bindings.members:${CONFIG_CONTROLLER_SA}\" \\ --flatten=\"bindings[].members\" \\ --format=\"table(bindings.role)\" gcloud projects get-iam-policy $HOST_PROJECT_ID \\ --filter=\"bindings.members:${CONFIG_CONTROLLER_SA}\" \\ --flatten=\"bindings[].members\" \\ --format=\"table(bindings.role)\" ​ Folder level Folder level Org level Org level gcloud resource-manager folders get-iam-policy $FOLDER_OR_ORG_ID \\ --filter=\"bindings.members:${CONFIG_CONTROLLER_SA}\" \\ --flatten=\"bindings[].members\" \\ --format=\"table(bindings.role)\" gcloud organizations get-iam-policy $FOLDER_OR_ORG_ID \\ --filter=\"bindings.members:${CONFIG_CONTROLLER_SA}\" \\ --flatten=\"bindings[].members\" \\ --format=\"table(bindings.role)\" ",
    "description": "Duration: 20 min | Persona: Org Admin",
    "tags": [
      "org-admin"
    ],
    "title": "Create Config Controller",
    "uri": "/acm-workshop/host-project/create-config-controller/index.html"
  },
  {
    "content": " Duration: 10 min | Persona: Platform Admin\nIn this section, you will create a Spanner instance and database for the Online Boutique’s cartservice app to connect to. You will also configure the associated cartservice’s Google Service account to have fine granular read access to the Spanner database via Workload Identity.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export SPANNER_INSTANCE_NAME=onlineboutique\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh echo \"export SPANNER_DATABASE_NAME=carts\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh echo \"export SPANNER_DATABASE_USER_GSA_NAME=spanner-db-user\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Define Spanner instance Define the Spanner instance resource:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/$ONLINEBOUTIQUE_NAMESPACE/spanner-instance.yaml apiVersion: spanner.cnrm.cloud.google.com/v1beta1 kind: SpannerInstance metadata: annotations: cnrm.cloud.google.com/project-id: ${TENANT_PROJECT_ID} name: ${SPANNER_INSTANCE_NAME} namespace: ${TENANT_PROJECT_ID} spec: config: regional-${GKE_LOCATION} displayName: ${SPANNER_INSTANCE_NAME} numNodes: 2 EOF Define Spanner database Define the Spanner database resource:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/$ONLINEBOUTIQUE_NAMESPACE/spanner-database.yaml apiVersion: spanner.cnrm.cloud.google.com/v1beta1 kind: SpannerDatabase metadata: name: ${SPANNER_DATABASE_NAME} namespace: ${TENANT_PROJECT_ID} annotations: config.kubernetes.io/depends-on: spanner.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/SpannerInstance/${SPANNER_INSTANCE_NAME} spec: instanceRef: name: ${SPANNER_INSTANCE_NAME} databaseDialect: GOOGLE_STANDARD_SQL ddl: - \"CREATE TABLE CartItems (userId STRING(1024), productId STRING(1024), quantity INT64,) PRIMARY KEY (userId, productId)\" - \"CREATE INDEX CartItemsByUserId ON CartItems(userId)\" EOF Grant the cartservice’s service account access to the Spanner database cat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/$ONLINEBOUTIQUE_NAMESPACE/spanner-db-user-service-account.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMServiceAccount metadata: annotations: cnrm.cloud.google.com/project-id: ${TENANT_PROJECT_ID} name: ${SPANNER_DATABASE_USER_GSA_NAME} namespace: ${TENANT_PROJECT_ID} spec: displayName: ${SPANNER_DATABASE_USER_GSA_NAME} EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/$ONLINEBOUTIQUE_NAMESPACE/spanner-db-user.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: ${SPANNER_DATABASE_USER_GSA_NAME} namespace: ${TENANT_PROJECT_ID} annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/IAMServiceAccount/${SPANNER_DATABASE_USER_GSA_NAME},spanner.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/SpannerDatabase/${SPANNER_DATABASE_NAME} spec: memberFrom: serviceAccountRef: name: ${SPANNER_DATABASE_USER_GSA_NAME} namespace: ${TENANT_PROJECT_ID} resourceRef: kind: SpannerDatabase name: ${SPANNER_DATABASE_NAME} role: roles/spanner.databaseUser EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/$ONLINEBOUTIQUE_NAMESPACE/spanner-db-user-workload-identity-user.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPartialPolicy metadata: name: ${SPANNER_DATABASE_USER_GSA_NAME} namespace: ${TENANT_PROJECT_ID} annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/IAMServiceAccount/${SPANNER_DATABASE_USER_GSA_NAME} spec: resourceRef: name: ${SPANNER_DATABASE_USER_GSA_NAME} kind: IAMServiceAccount bindings: - role: roles/iam.workloadIdentityUser members: - member: serviceAccount:${TENANT_PROJECT_ID}.svc.id.goog[${ONLINEBOUTIQUE_NAMESPACE}/cartservice] EOF Deploy Kubernetes manifests cd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Spanner instance and database and configure cartservice's gsa\" \u0026\u0026 git push origin main Check deployments graph TD; SpannerInstance-.-\u003eProject SpannerDatabase--\u003eSpannerInstance IAMServiceAccount-.-\u003eProject IAMPolicyMember--\u003eSpannerDatabase IAMPolicyMember--\u003eIAMServiceAccount List the Kubernetes resources managed by Config Sync in Config Controller for the Tenant project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $TENANT_PROJECT_ID Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Tenant project configs repository:\ncd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud spanner instances list \\ --project=$TENANT_PROJECT_ID gcloud spanner databases list \\ --instance $SPANNER_INSTANCE_NAME \\ --project=$TENANT_PROJECT_ID gcloud spanner databases get-iam-policy $SPANNER_DATABASE_NAME \\ --project $TENANT_PROJECT_ID \\ --instance $SPANNER_INSTANCE_NAME \\ --filter=\"bindings.members:${SPANNER_DATABASE_USER_GSA_NAME}@${TENANT_PROJECT_ID}.iam.gserviceaccount.com\" \\ --flatten=\"bindings[].members\" \\ --format=\"table(bindings.role)\" ",
    "description": "Duration: 10 min | Persona: Platform Admin",
    "tags": [
      "kcc",
      "platform-admin"
    ],
    "title": "Create Spanner",
    "uri": "/acm-workshop/onlineboutique/spanner/create-spanner/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Org Admin\nIn this section, you will set up policies in order to enforce governance against the Kubernetes manifests defining your GKE cluster. This will guarantee that the best practices in term of security are respected.\nDefine variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Enforce GKE clusters policies Define the ConstraintTemplate:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/policies/templates/gkeclusterrequirement.yaml apiVersion: templates.gatekeeper.sh/v1 kind: ConstraintTemplate metadata: name: gkeclusterrequirement annotations: description: \"Requirements for any GKE cluster.\" spec: crd: spec: names: kind: GkeClusterRequirement targets: - target: admission.k8s.gatekeeper.sh rego: |- package gkeclusterrequirement violation[{\"msg\":msg}] { input.review.object.kind == \"ContainerCluster\" not input.review.object.spec.confidentialNodes.enabled == true msg := sprintf(\"GKE cluster %s should enable confidentialNodes.\", [input.review.object.metadata.name]) } violation[{\"msg\":msg}] { input.review.object.kind == \"ContainerCluster\" not input.review.object.spec.enableShieldedNodes == true msg := sprintf(\"GKE cluster %s should enable enableShieldedNodes.\", [input.review.object.metadata.name]) } violation[{\"msg\":msg}] { input.review.object.kind == \"ContainerCluster\" not input.review.object.spec.networkingMode == \"VPC_NATIVE\" msg := sprintf(\"GKE cluster %s should use VPC_NATIVE networkingMode.\", [input.review.object.metadata.name]) } violation[{\"msg\":msg}] { input.review.object.kind == \"ContainerCluster\" not input.review.object.spec.privateClusterConfig.enablePrivateNodes == true msg := sprintf(\"GKE cluster %s should enable enablePrivateNodes.\", [input.review.object.metadata.name]) } violation[{\"msg\":msg}] { input.review.object.kind == \"ContainerCluster\" not input.review.object.spec.workloadIdentityConfig.workloadPool msg := sprintf(\"GKE cluster %s should define workloadIdentityConfig.workloadPool.\", [input.review.object.metadata.name]) } violation[{\"msg\":msg}] { input.review.object.kind == \"ContainerCluster\" not input.review.object.spec.datapathProvider == \"ADVANCED_DATAPATH\" msg := sprintf(\"GKE cluster %s should define datapathProvider as ADVANCED_DATAPATH to use GKE Dataplane V2.\", [input.review.object.metadata.name]) } violation[{\"msg\":msg}] { input.review.object.kind == \"ContainerCluster\" not input.review.object.spec.addonsConfig.httpLoadBalancing.disabled == false msg := sprintf(\"GKE cluster %s should enable addonsConfig.httpLoadBalancing.\", [input.review.object.metadata.name]) } violation[{\"msg\":msg}] { input.review.object.kind == \"ContainerNodePool\" not input.review.object.spec.management.autoRepair == true msg := sprintf(\"GKE node pool %s should enable management.autoRepair.\", [input.review.object.metadata.name]) } violation[{\"msg\":msg}] { input.review.object.kind == \"ContainerNodePool\" not input.review.object.spec.management.autoUpgrade == true msg := sprintf(\"GKE node pool %s should enable management.autoUpgrade.\", [input.review.object.metadata.name]) } violation[{\"msg\":msg}] { input.review.object.kind == \"ContainerNodePool\" not input.review.object.spec.nodeConfig.imageType == \"COS_CONTAINERD\" msg := sprintf(\"GKE node pool %s should define nodeConfig.imageType as COS_CONTAINERD.\", [input.review.object.metadata.name]) } violation[{\"msg\":msg}] { input.review.object.kind == \"ContainerNodePool\" not input.review.object.spec.nodeConfig.shieldedInstanceConfig.enableIntegrityMonitoring == true msg := sprintf(\"GKE node pool %s should enable nodeConfig.shieldedInstanceConfig.enableIntegrityMonitoring.\", [input.review.object.metadata.name]) } violation[{\"msg\":msg}] { input.review.object.kind == \"ContainerNodePool\" not input.review.object.spec.nodeConfig.shieldedInstanceConfig.enableSecureBoot == true msg := sprintf(\"GKE node pool %s should enable nodeConfig.shieldedInstanceConfig.enableSecureBoot.\", [input.review.object.metadata.name]) } violation[{\"msg\":msg}] { input.review.object.kind == \"ContainerNodePool\" not input.review.object.spec.nodeConfig.serviceAccountRef.name msg := sprintf(\"GKE node pool %s should define nodeConfig.serviceAccountRef.\", [input.review.object.metadata.name]) } EOF Define the gke-clusters-requirements Constraint based on the GkeClusterRequirement ConstraintTemplate just defined:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/policies/constraints/gke-clusters-requirements.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: GkeClusterRequirement metadata: name: gke-clusters-requirements annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Requires ContainerClusters and ContainerNodePools to use mandatory and security features.', remediation: 'Any ContainerClusters and ContainerNodePools should use mandatory and security features.' }\" spec: enforcementAction: deny match: kinds: - apiGroups: - container.cnrm.cloud.google.com kinds: - ContainerCluster - ContainerNodePool EOF Define the gke-clusters-require-asm-label Constraint based on the K8sRequiredLabels ConstraintTemplate:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/policies/constraints/gke-clusters-require-asm-label.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sRequiredLabels metadata: name: gke-clusters-require-asm-label annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Requires ContainerClusters to have the \"mesh_id\" label in order to leverage the ASM UI features.', remediation: 'Any ContainerClusters should have the \"mesh_id\" label with the value like \"proj-*\", where \"*\" is the Project Number.' }\" spec: enforcementAction: deny match: kinds: - apiGroups: - container.cnrm.cloud.google.com kinds: - ContainerCluster parameters: labels: - allowedRegex: proj-* key: mesh_id EOF Deploy Kubernetes manifests cd ${WORK_DIR}$HOST_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Policies for GKE clusters\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in Config Controller for the Host project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Host project configs repository:\ncd ${WORK_DIR}$HOST_PROJECT_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 5 min | Persona: Org Admin",
    "tags": [
      "asm",
      "gke",
      "org-admin",
      "policies",
      "security-tips"
    ],
    "title": "Enforce GKE policies",
    "uri": "/acm-workshop/gke-cluster/enforce-gke-policies/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Org Admin\nIn this section, you will set up policies in order to enforce governance against the Kubernetes manifests defining your Memorystore (Redis) instances.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Enforce Memorystore policies Define the ConstraintTemplate resource:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/policies/templates/limitmemorystoreredis.yaml apiVersion: templates.gatekeeper.sh/v1 kind: ConstraintTemplate metadata: name: limitmemorystoreredis annotations: description: \"Requirements for any Memorystore (Redis) instance.\" spec: crd: spec: names: kind: LimitMemorystoreRedis targets: - target: admission.k8s.gatekeeper.sh rego: |- package limitmemorystoreredis violation[{\"msg\":msg}] { input.review.object.kind == \"RedisInstance\" not input.review.object.spec.redisVersion == \"REDIS_6_X\" msg := sprintf(\"Memorystore (Redis) %s's version should be REDIS_6_X instead of %s.\", [input.review.object.metadata.name, input.review.object.spec.redisVersion]) } violation[{\"msg\":msg}] { input.review.object.kind == \"RedisInstance\" not input.review.object.spec.authorizedNetworkRef msg := sprintf(\"Memorystore (Redis) %s's VPC shouldn't be default.\", [input.review.object.metadata.name]) } violation[{\"msg\":msg}] { input.review.object.kind == \"RedisInstance\" input.review.object.spec.authorizedNetworkRef.name == \"default\" msg := sprintf(\"Memorystore (Redis) %s's VPC shouldn't be default.\", [input.review.object.metadata.name]) } EOF Define the Constraint resource:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/policies/constraints/allowed-memorystore-redis.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: LimitMemorystoreRedis metadata: name: allowed-memorystore-redis annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Requires RedisInstances to use mandatory and security features.', remediation: 'Any RedisInstances should use mandatory and security features.' }\" spec: enforcementAction: deny match: kinds: - apiGroups: - redis.cnrm.cloud.google.com kinds: - RedisInstance EOF Deploy Kubernetes manifests cd ${WORK_DIR}$HOST_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Enforce Memorystore policies\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in Config Controller for the Host project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Host project configs repository:\ncd ${WORK_DIR}$HOST_PROJECT_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 5 min | Persona: Org Admin",
    "tags": [
      "org-admin",
      "policies",
      "security-tips"
    ],
    "title": "Enforce Memorystore policies",
    "uri": "/acm-workshop/onlineboutique/memorystore/enforce-memorystore-policies/index.html"
  },
  {
    "content": " Duration: 15 min | Persona: Platform Admin\nIn this section, you will install a Managed Service Mesh for your GKE cluster. This will opt your cluster in a specific channel in order to get the upgrades handled by Google for the managed control plane and managed data plane.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Define ASM feature for the tenant project Define the ASM GKEHubFeature resource:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/gke-hub-feature-asm.yaml apiVersion: gkehub.cnrm.cloud.google.com/v1beta1 kind: GKEHubFeature metadata: name: servicemesh namespace: ${TENANT_PROJECT_ID} spec: projectRef: name: ${TENANT_PROJECT_ID} location: global resourceID: servicemesh EOF Note The resourceID must be servicemesh if you want to use Managed Control Plane feature of Anthos Service Mesh.\nDefine Managed ASM for the GKE cluster Define the Managed ASM GKEHubFeatureMembership resource:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/gke-mesh-membership.yaml apiVersion: gkehub.cnrm.cloud.google.com/v1beta1 kind: GKEHubFeatureMembership metadata: name: ${GKE_NAME}-mesh-membership namespace: ${TENANT_PROJECT_ID} annotations: config.kubernetes.io/depends-on: gkehub.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/GKEHubMembership/${GKE_NAME},gkehub.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/GKEHubFeature/servicemesh spec: projectRef: name: ${TENANT_PROJECT_ID} location: global membershipRef: name: ${GKE_NAME} featureRef: name: servicemesh mesh: management: MANAGEMENT_AUTOMATIC EOF Deploy Kubernetes manifests cd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Managed ASM for GKE cluster in Tenant project\" \u0026\u0026 git push origin main Check deployments graph TD; GKEHubFeature-.-\u003eProject List the Kubernetes resources managed by Config Sync in Config Controller for the Tenant project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $TENANT_PROJECT_ID Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Tenant project configs repository:\ncd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud container fleet mesh describe \\ --project $TENANT_PROJECT_ID For the result of the last command, in order to make sure the Managed ASM is successfully installed you should see something like this:\ncreateTime: '2022-12-30T05:07:47.372234169Z' labels: managed-by-cnrm: 'true' membershipSpecs: projects/827641929724/locations/global/memberships/gke: mesh: management: MANAGEMENT_AUTOMATIC membershipStates: projects/827641929724/locations/global/memberships/gke: servicemesh: controlPlaneManagement: details: - code: REVISION_READY details: 'Ready: asm-managed-rapid' state: ACTIVE dataPlaneManagement: details: - code: OK details: Service is running. state: ACTIVE state: code: OK description: 'Revision(s) ready for use: asm-managed-rapid.' updateTime: '2022-12-30T05:20:57.379679757Z' name: projects/acm-workshop-618-tenant/locations/global/features/servicemesh resourceState: state: ACTIVE spec: {} state: state: {} updateTime: '2022-12-30T05:21:02.601258121Z' Wait and re-run this command above until you see both controlPlaneManagement and dataPlaneManagement with state: ACTIVE.\nNote The Managed ASM provisioning could take around ~10 min.\n",
    "description": "Duration: 15 min | Persona: Platform Admin",
    "tags": [
      "asm",
      "kcc",
      "platform-admin",
      "security-tips"
    ],
    "title": "Install ASM",
    "uri": "/acm-workshop/service-mesh/install-asm/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, you will monitor security features such as Network Policies and Service requests of your apps in the Google Cloud console.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh In the Google Cloud console, navigate to Anthos \u003e Security \u003e Policy Audit and filter for example by onlineboutique Namespace to see that the 3 security features Kubernetes Network policy, Service access control and mTLS status are enabled in green:\nClick on the link displayed by the command below:\necho -e \"https://console.cloud.google.com/anthos/security/policy-summary?project=${TENANT_PROJECT_ID}\" Select the onlineboutique Namespace on the Policy audit tab: Select the frontend Workload to open a more detailed view: From this view you could gain more visibility about Inbound denials or Outbound denials for both Network policy requests (NetworkPolicies) or Service requests (AuthorizationPolicies).\nYou could also leverage the gcloud commands below to get such insights.\nRun this command to get the Service requests denied for the last hour for the onlineboutique Namespace:\nfilter=\"resource.type=\\\"k8s_container\\\" \"\\ \"logName=\\\"projects/${TENANT_PROJECT_ID}/logs/server-accesslog-stackdriver\\\" \"\\ \"(httpRequest.status=\\\"403\\\" OR labels.response_details=\\\"AuthzDenied\\\") \"\\ \"labels.destination_namespace=\\\"${ONLINEBOUTIQUE_NAMESPACE}\\\"\" gcloud logging read --project $TENANT_PROJECT_ID --freshness 1h \"$filter\" Run this command to get the Network policy requests denied for the last hour for the onlineboutique Namespace:\nfilter=\"resource.type=\\\"k8s_node\\\" \"\\ \"logName=\\\"projects/${TENANT_PROJECT_ID}/logs/policy-action\\\" \"\\ \"jsonPayload.disposition=\\\"deny\\\" \"\\ \"jsonPayload.dest.pod_namespace=\\\"${ONLINEBOUTIQUE_NAMESPACE}\\\" \"\\ \"resource.labels.cluster_name=\\\"${GKE_NAME}\\\"\" gcloud logging read --project $TENANT_PROJECT_ID --freshness 1h \"$filter\" You could explore all of this for all the other Namespaces too.\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "asm",
      "monitoring",
      "security-tips"
    ],
    "title": "Monitor apps security",
    "uri": "/acm-workshop/monitoring-and-audit/monitor-apps-security/index.html"
  },
  {
    "content": "3 personas are involved:\nOrg Admin Platform Admin Apps Operator Here are the needs of each persona:\nInfo The Developer persona is not involved in this workshop for the reason that they should focus on the code of their apps. The Apps Operator persona is responsible to set up the Continuous Integration part to build and push the container images associated to any apps the Developers are building. With this workshop we are not covering this part, we are assuming that the container images are already built, the Apps Operator persona will take it from here in this workshop and will configure the Continuous Deployment part.\nHere is what the 3 personas will accomplish throughout this workshop:\n",
    "description": "",
    "tags": [
      "apps-operator",
      "org-admin",
      "platform-admin"
    ],
    "title": "Personas",
    "uri": "/acm-workshop/overview/personas/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Org Admin\nIn this section, you will set up a dedicated GitHub repository containing all the Kubernetes manifests which will be deployed by Config Sync and Config Connector in order to provision the Google Cloud services in the Tenant project.\nDefine variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export TENANT_PROJECT_DIR_NAME=acm-workshop-tenant-project-repo\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Create GitHub repository Create a dedicated GitHub repository to store any Kubernetes manifests associated to the Tenant project:\ncd ${WORK_DIR} gh repo create $TENANT_PROJECT_DIR_NAME --public --clone --template https://github.com/mathieu-benoit/config-sync-template-repo cd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME git pull git checkout main GKE_PLATFORM_REPO_URL=$(gh repo view --json url --jq .url) Define RepoSync Define a RepoSync linking this Git repository:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/gke-config-repo-sync.yaml apiVersion: configsync.gke.io/v1beta1 kind: RepoSync metadata: name: repo-sync namespace: ${TENANT_PROJECT_ID} spec: sourceFormat: unstructured git: repo: ${GKE_PLATFORM_REPO_URL} revision: HEAD branch: main dir: \".\" auth: none EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/gke-config-repo-sync-role-binding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: syncs-repo namespace: ${TENANT_PROJECT_ID} subjects: - kind: ServiceAccount name: ns-reconciler-${TENANT_PROJECT_ID} namespace: config-management-system roleRef: kind: ClusterRole name: edit apiGroup: rbac.authorization.k8s.io EOF Info We are using the edit role here, see more information about the user-facing roles here.\nDeploy Kubernetes manifests cd ${WORK_DIR}$HOST_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"GitOps for Tenant project\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in Config Controller for the Host project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Host project configs repository:\ncd ${WORK_DIR}$HOST_PROJECT_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 5 min | Persona: Org Admin",
    "tags": [
      "gitops-tips",
      "org-admin",
      "security-tips"
    ],
    "title": "Set up Tenant project's Git repo",
    "uri": "/acm-workshop/tenant-project/set-up-gke-project-git-repo/index.html"
  },
  {
    "content": " Duration: 10 min | Persona: Platform Admin\nIn this section, you will set up an uptime check on the Bank of Anthos website URL.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh mkdir ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/$BANKOFANTHOS_NAMESPACE Define Uptime check config Define the MonitoringUptimeCheckConfig:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/$BANKOFANTHOS_NAMESPACE/uptime-check-config.yaml apiVersion: monitoring.cnrm.cloud.google.com/v1beta1 kind: MonitoringUptimeCheckConfig metadata: name: uptimecheckconfig-${BANKOFANTHOS_NAMESPACE} spec: projectRef: name: ${TENANT_PROJECT_ID} displayName: ${BANKOFANTHOS_NAMESPACE} period: 900s timeout: 5s monitoredResource: type: \"uptime_url\" filterLabels: host: ${BANK_OF_ANTHOS_INGRESS_GATEWAY_HOST_NAME} project_id: ${TENANT_PROJECT_ID} httpCheck: port: 443 requestMethod: GET useSsl: true validateSsl: true EOF Deploy Kubernetes manifests cd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"URL uptime check for Bank of Anthos\" \u0026\u0026 git push origin main Check deployments graph TD; MonitoringUptimeCheckConfig-.-\u003eProject List the Kubernetes resources managed by Config Sync in Config Controller for the Tenant project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $TENANT_PROJECT_ID Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Tenant project configs repository:\ncd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 10 min | Persona: Platform Admin",
    "tags": [
      "kcc",
      "monitoring",
      "platform-admin"
    ],
    "title": "Set up URL uptime check",
    "uri": "/acm-workshop/bankofanthos/set-up-url-uptime-check/index.html"
  },
  {
    "content": " Duration: 10 min | Persona: Platform Admin\nIn this section, you will set up an uptime check on the Online Boutique website URL.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh mkdir -p ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/$ONLINEBOUTIQUE_NAMESPACE Define Uptime check config Define the MonitoringUptimeCheckConfig:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/$ONLINEBOUTIQUE_NAMESPACE/uptime-check-config.yaml apiVersion: monitoring.cnrm.cloud.google.com/v1beta1 kind: MonitoringUptimeCheckConfig metadata: name: uptimecheckconfig-${ONLINEBOUTIQUE_NAMESPACE} spec: projectRef: name: ${TENANT_PROJECT_ID} displayName: ${ONLINEBOUTIQUE_NAMESPACE} period: 900s timeout: 5s monitoredResource: type: \"uptime_url\" filterLabels: host: ${ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME} project_id: ${TENANT_PROJECT_ID} httpCheck: port: 443 requestMethod: GET useSsl: true validateSsl: true EOF Deploy Kubernetes manifests cd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"URL uptime check for Online Boutique\" \u0026\u0026 git push origin main Check deployments graph TD; MonitoringUptimeCheckConfig-.-\u003eProject List the Kubernetes resources managed by Config Sync in Config Controller for the Tenant project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $TENANT_PROJECT_ID Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Tenant project configs repository:\ncd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 10 min | Persona: Platform Admin",
    "tags": [
      "kcc",
      "monitoring",
      "platform-admin"
    ],
    "title": "Set up URL uptime check",
    "uri": "/acm-workshop/onlineboutique/set-up-url-uptime-check/index.html"
  },
  {
    "content": " Duration: 10 min | Persona: Platform Admin\nIn this section, you will set up an uptime check on the Whereami URL.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh mkdir ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/$WHEREAMI_NAMESPACE Define Uptime check config Define the MonitoringUptimeCheckConfig:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/$WHEREAMI_NAMESPACE/uptime-check-config.yaml apiVersion: monitoring.cnrm.cloud.google.com/v1beta1 kind: MonitoringUptimeCheckConfig metadata: name: uptimecheckconfig-${WHEREAMI_NAMESPACE} spec: projectRef: name: ${TENANT_PROJECT_ID} displayName: ${WHEREAMI_NAMESPACE} period: 900s timeout: 5s monitoredResource: type: \"uptime_url\" filterLabels: host: ${WHERE_AMI_INGRESS_GATEWAY_HOST_NAME} project_id: ${TENANT_PROJECT_ID} httpCheck: port: 443 requestMethod: GET useSsl: true validateSsl: true EOF Deploy Kubernetes manifests cd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"URL uptime check for Whereami\" \u0026\u0026 git push origin main Check deployments graph TD; MonitoringUptimeCheckConfig-.-\u003eProject List the Kubernetes resources managed by Config Sync in Config Controller for the Tenant project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $TENANT_PROJECT_ID Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Tenant project configs repository:\ncd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 10 min | Persona: Platform Admin",
    "tags": [
      "kcc",
      "monitoring",
      "platform-admin"
    ],
    "title": "Set up URL uptime check",
    "uri": "/acm-workshop/whereami/set-up-url-uptime-check/index.html"
  },
  {
    "content": "Set up the Tenant project which will host later a GKE cluster, Artifact Registry, Memorystore (Redis), Cloud Armor, etc.\nCreate Tenant projectDuration: 10 min | Persona: Org Admin\nSet up Tenant project's Git repoDuration: 5 min | Persona: Org Admin\nEnforce GCP resources policiesDuration: 5 min | Persona: Org Admin\nAllow MonitoringDuration: 2 min | Persona: Org Admin\nSet up MonitoringDuration: 5 min | Persona: Platform Admin\n",
    "description": "",
    "tags": null,
    "title": "2. Tenant project",
    "uri": "/acm-workshop/tenant-project/index.html"
  },
  {
    "content": " Host project As Org Admin, create a Host project As Org Admin, create a Config Controller instance As Org Admin, set up Host project’s Git repo As Org Admin, enforce policies for tenant projects Tenant project As Org Admin, set up the Tenant project As Org Admin, set up the Tenant project’s Git repo As Org Admin, enforce policies for Google Cloud resources As Org Admin, allow Monitoring for Tenant project As Platform Admin, set up Monitoring in Tenant project Networking As Org Admin, allow Networking for Tenant project As Platform Admin, set up Network in Tenant project GKE cluster As Org Admin, allow GKE for Tenant project As Org Admin, enforce policies for GKE cluster resources As Platform Admin, create GKE cluster in Tenant project As Org Admin, allow Fleet for Tenant project As Platform Admin, set up GKE configs’s Git repo in Tenant project As Platform Admin, enforce Kubernetes policies with Pod Security Admission (PSA) and NetworkPolicies As Platform Admin, set up NetworkPolicies logging in GKE cluster Artifact Registry As Org Admin, allow Artifact Registry for Tenant project As Platform Admin, create Artifact Registry in Tenant project and allow GKE cluster to pull containers As Platform Admin, enforce policies for Artifact Registry (allowed container registries) Service Mesh As Org Admin, allow ASM for Tenant project As Platform Admin, install Managed ASM in GKE cluster As Platform Admin, set up ASM configs in GKE cluster As Platform Admin, enforce policies for ASM Ingress Gateway As Platform Admin, create the Public static IP address for the Ingress Gateway As Org Admin, allow Cloud Armor for Tenant project As Platform Admin, set up Cloud Armor in Tenant project As Platform Admin, deploy the Ingress Gateway linked to Cloud Armor in GKE cluster As Platform Admin, deploy NetworkPolicies for the Ingress Gateway namespace in GKE cluster As Platform Admin, deploy AuthorizationPolicies for the Ingress Gateway namespace in GKE cluster Whereami app As Platform Admin, set up DNS for the Whereami app As Platform Admin, set up URL uptime check on the Whereami app As Platform Admin, configure Config Sync for the Whereami app in GKE cluster As Apps Operator, copy Whereami container in private Artifact Registry As Apps Operator, deploy the Whereami app in GKE cluster As Apps Operator, deploy AuthorizationPolicies for the Whereami namespace in GKE cluster As Apps Operator, deploy NetworkPolicies for the Whereami namespace in GKE cluster As Apps Operator, deploy Sidecars for the Whereami namespace in GKE cluster Online Boutique apps As Platform Admin, set up DNS for the Online Boutique website As Platform Admin, set up URL uptime check on the Online Boutique website As Platform Admin, allow Config Sync for the Online Boutique apps in GKE cluster As Platform Admin, configure Config Sync for the Online Boutique apps in GKE cluster As Apps Operator, copy Online Boutique containers in private Artifact Registry As Apps Operator, deploy the Online Boutique apps in GKE cluster As Apps Operator, deploy AuthorizationPolicies for the Online Boutique namespace in GKE cluster As Apps Operator, deploy NetworkPolicies for the Online Boutique namespace in GKE cluster As Apps Operator, deploy Sidecars for the Online Boutique namespace in GKE cluster Memorystore (Redis) As Org Admin, allow Memorystore (Redis) for Tenant project As Org Admin, enforce policies for Memorystore (Redis) resources As Platform Admin, create Memorystore (Redis) instances with and without TLS in Tenant project As Apps Operator, configure Online Boutique apps to use Memorystore (Redis) instance As Apps Operator, secure Online Boutique apps to access Memorystore (Redis) instance via TLS Spanner As Org Admin, allow Spanner for Tenant project As Platform Admin, create Spanner instance in Tenant project As Apps Operator, configure Online Boutique apps to use Spanner instance Bank of Anthos apps As Platform Admin, set up DNS for the Bank of Anthos website As Platform Admin, set up URL uptime check on the Bank of Anthos website As Platform Admin, configure Config Sync for the Bank of Anthos apps in GKE cluster As Apps Operator, copy Bank of Anthos containers in private Artifact Registry As Apps Operator, deploy the Bank of Anthos apps in GKE cluster As Apps Operator, deploy AuthorizationPolicies for the Bank of Anthos namespace in GKE cluster As Apps Operator, deploy NetworkPolicies for the Bank of Anthos namespace in GKE cluster As Apps Operator, deploy Sidecars for the Bank of Anthos namespace in GKE cluster Monitoring \u0026 Audit As Platform Admin, verify ASM versions As Apps Operator, monitor apps security As Apps Operator, monitor apps health As Apps Operator, trace apps As Apps Operator, monitor Cloud Armor (WAF) rules As Apps Operator, scan workloads and configurations As Apps Operator, monitor resources synced by Config Sync As Apps Operator, monitor policies violations by Policy Controller As Apps Operator, monitor URLs uptime checks ",
    "description": "",
    "tags": [
      "apps-operator",
      "org-admin",
      "platform-admin"
    ],
    "title": "Agenda",
    "uri": "/acm-workshop/overview/agenda/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Platform Admin\nIn this section, you will bind the workload identity capability from the Online Boutique’s RepoSync Kubernetes Service Account to the Artifact Registry reader Google Service Account created earlier.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export ONLINEBOUTIQUE_NAMESPACE=onlineboutique\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh mkdir ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/$ONLINEBOUTIQUE_NAMESPACE Bind the Artifact Registry reader GSA to the Online Boutique’s RepoSync cat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/$ONLINEBOUTIQUE_NAMESPACE/artifactregistry-charts-reader-workload-identity-user.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPartialPolicy metadata: name: ${HELM_CHARTS_READER_GSA}-${ONLINEBOUTIQUE_NAMESPACE} namespace: ${TENANT_PROJECT_ID} annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/IAMServiceAccount/${HELM_CHARTS_READER_GSA} spec: resourceRef: name: ${HELM_CHARTS_READER_GSA} kind: IAMServiceAccount bindings: - role: roles/iam.workloadIdentityUser members: - member: serviceAccount:${TENANT_PROJECT_ID}.svc.id.goog[config-management-system/ns-reconciler-${ONLINEBOUTIQUE_NAMESPACE}] EOF Deploy Kubernetes manifests cd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Artifact Registry viewer for Online Boutique's RepoSync\" \u0026\u0026 git push origin main Check deployments graph TD; IAMPartialPolicy-.-\u003eIAMServiceAccount List the Kubernetes resources managed by Config Sync in Config Controller for the Tenant project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $TENANT_PROJECT_ID Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\n",
    "description": "Duration: 5 min | Persona: Platform Admin",
    "tags": [
      "gitops-tips",
      "platform-admin"
    ],
    "title": "Allow Config Sync",
    "uri": "/acm-workshop/onlineboutique/allow-config-sync/index.html"
  },
  {
    "content": " Duration: 10 min | Persona: Platform Admin\nIn this section, you will set up a dedicated GitHub repository which will contain all the Kubernetes manifests of the Bank of Anthos apps. You will also have the opportunity to catch a policies violation.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export BANKOFANTHOS_NAMESPACE=bankofanthos\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh echo \"export BANK_OF_ANTHOS_DIR_NAME=acm-workshop-bankofanthos-repo\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh mkdir -p ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs mkdir ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs/$BANKOFANTHOS_NAMESPACE Define Namespace Define a dedicated Namespace for the Bank of Anthos apps:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs/$BANKOFANTHOS_NAMESPACE/namespace.yaml apiVersion: v1 kind: Namespace metadata: labels: istio-injection: enabled pod-security.kubernetes.io/enforce: baseline name: ${BANKOFANTHOS_NAMESPACE} EOF Note In addition to the istio-injection to include this Namespace into our Service Mesh, we are also adding the pod-security.kubernetes.io/enforce label as the baseline Pod Security Standards policy.\nCreate GitHub repository cd ${WORK_DIR} gh repo create $BANK_OF_ANTHOS_DIR_NAME --public --clone --template https://github.com/mathieu-benoit/config-sync-app-template-repo cd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME git pull git checkout main BANK_OF_ANTHOS_REPO_URL=$(gh repo view --json url --jq .url) Define RepoSync Define a RepoSync linking this Git repository:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs/$BANKOFANTHOS_NAMESPACE/repo-sync.yaml apiVersion: configsync.gke.io/v1beta1 kind: RepoSync metadata: name: repo-sync namespace: ${BANKOFANTHOS_NAMESPACE} spec: sourceFormat: unstructured git: repo: ${BANK_OF_ANTHOS_REPO_URL} revision: HEAD branch: main dir: staging auth: none EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs/$BANKOFANTHOS_NAMESPACE/repo-sync-role-binding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: repo-sync namespace: ${BANKOFANTHOS_NAMESPACE} subjects: - kind: ServiceAccount name: ns-reconciler-${BANKOFANTHOS_NAMESPACE} namespace: config-management-system roleRef: kind: ClusterRole name: edit apiGroup: rbac.authorization.k8s.io EOF Tip We are using the edit user-facing role here, to follow the least privilege principle. Earlier in this workshop during the ASM installation, we extended the default edit role with more capabilities regarding to the Istio resources: VirtualServices, Sidecars and AuthorizationPolicies which will be leveraged in the Bank of Anthos’s namespace.\nDeploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Configure Config Sync for Bank of Anthos\" \u0026\u0026 git push origin main Check Policies violation List the Kubernetes resources managed by Config Sync in GKE cluster for the GKE cluster configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 10 min | Persona: Platform Admin",
    "tags": [
      "asm",
      "gitops-tips",
      "platform-admin",
      "security-tips"
    ],
    "title": "Configure Config Sync",
    "uri": "/acm-workshop/bankofanthos/configure-config-sync/index.html"
  },
  {
    "content": " Duration: 10 min | Persona: Platform Admin\nIn this section, you will configure Config Sync to sync the resources in the Whereami Namespace via its associated RepoSync and RoleBinding.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export WHEREAMI_NAMESPACE=whereami\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh echo \"export WHERE_AMI_DIR_NAME=acm-workshop-whereami-repo\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh mkdir -p ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs mkdir ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs/$WHEREAMI_NAMESPACE Create Namespace Define a dedicated Namespace for the Whereami app:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs/$WHEREAMI_NAMESPACE/namespace.yaml apiVersion: v1 kind: Namespace metadata: labels: istio-injection: enabled pod-security.kubernetes.io/enforce: baseline name: ${WHEREAMI_NAMESPACE} EOF Note In addition to the istio-injection to include this Namespace into our Service Mesh, we are also adding the pod-security.kubernetes.io/enforce label as the baseline Pod Security Standards policy.\nCreate GitHub repository cd ${WORK_DIR} gh repo create $WHERE_AMI_DIR_NAME --public --clone --template https://github.com/mathieu-benoit/config-sync-app-template-repo cd ${WORK_DIR}$WHERE_AMI_DIR_NAME git pull git checkout main WHERE_AMI_REPO_URL=$(gh repo view --json url --jq .url) Define RepoSync Define a RepoSync linking this Git repository:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs/$WHEREAMI_NAMESPACE/repo-sync.yaml apiVersion: configsync.gke.io/v1beta1 kind: RepoSync metadata: name: repo-sync namespace: ${WHEREAMI_NAMESPACE} spec: sourceFormat: unstructured git: repo: ${WHERE_AMI_REPO_URL} revision: HEAD branch: main dir: staging auth: none EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs/$WHEREAMI_NAMESPACE/repo-sync-role-binding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: repo-sync namespace: ${WHEREAMI_NAMESPACE} subjects: - kind: ServiceAccount name: ns-reconciler-${WHEREAMI_NAMESPACE} namespace: config-management-system roleRef: kind: ClusterRole name: edit apiGroup: rbac.authorization.k8s.io EOF Tip We are using the edit user-facing role here, to follow the least privilege principle. Earlier in this workshop during the ASM installation, we extended the default edit role with more capabilities regarding to the Istio resources: VirtualServices, Sidecars and AuthorizationPolicies which will be leveraged in the Whereami’s namespace.\nDeploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Configure Config Sync for Whereami\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the GKE cluster configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 10 min | Persona: Platform Admin",
    "tags": [
      "asm",
      "gitops-tips",
      "platform-admin",
      "security-tips"
    ],
    "title": "Configure Config Sync",
    "uri": "/acm-workshop/whereami/configure-config-sync/index.html"
  },
  {
    "content": " Duration: 20 min | Persona: Platform Admin\nIn this section, you will set up a secured GKE cluster including features like: workload identity, least privilege service account for the nodes, Dataplane V2, private nodes, confidential and shielded nodes, etc.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export TENANT_PROJECT_NUMBER=$(gcloud projects describe $TENANT_PROJECT_ID --format='get(projectNumber)')\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh echo \"export GKE_SA=gke-primary-pool\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Define GKE cluster Define the GKE cluster with empty node pool:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/gke-cluster.yaml apiVersion: container.cnrm.cloud.google.com/v1beta1 kind: ContainerCluster metadata: name: ${GKE_NAME} namespace: ${TENANT_PROJECT_ID} annotations: cnrm.cloud.google.com/project-id: ${TENANT_PROJECT_ID} cnrm.cloud.google.com/remove-default-node-pool: \"true\" config.kubernetes.io/depends-on: compute.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/ComputeSubnetwork/${GKE_NAME} labels: mesh_id: proj-${TENANT_PROJECT_NUMBER} spec: addonsConfig: gcePersistentDiskCsiDriverConfig: enabled: true httpLoadBalancing: disabled: false confidentialNodes: enabled: true datapathProvider: ADVANCED_DATAPATH dnsConfig: clusterDns: CLOUD_DNS clusterDnsScope: CLUSTER_SCOPE enableShieldedNodes: true initialNodeCount: 1 ipAllocationPolicy: servicesSecondaryRangeName: servicesrange clusterSecondaryRangeName: clusterrange location: ${GKE_LOCATION} loggingConfig: enableComponents: - \"SYSTEM_COMPONENTS\" - \"WORKLOADS\" monitoringConfig: enableComponents: - \"SYSTEM_COMPONENTS\" networkingMode: VPC_NATIVE networkRef: name: gke nodeConfig: machineType: n2d-standard-4 privateClusterConfig: enablePrivateEndpoint: false enablePrivateNodes: true masterIpv4CidrBlock: 172.16.0.0/28 releaseChannel: channel: RAPID subnetworkRef: name: gke workloadIdentityConfig: workloadPool: ${TENANT_PROJECT_ID}.svc.id.goog EOF Define GKE primary node pool’s service account Define the GKE primary node pool’s Google Service Account:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/gke-primary-pool-sa.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMServiceAccount metadata: annotations: cnrm.cloud.google.com/project-id: ${TENANT_PROJECT_ID} name: ${GKE_SA} namespace: ${TENANT_PROJECT_ID} spec: displayName: ${GKE_SA} EOF Define the logging.logWriter, monitoring.metricWriter and monitoring.viewer roles with an IAMPolicyMember resource for the GKE primary node pool’s service account:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/log-writer-gke-sa.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: log-writer namespace: ${TENANT_PROJECT_ID} annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/IAMServiceAccount/${GKE_SA} spec: memberFrom: serviceAccountRef: name: ${GKE_SA} namespace: ${TENANT_PROJECT_ID} resourceRef: kind: Project external: ${TENANT_PROJECT_ID} role: roles/logging.logWriter EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/metric-writer-gke-sa.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: metric-writer namespace: ${TENANT_PROJECT_ID} annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/IAMServiceAccount/${GKE_SA} spec: memberFrom: serviceAccountRef: name: ${GKE_SA} namespace: ${TENANT_PROJECT_ID} resourceRef: kind: Project external: ${TENANT_PROJECT_ID} role: roles/monitoring.metricWriter EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/monitoring-viewer-gke-sa.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: monitoring-viewer namespace: ${TENANT_PROJECT_ID} annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/IAMServiceAccount/${GKE_SA} spec: memberFrom: serviceAccountRef: name: ${GKE_SA} namespace: ${TENANT_PROJECT_ID} resourceRef: kind: Project external: ${TENANT_PROJECT_ID} role: roles/monitoring.viewer EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/cloudtrace-agent-gke-sa.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: cloudtrace-agent namespace: ${TENANT_PROJECT_ID} annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/IAMServiceAccount/${GKE_SA} spec: memberFrom: serviceAccountRef: name: ${GKE_SA} namespace: ${TENANT_PROJECT_ID} resourceRef: kind: Project external: ${TENANT_PROJECT_ID} role: roles/cloudtrace.agent EOF Define GKE primary node pool Define the GKE primary node pool:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/gke-primary-pool.yaml apiVersion: container.cnrm.cloud.google.com/v1beta1 kind: ContainerNodePool metadata: name: ${GKE_NAME}-primary namespace: ${TENANT_PROJECT_ID} annotations: config.kubernetes.io/depends-on: container.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/ContainerCluster/${GKE_NAME} spec: clusterRef: name: ${GKE_NAME} location: ${GKE_LOCATION} management: autoRepair: true autoUpgrade: true nodeConfig: imageType: COS_CONTAINERD diskSizeGb: 100 diskType: pd-ssd labels: gke.io/nodepool: primary machineType: n2d-standard-4 oauthScopes: - https://www.googleapis.com/auth/cloud-platform shieldedInstanceConfig: enableIntegrityMonitoring: true enableSecureBoot: true serviceAccountRef: name: ${GKE_SA} nodeCount: 3 EOF Deploy Kubernetes manifests cd ~/$TENANT_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"GKE cluster, primary nodepool and SA for Tenant project\" \u0026\u0026 git push origin main Check deployments graph TD; IAMServiceAccount-.-\u003eProject ContainerNodePool--\u003eContainerCluster ContainerNodePool--\u003eIAMServiceAccount IAMPolicyMember--\u003eIAMServiceAccount IAMPolicyMember-.-\u003eProject IAMPolicyMember--\u003eIAMServiceAccount IAMPolicyMember-.-\u003eProject IAMPolicyMember--\u003eIAMServiceAccount IAMPolicyMember-.-\u003eProject IAMPolicyMember--\u003eIAMServiceAccount IAMPolicyMember-.-\u003eProject ContainerCluster-.-\u003eComputeSubnetwork List the Kubernetes resources managed by Config Sync in Config Controller for the Tenant project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $TENANT_PROJECT_ID Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nNote The creation of the ContainerCluster and ContainerNodePool can take ~15 mins.\nList the GitHub runs for the Tenant project configs repository:\ncd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud projects get-iam-policy $TENANT_PROJECT_ID \\ --filter=\"bindings.members:${GKE_SA}@${TENANT_PROJECT_ID}.iam.gserviceaccount.com\" \\ --flatten=\"bindings[].members\" \\ --format=\"table(bindings.role)\" gcloud container clusters list \\ --project $TENANT_PROJECT_ID gcloud container node-pools list \\ --cluster $GKE_NAME \\ --project $TENANT_PROJECT_ID \\ --region $GKE_LOCATION Wait and re-run this command above until you see the resources created.\n",
    "description": "Duration: 20 min | Persona: Platform Admin",
    "tags": [
      "gke",
      "kcc",
      "platform-admin",
      "security-tips"
    ],
    "title": "Create GKE cluster",
    "uri": "/acm-workshop/gke-cluster/create-gke-cluster/index.html"
  },
  {
    "content": " Duration: 10 min | Persona: Platform Admin\nIn this section, you will create a Memorystore (Redis) instance for the Online Boutique’s cartservice app to connect to. We will also create a second Memorystore (Redis) with TLS enabled which will be leveraged in another section.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export REDIS_NAME=cart\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh echo \"export REDIS_TLS_NAME=cart-tls\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Define Memorystore (Redis) Define the Memorystore (Redis) resource:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/$ONLINEBOUTIQUE_NAMESPACE/memorystore.yaml apiVersion: redis.cnrm.cloud.google.com/v1beta1 kind: RedisInstance metadata: annotations: cnrm.cloud.google.com/project-id: ${TENANT_PROJECT_ID} config.kubernetes.io/depends-on: compute.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/ComputeNetwork/${GKE_NAME} name: ${REDIS_NAME} namespace: ${TENANT_PROJECT_ID} spec: authorizedNetworkRef: name: ${GKE_NAME} memorySizeGb: 1 redisVersion: REDIS_6_X region: ${GKE_LOCATION} tier: BASIC EOF Define Memorystore (Redis) with TLS enabled Define the Memorystore (Redis) resource with TLS enabled:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/$ONLINEBOUTIQUE_NAMESPACE/memorystore-tls.yaml apiVersion: redis.cnrm.cloud.google.com/v1beta1 kind: RedisInstance metadata: annotations: cnrm.cloud.google.com/project-id: ${TENANT_PROJECT_ID} config.kubernetes.io/depends-on: compute.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/ComputeNetwork/${GKE_NAME} name: ${REDIS_TLS_NAME} namespace: ${TENANT_PROJECT_ID} spec: authorizedNetworkRef: name: ${GKE_NAME} memorySizeGb: 1 redisVersion: REDIS_6_X region: ${GKE_LOCATION} tier: BASIC transitEncryptionMode: SERVER_AUTHENTICATION EOF Deploy Kubernetes manifests cd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Memorystore (Redis) instance\" \u0026\u0026 git push origin main Check deployments graph TD; RedisInstance-.-\u003eProject RedisInstance-.-\u003eComputeNetwork RedisInstance-.-\u003eProject RedisInstance-.-\u003eComputeNetwork List the Kubernetes resources managed by Config Sync in Config Controller for the Tenant project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $TENANT_PROJECT_ID Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nNote The creation of the RedisInstance can take ~10 mins.\nList the GitHub runs for the Tenant project configs repository:\ncd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud redis instances list \\ --region=$GKE_LOCATION \\ --project=$TENANT_PROJECT_ID ",
    "description": "Duration: 10 min | Persona: Platform Admin",
    "tags": [
      "kcc",
      "platform-admin"
    ],
    "title": "Create Memorystore",
    "uri": "/acm-workshop/onlineboutique/memorystore/create-memorystore/index.html"
  },
  {
    "content": " Duration: 10 min | Persona: Platform Admin\nIn this section, you will enforce policies in order to make sure that the containers in your clusters are coming from a restricted list of container registries.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Define “Allowed container registries” policy Define the Constraint based on the K8sAllowedRepos ConstraintTemplate for Pods:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/policies/constraints/pod-allowed-container-registries.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sAllowedRepos metadata: name: pod-allowed-container-registries annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Requires container images to begin with a string from the specified list.', remediation: 'Any container images should begin with a string from the specified list, they are the only container registries allowed.' }\" spec: enforcementAction: deny match: kinds: - apiGroups: - \"\" kinds: - Pod parameters: repos: - auto - gcr.io/config-management-release - gcr.io/gke-release - gke.gcr.io - k8s.gcr.io - ${CONTAINER_REGISTRY_REPOSITORY} EOF Tip We are restricting the source of the container images in the GKE cluster. Only system container images and the images from your own private Artifact Registry can be deployed in your GKE cluster.\nDeploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Policies for Artifact Registry\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the GKE cluster configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\".\nSee the Policy Controller Constraints without any violations in the GKE cluster, by running this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/policy_controller/dashboard?project=${TENANT_PROJECT_ID}\" List the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 10 min | Persona: Platform Admin",
    "tags": [
      "platform-admin",
      "policies",
      "security-tips"
    ],
    "title": "Enforce Artifact Registry policies",
    "uri": "/acm-workshop/artifact-registry/enforce-artifact-registry-policies/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Org Admin\nIn this section, you will set up policies in order to enforce governance against the Kubernetes manifests defining your Google Cloud services. As an example, you will limit the locations and the kind available for the Google Cloud services.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export GKE_LOCATION=northamerica-northeast1\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Info We are defining the GKE_LOCATION in northamerica-northeast1 this will be used later for the location of the VPC, GKE, Artifact Registry, etc. in the Tenant project. We are using this region because that’s the greenest Google Cloud region (Low CO2) in the regions supported by GKE Confidential Nodes used in this workshop.\nDefine “Allowed KCC resources” policies Define the ConstraintTemplate resource:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/policies/templates/allowedkccresources.yaml apiVersion: templates.gatekeeper.sh/v1 kind: ConstraintTemplate metadata: name: allowedkccresources annotations: description: \"Requirements for any KCC resources.\" spec: crd: spec: names: kind: AllowedKccResources validation: legacySchema: false openAPIV3Schema: properties: allowedKinds: items: type: string type: array type: object targets: - target: admission.k8s.gatekeeper.sh rego: |- package allowedkccresources violation[{\"msg\": msg}] { _matches_group(input.review.kind.group) objectKind := input.review.kind.kind not _matches_kind(input.parameters.allowedKinds, objectKind) msg := sprintf(\"KCC resource of kind: %v is not allowed\", [objectKind]) } _matches_group(group) { endswith(group, \".cnrm.cloud.google.com\") not group == \"core.cnrm.cloud.google.com\" } _matches_kind(allowedKinds, objectKind) { allowedKinds[_] = objectKind } EOF Define the Constraint resource:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/policies/constraints/allowed-kcc-resources.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: AllowedKccResources metadata: name: allowedkccresources annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Requires Config Connector resources kind to be in the specified list.', remediation: 'Any Config Connector resources kind should be in the specified list, they are the only Config Connector resources kind allowed.' }\" spec: enforcementAction: deny parameters: allowedKinds: - ArtifactRegistryRepository - ComputeAddress - ComputeNetwork - ComputeRouter - ComputeRouterNAT - ComputeSecurityPolicy - ComputeSSLPolicy - ComputeSubnetwork - ContainerCluster - ContainerNodePool - GKEHubFeature - GKEHubFeatureMembership - GKEHubMembership - IAMPartialPolicy - IAMPolicyMember - IAMServiceAccount - MonitoringAlertPolicy - MonitoringNotificationChannel - MonitoringUptimeCheckConfig - Project - RedisInstance - Service - SpannerDatabase - SpannerInstance EOF Define “Allowed GCP locations” policies Define the ConstraintTemplate resource:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/policies/templates/limitlocations.yaml apiVersion: templates.gatekeeper.sh/v1 kind: ConstraintTemplate metadata: name: limitlocations annotations: description: \"Allowed GCP locations.\" spec: crd: spec: names: kind: LimitLocations validation: openAPIV3Schema: type: object properties: locations: description: List of allowed GCP locations type: array items: type: string targets: - target: admission.k8s.gatekeeper.sh rego: |- package limitlocations violation[{\"msg\":msg}] { contains(input.review.object.apiVersion, \"cnrm.cloud.google.com\") not allowedLocation(input.review.object.spec.location) msg := sprintf(\"%s %s uses a disallowed location: %s, authorized locations are: %s\", [input.review.object.kind, input.review.object.metadata.name, input.review.object.spec.location, input.parameters.locations]) } violation[{\"msg\":msg}] { contains(input.review.object.apiVersion, \"cnrm.cloud.google.com\") not allowedLocation(input.review.object.spec.region) msg := sprintf(\"%s %s uses a disallowed location: %s, authorized locations are: %s\", [input.review.object.kind, input.review.object.metadata.name, input.review.object.spec.region, input.parameters.locations]) } allowedLocation(reviewLocation) { locations := input.parameters.locations satisfied := [good | location = locations[_] good = lower(location) == lower(reviewLocation) ] any(satisfied) } EOF Define the Constraint resource:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/policies/constraints/allowed-locations.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: LimitLocations metadata: name: allowed-locations annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Requires Config Connector resources location to be in the specified list.', remediation: 'Any Config Connector resources location should be in the specified list, they are the only Config Connector resources location allowed.' }\" spec: enforcementAction: deny parameters: locations: - \"northamerica-northeast1\" - \"global\" EOF Deploy Kubernetes manifests cd ${WORK_DIR}$HOST_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Enforce policies for GCP resources\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in Config Controller for the Host project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Host project configs repository:\ncd ${WORK_DIR}$HOST_PROJECT_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 5 min | Persona: Org Admin",
    "tags": [
      "org-admin",
      "policies",
      "security-tips"
    ],
    "title": "Enforce GCP resources policies",
    "uri": "/acm-workshop/tenant-project/enforce-gcp-resources-policies/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, you will navigate to the topology of your Service Mesh as well as monitor your apps in terms of security, health and performance.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh In the Google Cloud console, navigate to Anthos \u003e Service Mesh \u003e Topology to see the topology graph of your Service Mesh: Click on the link displayed by the command below:\necho -e \"https://console.cloud.google.com/anthos/services?project=${TENANT_PROJECT_ID}\u0026pageState=%28%22topologyViewToggle%22:%28%22value%22:%22graph%22%29%29\" Select the Online Boutique’s frontend app. On your right, click on –\u003e Go to service dashboard: On the left, click the Metrics tab where you could get more insights about the golden signals of the frontend app: On the left, click the Security tab where you could get more insights about the Service requests (AuthorizationPolicy) of the frontend app: From there you will have access to a lot more monitoring features out of the box, feel free to discover these features and play with them for the different apps in your Service Mesh.\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "asm",
      "apps-operator",
      "monitoring",
      "security-tips"
    ],
    "title": "Monitor apps health",
    "uri": "/acm-workshop/monitoring-and-audit/monitor-apps-health/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Platform Admin\nIn this section, you will set up some configurations in order to get more insights with Cloud Trace and use the distroless image for your sidecar proxies.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh ASM_VERSION=asm-managed-rapid echo \"export ASM_VERSION=${ASM_VERSION}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Info ASM_VERSION is set to asm-managed-rapid because the Managed ASM is following the GKE’s channel: rapid.\nCreate a dedicated istio-system folder in the GKE configs’s Git repo:\nmkdir ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/istio-system Define ASM configs Mesh-wide Define the optional Mesh configs:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/istio-system/mesh-configs.yaml apiVersion: v1 data: mesh: |- defaultConfig: image: imageType: distroless tracing: stackdriver: {} discoverySelectors: - matchLabels: istio-injection: enabled kind: ConfigMap metadata: name: istio-${ASM_VERSION} namespace: istio-system EOF Tip The distroless base image ensures that the proxy image contains the minimal number of packages required to run the proxy. This improves security posture by reducing the overall attack surface of the image and gets cleaner results with CVE scanners.\nTip discoverySelectors is a way to dynamically restrict the set of namespaces that are part of the mesh so that the Istio control plane only processes resources in those namespaces.\nDefine mTLS STRICT Mesh-wide Define the mTLS STRICT policy Mesh-wide:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/istio-system/mesh-mtls.yaml apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default namespace: istio-system spec: mtls: mode: STRICT EOF Tip Here we are locking down mutual TLS to STRICT for the entire mesh.\nDefine Sidecar Mesh-wide Define the default Sidecar in the istio-system Namespace:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/istio-system/sidecar.yaml apiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: default namespace: istio-system spec: egress: - hosts: - ./* - istio-system/* EOF Tip A Sidecar configuration in the istio-system Namespace will be applied by default to all Namespaces.\nDefine default deny-all AuthorizationPolicy Mesh-wide Define the default deny-all AuthorizationPolicy in the istio-system Namespace:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/istio-system/authorizationpolicy_denyall.yaml apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: deny-all namespace: istio-system spec: {} EOF Define new ClusterRole with Istio capabilities for Config Sync Define the extended edit user-facing role with more Istio resources capabilities:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/custom-edit-clusterrole-istio.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: rbac.authorization.k8s.io/aggregate-to-edit: \"true\" name: custom:aggregate-to-edit:istio rules: - apiGroups: - \"networking.istio.io\" - \"security.istio.io\" resources: - \"virtualservices\" - \"authorizationpolicies\" - \"sidecars\" - \"serviceentries\" - \"destinationrules\" verbs: - \"*\" EOF Tip Later in this workshop, for each app namespace, we will define a Config Sync’s RepoSync which will be bound to the edit ClusterRole. With that new extension, it will allow each namespace to deploy Istio resources such as Sidecar, VirtualService, AuthorizationPolicy, ServiceEntry and DestinationRule while meeting with the least privilege principle requirement.\nDeploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"ASM Mesh configs in GKE cluster\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the GKE cluster configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 5 min | Persona: Platform Admin",
    "tags": [
      "asm",
      "gitops-tips",
      "platform-admin",
      "security-tips"
    ],
    "title": "Set up ASM configs",
    "uri": "/acm-workshop/service-mesh/set-up-asm-configs/index.html"
  },
  {
    "content": " Duration: 10 min | Persona: Platform Admin\nIn this section, you will set up Cloud Armor preconfigured WAF rules such as: SQL injection, local/remote file inclusion, etc.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh SECURITY_POLICY_NAME=$GKE_NAME-asm-ingressgateway echo \"export SECURITY_POLICY_NAME=${SECURITY_POLICY_NAME}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh echo \"export SSL_POLICY_NAME=${SECURITY_POLICY_NAME}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Define Cloud Armor rules Define the Ingress Gateway’s Cloud Armor rules:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/cloud-armor.yaml apiVersion: compute.cnrm.cloud.google.com/v1beta1 kind: ComputeSecurityPolicy metadata: annotations: cnrm.cloud.google.com/project-id: ${TENANT_PROJECT_ID} name: ${SECURITY_POLICY_NAME} namespace: ${TENANT_PROJECT_ID} spec: adaptiveProtectionConfig: layer7DdosDefenseConfig: enable: true advancedOptionsConfig: logLevel: VERBOSE rule: - action: allow description: \"Default rule\" match: versionedExpr: SRC_IPS_V1 config: srcIpRanges: - \"*\" priority: 2147483647 - action: deny(403) description: \"XSS\" match: expr: expression: \"evaluatePreconfiguredExpr('xss-v33-canary')\" priority: 1000 - action: deny(403) description: \"SQL injection level 2\" match: expr: expression: \"evaluatePreconfiguredWaf('sqli-v33-canary', {'sensitivity': 2, 'opt_out_rule_ids': ['owasp-crs-v030301-id942200-sqli', 'owasp-crs-v030301-id942260-sqli', 'owasp-crs-v030301-id942430-sqli']})\" priority: 2000 - action: deny(403) description: \"Local file inclusion\" match: expr: expression: \"evaluatePreconfiguredExpr('lfi-v33-canary')\" priority: 3000 - action: deny(403) description: \"Remote file inclusion\" match: expr: expression: \"evaluatePreconfiguredExpr('rfi-v33-canary')\" priority: 4000 - action: deny(403) description: \"CVE-2021-44228 and CVE-2021-45046\" match: expr: expression: \"evaluatePreconfiguredExpr('cve-canary')\" priority: 12345 - action: deny(403) description: \"Remote code execution\" match: expr: expression: \"evaluatePreconfiguredExpr('rce-v33-canary')\" priority: 5000 - action: deny(403) description: \"Method enforcement\" match: expr: expression: \"evaluatePreconfiguredExpr('methodenforcement-v33-canary')\" priority: 6000 - action: deny(403) description: \"Scanner detection\" match: expr: expression: \"evaluatePreconfiguredExpr('scannerdetection-v33-canary')\" priority: 7000 - action: deny(403) description: \"Protocol attack\" match: expr: expression: \"evaluatePreconfiguredExpr('protocolattack-v33-canary')\" priority: 8000 - action: deny(403) description: \"PHP injection attack\" match: expr: expression: \"evaluatePreconfiguredExpr('php-v33-canary')\" priority: 9000 - action: deny(403) description: \"Session fixation attack\" match: expr: expression: \"evaluatePreconfiguredExpr('sessionfixation-v33-canary')\" priority: 10000 - action: deny(403) description: \"Java attack\" match: expr: expression: \"evaluatePreconfiguredExpr('java-v33-canary')\" priority: 11000 - action: deny(403) description: \"NodeJS attack\" match: expr: expression: \"evaluatePreconfiguredExpr('nodejs-v33-canary')\" priority: 12000 EOF Info Here we are leveraging the Cloud Armor preconfigured WAF rules: xss, sqli, lfi, rfi, cve, rce, methodenforcement, scannerdetection, protocolattack, php, sessionfixation, java and nodejs. All of them in canary version to have the latest version and ModSecurity Core Rule Set (CRS) 3.3. For sqli, we are only using sensitivity level 2 and exluding some of its rules, otherwise the Bank of Anthos is not working properly.\nDefine SSL policy Not directly related to Cloud Armor, but let’s define an SSL policy which will allow us to set an HTTP to HTTPS redirect on the Ingress.\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/ssl-policy.yaml apiVersion: compute.cnrm.cloud.google.com/v1beta1 kind: ComputeSSLPolicy metadata: annotations: cnrm.cloud.google.com/project-id: ${TENANT_PROJECT_ID} name: ${SSL_POLICY_NAME} namespace: ${TENANT_PROJECT_ID} spec: minTlsVersion: TLS_1_0 profile: COMPATIBLE EOF Deploy Kubernetes manifests cd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Ingress Gateway's Cloud Armor rules and SSL policy\" \u0026\u0026 git push origin main Check deployments graph TD; ComputeSecurityPolicy-.-\u003eProject ComputeSSLPolicy-.-\u003eProject List the Kubernetes resources managed by Config Sync in Config Controller for the Tenant project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $TENANT_PROJECT_ID Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Tenant project configs repository:\ncd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud compute security-policies list \\ --project $TENANT_PROJECT_ID gcloud compute ssl-policies list \\ --project $TENANT_PROJECT_ID Wait and re-run this command above until you see the resources created.\n",
    "description": "Duration: 10 min | Persona: Platform Admin",
    "tags": [
      "kcc",
      "platform-admin",
      "security-tips"
    ],
    "title": "Set up Cloud Armor",
    "uri": "/acm-workshop/ingress-gateway/set-up-cloud-armor/index.html"
  },
  {
    "content": " Duration: 10 min | Persona: Org Admin\nIn this section, you will set up the primary Git repository of the Config Controller instance in order to have in place a GitOps approach to deploy your infrastructure in Google Cloud. You will also configure a Cloud NAT to this Config Controller instance to give it access to the Internet (GitHub repositories) in Egress. Finally, you will enable the cloudbilling API in the Host project, which will allow the assignment of the Billing Account Id to any Google Cloud project Config Controller will create.\nDefine variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export HOST_PROJECT_DIR_NAME=acm-workshop-org-repo\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Set up Cloud NAT Open Config Controller’s egress to the Internet (GitHub access):\nCONFIG_CONTROLLER_NAT_ROUTER_NAME=nat-router gcloud compute routers create $CONFIG_CONTROLLER_NAT_ROUTER_NAME \\ --network $CONFIG_CONTROLLER_NETWORK \\ --region $CONFIG_CONTROLLER_LOCATION CONFIG_CONTROLLER_NAT_CONFIG_NAME=nat-config gcloud compute routers nats create $CONFIG_CONTROLLER_NAT_CONFIG_NAME \\ --router-region $CONFIG_CONTROLLER_LOCATION \\ --router $CONFIG_CONTROLLER_NAT_ROUTER_NAME \\ --nat-all-subnet-ip-ranges \\ --auto-allocate-nat-external-ips Customize Policy Controller Customize setup for the Config Controller’s Config Management component:\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: configmanagement.gke.io/v1 kind: ConfigManagement metadata: name: config-management spec: enableMultiRepo: true policyController: enabled: true logDeniesEnabled: true referentialRulesEnabled: true templateLibraryInstalled: true preventDrift: true EOF Info We explicitly set the Policy Controller’s referentialRulesEnabled field to true and logDeniesEnabled field to true, the others are enabled by default.\nDefine the Host project’s Git repository Create a dedicated private GitHub repository to store any Kubernetes manifests associated to the Host project:\ncd ${WORK_DIR} gh auth login gh repo create $HOST_PROJECT_DIR_NAME --private --clone --template https://github.com/mathieu-benoit/config-sync-template-repo cd $HOST_PROJECT_DIR_NAME git pull git checkout main ORG_REPO_URL=$(gh repo view --json sshUrl --jq .sshUrl) ORG_REPO_NAME_WITH_OWNER=$(gh repo view --json nameWithOwner --jq .nameWithOwner) Generate SSH key pair in order to get a read access to the private Git repository:\nmkdir tmp ssh-keygen -t rsa -b 4096 \\ -C \"${ORG_REPO_NAME_WITH_OWNER}@github\" \\ -N '' \\ -f ./tmp/github-org-repo kubectl create secret generic git-creds \\ -n config-management-system \\ --from-file ssh=./tmp/github-org-repo gh repo deploy-key add ./tmp/github-org-repo.pub rm -r tmp Deploy a RootSync linking this GitHub repository to the Config Controller instance as the main/root GitOps configuration:\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: configsync.gke.io/v1beta1 kind: RootSync metadata: name: root-sync namespace: config-management-system spec: sourceFormat: unstructured git: repo: ${ORG_REPO_URL} revision: HEAD branch: main dir: . auth: ssh secretRef: name: git-creds EOF Tip The GitHub repository is private in order to demonstrate how to allow read access to Config Sync when you use a private Git repository.\nSince you started this workshop, you just ran 6 kubectl commands. For your information, moving forward you won’t run any other kubectl commands because the design and intent of this workshop is to only deploy any Kubernetes resources via GitOps with Config Sync. You will also use some handy gcloud commands when appropriate.\nDefine API In order to have Config Controller’s Config Sync linking a Billing Account to GCP projects later in this workshop, we need to define the Cloud Billing API Service resource for Config Controller’s GCP project:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/cloudbilling-service.yaml apiVersion: serviceusage.cnrm.cloud.google.com/v1beta1 kind: Service metadata: annotations: cnrm.cloud.google.com/deletion-policy: \"abandon\" cnrm.cloud.google.com/disable-dependent-services: \"false\" name: cloudbilling.googleapis.com namespace: config-control EOF Deploy Kubernetes manifests cd ${WORK_DIR}$HOST_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Billing API in Host project\" \u0026\u0026 git push origin main Info Because it’s the first git commit of this workshop, if you don’t have your own environment set up with git, you may be prompted to properly set up git config --global user.email \"you@example.com\" and git config --global user.name \"Your Name\".\nCheck deployments List the GitHub runs for the Host project configs repository:\ncd ${WORK_DIR}$HOST_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Kubernetes resources managed by Config Sync in Config Controller for the Host project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\n",
    "description": "Duration: 10 min | Persona: Org Admin",
    "tags": [
      "gitops-tips",
      "kcc",
      "org-admin",
      "security-tips"
    ],
    "title": "Set up Host project's Git repo",
    "uri": "/acm-workshop/host-project/set-up-host-project-git-repo/index.html"
  },
  {
    "content": " Duration: 10 min | Persona: Apps Operator\nIn this section, you will update the OnlineBoutique’s cartservice app in order to point to the Spanner database previously created.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Update RepoSync to deploy the Online Boutique’s Helm chart Get the Spanner database connection information:\nexport SPANNER_CONNECTION_STRING=projects/${TENANT_PROJECT_ID}/instances/${SPANNER_INSTANCE_NAME}/databases/${SPANNER_DATABASE_NAME} export SPANNER_DB_USER_GSA_ID=${SPANNER_DATABASE_USER_GSA_NAME}@${TENANT_PROJECT_ID}.iam.gserviceaccount.com Define the RepoSync to deploy the Online Boutique’s Helm chart with the cartservice pointing to the Spanner database:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs/$ONLINEBOUTIQUE_NAMESPACE/repo-sync.yaml apiVersion: configsync.gke.io/v1beta1 kind: RepoSync metadata: name: repo-sync namespace: ${ONLINEBOUTIQUE_NAMESPACE} spec: sourceFormat: unstructured sourceType: helm helm: repo: oci://${CHART_REGISTRY_REPOSITORY} chart: ${ONLINEBOUTIQUE_NAMESPACE} version: ${ONLINE_BOUTIQUE_VERSION:1} releaseName: ${ONLINEBOUTIQUE_NAMESPACE} auth: gcpserviceaccount gcpServiceAccountEmail: ${HELM_CHARTS_READER_GSA}@${TENANT_PROJECT_ID}.iam.gserviceaccount.com values: cartDatabase: inClusterRedis: create: false type: spanner connectionString: ${SPANNER_CONNECTION_STRING} images: repository: ${PRIVATE_ONLINE_BOUTIQUE_REGISTRY} tag: ${ONLINE_BOUTIQUE_VERSION} nativeGrpcHealthCheck: true seccompProfile: enable: true loadGenerator: checkFrontendInitContainer: false frontend: externalService: false virtualService: create: true gateway: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} labelKey: asm labelValue: ingressgateway hosts: - ${ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME} serviceAccounts: create: true annotationsOnlyForCartservice: true annotations: iam.gke.io/gcp-service-account: ${SPANNER_DB_USER_GSA_ID} authorizationPolicies: create: true networkPolicies: create: true sidecars: create: true EOF Info This will change the SPANNER_CONNECTION_STRING environment variable of the cartservice to point to the Spanner database as well as removing the Deployment and the Service of the default in-cluster redis database. We are also setting the GSA annotation only on the cartserviece service account in order to leverage Workload Identity with a least-privilege approach.\nDeploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Use Spanner\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the Online Boutique apps repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $ONLINEBOUTIQUE_NAMESPACE Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list Check the Online Boutique website Navigate to the Online Boutique website, click on the link displayed by the command below:\necho -e \"https://${ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME}\" You should still have the Online Boutique website working successfully, but now linked to an external Spanner database.\n",
    "description": "Duration: 10 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "helm"
    ],
    "title": "Use Spanner",
    "uri": "/acm-workshop/onlineboutique/spanner/use-spanner/index.html"
  },
  {
    "content": "Set up the Networking in the Tenant project.\nAllow NetworkingDuration: 2 min | Persona: Org Admin\nSet up NetworkDuration: 5 min | Persona: Platform Admin\n",
    "description": "",
    "tags": null,
    "title": "3. Networking",
    "uri": "/acm-workshop/networking/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Org Admin\nIn this section, you will enable and grant the appropriate APIs in the Tenant project and the IAM role for the Tenant project’s service account. This will allow later this service account to provision the Fleet features.\nDefine variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Define role Define the gkehub.admin role with an IAMPolicyMember for the Tenant project’s service account:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/gke-hub-admin.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: gke-hub-admin-${TENANT_PROJECT_ID} namespace: config-control annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/config-control/IAMServiceAccount/${TENANT_PROJECT_ID},resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} spec: memberFrom: serviceAccountRef: name: ${TENANT_PROJECT_ID} role: roles/gkehub.admin resourceRef: kind: Project external: projects/${TENANT_PROJECT_ID} EOF Define APIs Define the GKE and Fleet APIs Service resources for the Tenant project:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/gke-hub-service.yaml apiVersion: serviceusage.cnrm.cloud.google.com/v1beta1 kind: Service metadata: annotations: cnrm.cloud.google.com/deletion-policy: \"abandon\" cnrm.cloud.google.com/disable-dependent-services: \"false\" config.kubernetes.io/depends-on: resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} name: ${TENANT_PROJECT_ID}-gkehub namespace: config-control spec: projectRef: name: ${TENANT_PROJECT_ID} resourceID: gkehub.googleapis.com EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/anthos-configmanagement-service.yaml apiVersion: serviceusage.cnrm.cloud.google.com/v1beta1 kind: Service metadata: annotations: cnrm.cloud.google.com/deletion-policy: \"abandon\" cnrm.cloud.google.com/disable-dependent-services: \"false\" config.kubernetes.io/depends-on: resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} name: ${TENANT_PROJECT_ID}-anthosconfigmanagement namespace: config-control spec: projectRef: name: ${TENANT_PROJECT_ID} resourceID: anthosconfigmanagement.googleapis.com EOF Deploy Kubernetes manifests cd ~/$HOST_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Allow Fleet for Tenant project\" \u0026\u0026 git push origin main Check deployments graph TD; IAMPolicyMember-.-\u003eIAMServiceAccount IAMPolicyMember-.-\u003eProject Service-.-\u003eProject List the Kubernetes resources managed by Config Sync in Config Controller for the Host project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Host project configs repository:\ncd ${WORK_DIR}$HOST_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud services list \\ --enabled \\ --project ${TENANT_PROJECT_ID} \\ | grep -E 'anthosconfigmanagement|gkehub' gcloud projects get-iam-policy $TENANT_PROJECT_ID \\ --filter=\"bindings.members:${TENANT_PROJECT_SA_EMAIL}\" \\ --flatten=\"bindings[].members\" \\ --format=\"table(bindings.role)\" \\ | grep gkehub Wait and re-run this command above until you see the resources created.\n",
    "description": "Duration: 5 min | Persona: Org Admin",
    "tags": [
      "kcc",
      "org-admin"
    ],
    "title": "Allow Fleet",
    "uri": "/acm-workshop/gke-cluster/allow-fleet/index.html"
  },
  {
    "content": " Duration: 2 min | Persona: Org Admin\nIn this section, you will grant the appropriate the IAM roles for the Tenant project’s service account. This will allow later this service account to provision some monitoring features.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Define roles Define the monitoring.uptimeCheckConfigEditor role with an IAMPolicyMember for the Tenant project’s service account:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/uptime-check-config-editor.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: uptime-check-config-editor-${TENANT_PROJECT_ID} namespace: config-control annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/config-control/IAMServiceAccount/${TENANT_PROJECT_ID},resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} spec: memberFrom: serviceAccountRef: name: ${TENANT_PROJECT_ID} role: roles/monitoring.uptimeCheckConfigEditor resourceRef: kind: Project external: projects/${TENANT_PROJECT_ID} EOF Define the monitoring.notificationChannelEditor role with an IAMPolicyMember for the Tenant project’s service account:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/notification-channel-editor.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: notification-channel-editor-${TENANT_PROJECT_ID} namespace: config-control annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/config-control/IAMServiceAccount/${TENANT_PROJECT_ID},resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} spec: memberFrom: serviceAccountRef: name: ${TENANT_PROJECT_ID} role: roles/monitoring.notificationChannelEditor resourceRef: kind: Project external: projects/${TENANT_PROJECT_ID} EOF Define the monitoring.alertPolicyEditor role with an IAMPolicyMember for the Tenant project’s service account:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/projects/$TENANT_PROJECT_ID/alert-policy-editor.yaml apiVersion: iam.cnrm.cloud.google.com/v1beta1 kind: IAMPolicyMember metadata: name: alert-policy-editor-${TENANT_PROJECT_ID} namespace: config-control annotations: config.kubernetes.io/depends-on: iam.cnrm.cloud.google.com/namespaces/config-control/IAMServiceAccount/${TENANT_PROJECT_ID},resourcemanager.cnrm.cloud.google.com/namespaces/config-control/Project/${TENANT_PROJECT_ID} spec: memberFrom: serviceAccountRef: name: ${TENANT_PROJECT_ID} role: roles/monitoring.alertPolicyEditor resourceRef: kind: Project external: projects/${TENANT_PROJECT_ID} EOF Deploy Kubernetes manifests cd ${WORK_DIR}$HOST_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Allow Monitoring for Tenant project\" \u0026\u0026 git push origin main Check deployments graph TD; IAMPolicyMember-.-\u003eProject List the Kubernetes resources managed by Config Sync in Config Controller for the Host project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Host project configs repository:\ncd ${WORK_DIR}$HOST_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud projects get-iam-policy $TENANT_PROJECT_ID \\ --filter=\"bindings.members:${TENANT_PROJECT_SA_EMAIL}\" \\ --flatten=\"bindings[].members\" \\ --format=\"table(bindings.role)\" \\ | grep -E 'uptimeCheckConfigEditor|notificationChannelEditor|alertPolicyEditor' Wait and re-run this command above until you see the resources created.\n",
    "description": "Duration: 2 min | Persona: Org Admin",
    "tags": [
      "kcc",
      "monitoring",
      "org-admin"
    ],
    "title": "Allow Monitoring",
    "uri": "/acm-workshop/tenant-project/allow-monitoring/index.html"
  },
  {
    "content": "Before you begin you need to make sure you have the prerequisites in place.\nMinimum knowledge It’s recommended that you have a minimum knowledge about Istio, Anthos Service Mesh (ASM), Anthos Config Management (ACM), GitOps, etc. Here is a list of resources you could leverage to get some familiarities depending on where you are at with these concepts:\nWhat Is Kubernetes? Managing Kubernetes with Config Sync Using Config Connector for Google Cloud resource management Automating infrastructure compliance with Policy Controller Istio in 5 minutes ASM Value over Istio Cloud Armor in a minute Memorystore in a minute Cloud Operations Suite in a minute Based on these introductions, here are higly recommended resources to watch before running this workshop:\nOrganizing Teams for GitOps and Cloud Native Deployments ACM @ Goldman Sachs Enforcing Service Mesh Structure using OPA Gatekeeper Managing microservice architectures with ASM Your setup to run this workshop You can run this workshop on Cloud Shell or on your local machine running Linux. Cloud Shell pre-installs all the required tools.\nInstall the required tools:\ngcloud kubectl kustomize git gh (GitHub CLI) kpt curl nomos docker crane helm You need to have:\nGCP account with the role owner in your Organization in order to deploy the resources needed for this workshop GitHub account, it’s free. We will leverage GitHub throughout this workshop. ",
    "description": "",
    "tags": null,
    "title": "Before you begin",
    "uri": "/acm-workshop/overview/before-you-begin/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Platform Admin\nIn this section, you will configure Config Sync to sync the resources in the Online Boutique Namespace via its associated RoleBinding.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh mkdir -p ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs mkdir ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs/$ONLINEBOUTIQUE_NAMESPACE Define Namespace Define a dedicated Namespace for the Online Boutique apps:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs/$ONLINEBOUTIQUE_NAMESPACE/namespace.yaml apiVersion: v1 kind: Namespace metadata: labels: istio-injection: enabled pod-security.kubernetes.io/enforce: restricted name: ${ONLINEBOUTIQUE_NAMESPACE} EOF Note In addition to the istio-injection to include this Namespace into our Service Mesh, we are also adding the pod-security.kubernetes.io/enforce label as the restricted Pod Security Standards policy.\nAllo Config Sync to sync Istio resources cat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs/$ONLINEBOUTIQUE_NAMESPACE/repo-sync-role-binding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: repo-sync namespace: ${ONLINEBOUTIQUE_NAMESPACE} subjects: - kind: ServiceAccount name: ns-reconciler-${ONLINEBOUTIQUE_NAMESPACE} namespace: config-management-system roleRef: kind: ClusterRole name: edit apiGroup: rbac.authorization.k8s.io EOF Tip We are using the edit user-facing role here, to follow the least privilege principle. Earlier in this workshop during the ASM installation, we extended the default edit role with more capabilities regarding to the Istio resources: VirtualServices, Sidecars and AuthorizationPolicies which will be leveraged in the OnlineBoutique’s namespace.\nDeploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Configure Config Sync for Online Boutique\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the GKE cluster configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 5 min | Persona: Platform Admin",
    "tags": [
      "asm",
      "gitops-tips",
      "platform-admin",
      "security-tips"
    ],
    "title": "Configure Config Sync",
    "uri": "/acm-workshop/onlineboutique/configure-config-sync/index.html"
  },
  {
    "content": " Duration: 15 min | Persona: Platform Admin\nIn this section, you will deploy a secured Ingress Gateway (unprivileged container, managed certificates, Cloud Armor, etc.) in its dedicated namespace in the GKE cluster.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export INGRESS_GATEWAY_NAME=asm-ingressgateway\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh echo \"export INGRESS_GATEWAY_LABEL='asm: ingressgateway'\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Create a dedicated folder for the ASM Ingress Gateway in the GKE configs’s Git repo:\nmkdir ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE Define Namespace cat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE/namespace.yaml apiVersion: v1 kind: Namespace metadata: labels: istio-injection: enabled pod-security.kubernetes.io/enforce: restricted name: ${INGRESS_GATEWAY_NAMESPACE} EOF Note In addition to the istio-injection to include this Namespace into our Service Mesh, we are also adding the pod-security.kubernetes.io/enforce label as the restricted Pod Security Standards policy.\nDefine Deployment cat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} spec: selector: matchLabels: ${INGRESS_GATEWAY_LABEL} app: ${INGRESS_GATEWAY_NAME} template: metadata: annotations: # This is required to tell Anthos Service Mesh to inject the gateway with the required configuration. inject.istio.io/templates: gateway labels: app: ${INGRESS_GATEWAY_NAME} ${INGRESS_GATEWAY_LABEL} spec: containers: - name: istio-proxy image: auto # The image will automatically update each time the pod starts. env: - name: ISTIO_META_UNPRIVILEGED_POD value: \"true\" resources: limits: cpu: 2000m memory: 1024Mi requests: cpu: 100m memory: 128Mi securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL privileged: false readOnlyRootFilesystem: true securityContext: fsGroup: 1337 runAsGroup: 1337 runAsNonRoot: true runAsUser: 1337 seccompProfile: type: RuntimeDefault serviceAccountName: ${INGRESS_GATEWAY_NAME} EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE/hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} spec: maxReplicas: 5 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 80 minReplicas: 3 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: ${INGRESS_GATEWAY_NAME} EOF Tip Note that we are configuring the replicas in the HorizontalPodAutoscaler and not via the Deployment itself, this is a best practice to avoid any conflict with the dynamic value of the Deployment replicas actually in the Kubernetes cluster managed by the HorizontalPodAutoscaler resource.\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE/service-account.yaml apiVersion: v1 kind: ServiceAccount metadata: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE/read-secrets-role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\", \"watch\", \"list\"] EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE/service-account-role-binding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ${INGRESS_GATEWAY_NAME} subjects: - kind: ServiceAccount name: ${INGRESS_GATEWAY_NAME} EOF Define Service and Ingress cat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE/backend-config.yaml apiVersion: cloud.google.com/v1 kind: BackendConfig metadata: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} spec: healthCheck: requestPath: /healthz/ready port: 15021 type: HTTP securityPolicy: name: ${SECURITY_POLICY_NAME} EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE/frontend-config.yaml apiVersion: networking.gke.io/v1beta1 kind: FrontendConfig metadata: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} spec: sslPolicy: ${SSL_POLICY_NAME} redirectToHttps: enabled: true responseCodeName: MOVED_PERMANENTLY_DEFAULT EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE/service.yaml apiVersion: v1 kind: Service metadata: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} annotations: cloud.google.com/neg: '{\"ingress\": true}' cloud.google.com/backend-config: '{\"default\": \"${INGRESS_GATEWAY_NAME}\"}' cloud.google.com/app-protocols: '{\"http2\":\"HTTP\"}' labels: ${INGRESS_GATEWAY_LABEL} spec: ports: - name: tcp-status port: 15021 protocol: TCP targetPort: 15021 - name: http2 port: 80 targetPort: 8080 - name: https port: 443 targetPort: 8443 selector: ${INGRESS_GATEWAY_LABEL} type: ClusterIP EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE/ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} annotations: kubernetes.io/ingress.global-static-ip-name: \"${INGRESS_GATEWAY_PUBLIC_IP_NAME}\" kubernetes.io/ingress.class: \"gce\" networking.gke.io/v1beta1.FrontendConfig: ${INGRESS_GATEWAY_NAME} spec: defaultBackend: service: name: ${INGRESS_GATEWAY_NAME} port: number: 80 rules: - http: paths: - path: /* pathType: ImplementationSpecific backend: service: name: ${INGRESS_GATEWAY_NAME} port: number: 80 EOF Define Gateway cat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE/gateway.yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} spec: selector: ${INGRESS_GATEWAY_LABEL} servers: - port: number: 80 name: http protocol: HTTP hosts: - '*' EOF Tip We define a shared Gateway resource. Gateways are generally owned by the platform admins or network admins team. Therefore, the Gateway resource is created in the Ingress Gateway namespace owned by the platform admin and could be use in other namespaces via their own VirtualService entries.\nDeploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"ASM Ingress Gateway in GKE cluster\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the GKE cluster configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\".\n",
    "description": "Duration: 15 min | Persona: Platform Admin",
    "tags": [
      "asm",
      "gke",
      "platform-admin",
      "security-tips"
    ],
    "title": "Deploy Ingress Gateway",
    "uri": "/acm-workshop/ingress-gateway/deploy-ingress-gateway/index.html"
  },
  {
    "content": " Duration: 10 min | Persona: Platform Admin\nIn this section, you will enforce policies in order to make sure that your clusters, namespaces and apps are well configured to be secured by your Service Mesh.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export INGRESS_GATEWAY_NAMESPACE=asm-ingress\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Define “Automatic sidecar proxy injection” policies https://cloud.google.com/service-mesh/docs/anthos-service-mesh-proxy-injection\nDefine the namespaces-automatic-sidecar-injection-label Constraint based on the K8sRequiredLabels ConstraintTemplate for Namespaces:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/policies/constraints/namespaces-automatic-sidecar-injection-label.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sRequiredLabels metadata: name: namespaces-automatic-sidecar-injection-label annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Requires Namespaces to have the \"istio-injection\" label in order to be included in the Service Mesh.', remediation: 'Any Namespaces should have the \"istio-injection\" label with the \"enabled\" value.' }\" spec: enforcementAction: deny match: kinds: - apiGroups: - \"\" kinds: - Namespace excludedNamespaces: - config-management-monitoring - config-management-system - default - gatekeeper-system - istio-system - kube-node-lease - kube-public - kube-system - resource-group-system - poco-trial parameters: labels: - allowedRegex: enabled key: istio-injection EOF Define the pods-sidecar-injection-annotation Constraint based on the AsmSidecarInjection ConstraintTemplate for Pods:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/policies/constraints/pods-sidecar-injection-annotation.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: AsmSidecarInjection metadata: name: pods-sidecar-injection-annotation annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Enforce the istio proxy sidecar always been injected to workload pods.', remediation: 'Any Pods shouldn't have the \"sidecar.istio.io/inject\" annotation set to \"false\".' }\" spec: enforcementAction: deny match: kinds: - apiGroups: - \"\" kinds: - Pod excludedNamespaces: - kube-system # to exclude istio-cni pods parameters: strictnessLevel: High EOF Define “STRICT mTLS in the Mesh” policies Define the mesh-level-strict-mtls Constraint based on the AsmPeerAuthnMeshStrictMtls ConstraintTemplate:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/policies/constraints/mesh-level-strict-mtls.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: AsmPeerAuthnMeshStrictMtls metadata: name: mesh-level-strict-mtls annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Enforce the mesh level strict mtls PeerAuthentication.', remediation: 'The istio-system namespace should have a default PeerAuthentication with STRICT mTLS.' }\" spec: enforcementAction: deny parameters: rootNamespace: istio-system strictnessLevel: High EOF Define the peerauthentication-strict-mtls Constraint based on the AsmPeerAuthnStrictMtls ConstraintTemplate for PeerAuthentications:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/policies/constraints/peerauthentication-strict-mtls.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: AsmPeerAuthnStrictMtls metadata: name: peerauthentication-strict-mtls annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Enforce all PeerAuthentications cannot overwrite strict mtls.', remediation: 'Any PeerAuthentications should have STRICT mTLS.' }\" spec: enforcementAction: deny match: kinds: - apiGroups: - security.istio.io kinds: - PeerAuthentication parameters: rootNamespace: istio-system strictnessLevel: High EOF Define the destination-rule-tls-enabled Constraint based on the DestinationRuleTLSEnabled ConstraintTemplate for DestinationRules:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/policies/constraints/destinationrule-tls-enabled.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: DestinationRuleTLSEnabled metadata: name: destinationrule-tls-enabled annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Prohibits disabling TLS for all hosts and host subsets in Istio DestinationRules.', remediation: 'Any DestinationRules should not disable TLS.' }\" spec: enforcementAction: deny match: kinds: - apiGroups: - networking.istio.io kinds: - DestinationRule EOF Define AuthorizationPolicy policies https://istio.io/latest/docs/reference/config/security/authorization-policy/\nDefine the default-deny-authorization-policies Constraint based on the AsmAuthzPolicyDefaultDeny ConstraintTemplate for DestinationRules:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/policies/constraints/default-deny-authorization-policies.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: AsmAuthzPolicyDefaultDeny metadata: name: default-deny-authorization-policies annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Enforce the mesh level default deny AuthorizationPolicy. Reference to https://istio.io/latest/docs/ops/best-practices/security/#use-default-deny-patterns.', remediation: 'The istio-system namespace should have a default deny-all AuthorizationPolicy for the entire mesh.' }\" spec: enforcementAction: deny parameters: rootNamespace: istio-system strictnessLevel: High EOF Define the authz-source-principals-not-all Constraint based on the AsmAuthzPolicyEnforceSourcePrincipals ConstraintTemplate for DestinationRules:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/policies/constraints/authz-source-principals-not-all.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: AsmAuthzPolicyEnforceSourcePrincipals metadata: name: authz-source-principals-not-all annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Requires that Istio AuthorizationPolicy \"from\" field, when defined, has source principles, which must be set to something other than \"*\".', remediation: 'Any AuthorizationPolicies shouldn't define the \"from\" field with \"*\".' }\" spec: enforcementAction: deny match: kinds: - apiGroups: - security.istio.io kinds: - AuthorizationPolicy excludedNamespaces: - ${INGRESS_GATEWAY_NAMESPACE} EOF Define the authz-source-principals-prefix-not-default Constraint based on the AsmAuthzPolicyDisallowedPrefix ConstraintTemplate for AuthorizationPolicies:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/policies/constraints/authz-source-principals-prefix-not-default.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: AsmAuthzPolicyDisallowedPrefix metadata: name: authz-source-principals-prefix-not-default annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Requires that principals and namespaces in Istio AuthorizationPolicy rules not have a prefix from a specified list.', remediation: 'Any AuthorizationPolicies shouldn't have the principal as \"default\".' }\" spec: enforcementAction: deny match: kinds: - apiGroups: - security.istio.io kinds: - AuthorizationPolicy parameters: disallowedPrincipalPrefixes: - default EOF Define K8sBlockAllIngress policy Define the block-all-ingress Constraint based on the K8sBlockAllIngress ConstraintTemplate to only allow public ingress from the ASM Ingress Gateway:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/policies/constraints/block-all-ingress.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sBlockAllIngress metadata: name: block-all-ingress annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Disallows the creation of Ingress objects (Ingress, Gateway, and Service types of NodePort and LoadBalancer).', remediation: 'Any Ingress objects (Ingress, Gateway, and Service) should go through the ASM Ingress Gateway instead.' }\" spec: enforcementAction: deny match: excludedNamespaces: - kube-system # default-http-backend as NodePort - ${INGRESS_GATEWAY_NAMESPACE} # asm-ingressgateway as LoadBalancer EOF Define VirtualServiceWithHost policy Define the ConstraintTemplate:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/policies/templates/virtualservicewithhost.yaml apiVersion: templates.gatekeeper.sh/v1 kind: ConstraintTemplate metadata: name: virtualservicewithhost annotations: description: \"VirtualService shouldn't define the hosts with *.\" spec: crd: spec: names: kind: VirtualServiceWithHost targets: - target: admission.k8s.gatekeeper.sh rego: |- package virtualservicewithhost # spec.hosts does not exist violation[{\"msg\": msg}] { is_virtualservice(input.review.kind) not contains_hosts(input.review.object.spec) msg := \"hosts does not exist\" } # spec.hosts does not contain '*' violation[{\"msg\": msg}] { is_virtualservice(input.review.kind) principal := input.review.object.spec.hosts[_] principal == \"*\" msg := \"hosts[] cannot be '*'\" } is_virtualservice(kind) { kind.kind == \"VirtualService\" kind.group == \"networking.istio.io\" } contains_hosts(spec) { spec.hosts } EOF Define the virtual-service-with-host Constraint based on the VirtualServiceWithHost ConstraintTemplate just defined:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/policies/constraints/virtual-service-with-host.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: VirtualServiceWithHost metadata: name: virtual-service-with-host annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Requires that Istio VirtualService \"hosts\" must be set to something other than \"*\".', remediation: 'Any VirtualService shouldn't define the \"hosts\" with \"*\".' }\" spec: enforcementAction: deny match: kinds: - apiGroups: - networking.istio.io kinds: - VirtualService EOF Update Gatekeeper config for Referrential Constraints Update the previously defined config-referential-constraints Config:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/gatekeeper-system/config-referential-constraints.yaml apiVersion: config.gatekeeper.sh/v1alpha1 kind: Config metadata: name: config namespace: gatekeeper-system spec: sync: syncOnly: - group: \"\" version: \"v1\" kind: \"Namespace\" - group: \"networking.k8s.io\" version: \"v1\" kind: \"NetworkPolicy\" - group: \"security.istio.io\" version: \"v1beta1\" kind: \"PeerAuthentication\" - group: \"security.istio.io\" version: \"v1beta1\" kind: \"AuthorizationPolicy\" EOF Deploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Policies for ASM/Istio\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the GKE cluster configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\".\nSee the Policy Controller Constraints without any violations in the GKE cluster, by running this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/policy_controller/dashboard?project=${TENANT_PROJECT_ID}\" List the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 10 min | Persona: Platform Admin",
    "tags": [
      "asm",
      "platform-admin",
      "policies",
      "security-tips"
    ],
    "title": "Enforce Service Mesh policies",
    "uri": "/acm-workshop/service-mesh/enforce-service-mesh-policies/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Org Admin\nIn this section you will enforce policies to guarantee that any Namespaces in the ConfigController instance defining any Tenant project should contain its own ConfigConnectorContext object in order to leverage the namespaced mode of Config Connector.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Define the “Require ConfigConnectorContext for Namespaces” policies Define the ConstraintTemplate making sure that any Namespaces has a ConfigConnectorContext in it:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/policies/templates/requirenamespaceconfigconnectorcontext.yaml apiVersion: templates.gatekeeper.sh/v1 kind: ConstraintTemplate metadata: name: requirenamespaceconfigconnectorcontext annotations: description: \"Requires that every namespaces defined in the cluster has a ConfigConnectorContext. Note: This constraint is referential. See https://cloud.google.com/anthos-config-management/docs/how-to/creating-constraints#referential for details.\" spec: crd: spec: names: kind: RequireNamespaceConfigConnectorContext targets: - target: admission.k8s.gatekeeper.sh rego: |- package requirenamespaceconfigconnectorcontext violation[{\"msg\": msg}] { input.review.kind.kind == \"Namespace\" not namespace_has_configconnectorcontext(input.review.object.metadata.name) msg := sprintf(\"Namespace \u003c%v\u003e does not have a ConfigConnectorContext\", [input.review.object.metadata.name]) } namespace_has_configconnectorcontext(ns) { ccc := data.inventory.namespace[ns][_].ConfigConnectorContext[_] } EOF Define the namespaces-required-configconnectorcontext Constraint based on the RequireNamespaceConfigConnectorContext ConstraintTemplate for Namespaces:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/policies/constraints/namespaces-required-configconnectorcontext.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: RequireNamespaceConfigConnectorContext metadata: name: namespaces-required-configconnectorcontext annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Requires Namespaces to have a ConfigConnectorContext in order to leverage Config Connector.', remediation: 'Any Namespaces should have a ConfigConnectorContext.' }\" spec: enforcementAction: dryrun match: kinds: - apiGroups: - \"\" kinds: - Namespace excludedNamespaces: - cnrm-system - config-control - config-management-monitoring - config-management-system - configconnector-operator-system - default - gatekeeper-system - krmapihosting-monitoring - krmapihosting-system - kube-node-lease - kube-public - kube-system - resource-group-system EOF Because this is constraint is referential (look at ConfigConnectorContext in Namespace), we need to define an associated Config in the gatekeeper-system Namespace:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$HOST_PROJECT_DIR_NAME/config-referential-constraints.yaml apiVersion: config.gatekeeper.sh/v1alpha1 kind: Config metadata: name: config namespace: gatekeeper-system spec: sync: syncOnly: - group: \"\" version: \"v1\" kind: \"Namespace\" - group: \"core.cnrm.cloud.google.com\" version: \"v1beta1\" kind: \"ConfigConnectorContext\" EOF Deploy Kubernetes manifests cd ${WORK_DIR}$HOST_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Enforce policies for Tenant projects\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in Config Controller for the Host project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Host project configs repository:\ncd ${WORK_DIR}$HOST_PROJECT_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 5 min | Persona: Org Admin",
    "tags": [
      "org-admin",
      "policies",
      "security-tips"
    ],
    "title": "Enforce Tenant projects policies",
    "uri": "/acm-workshop/host-project/enforce-tenant-projects-policies/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, you will copy the Whereami app container in your private Artifact Registry. You will also scan this container image.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh WHEREAMI_VERSION=v1.2.14 PRIVATE_WHEREAMI_IMAGE_NAME=$CONTAINER_REGISTRY_REPOSITORY/whereami:$WHEREAMI_VERSION echo \"export PRIVATE_WHEREAMI_IMAGE_NAME=${PRIVATE_WHEREAMI_IMAGE_NAME}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Copy the public container image to your private registry:\nUPSTREAM_WHEREAMI_IMAGE_NAME=us-docker.pkg.dev/google-samples/containers/gke/whereami:$WHEREAMI_VERSION gcloud auth configure-docker $CONTAINER_REGISTRY_HOST_NAME --quiet crane copy $UPSTREAM_WHEREAMI_IMAGE_NAME $PRIVATE_WHEREAMI_IMAGE_NAME List the container images in your private registry:\ngcloud artifacts docker images list $CONTAINER_REGISTRY_REPOSITORY \\ --include-tags Scan the whereami container image:\ngcloud artifacts docker images scan $PRIVATE_WHEREAMI_IMAGE_NAME \\ --project ${TENANT_PROJECT_ID} \\ --remote \\ --format='value(response.scan)' \u003e ${WORK_DIR}scan_id.txt gcloud artifacts docker images list-vulnerabilities $(cat ${WORK_DIR}scan_id.txt) \\ --project ${TENANT_PROJECT_ID} \\ --format='table(vulnerability.effectiveSeverity, vulnerability.cvssScore, noteName, vulnerability.packageIssue[0].affectedPackage, vulnerability.packageIssue[0].affectedVersion.name, vulnerability.packageIssue[0].fixedVersion.name)' Tip You could use this gcloud artifacts docker images scan command in your Continuous Integration system in order to detect as early as possible for example Critical or High vulnerabilities.\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "security-tips"
    ],
    "title": "Prepare container",
    "uri": "/acm-workshop/whereami/prepare-container/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, you will copy the Bank of Anthos apps containers in your private Artifact Registry. You will also scan one container image.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh BANK_OF_ANTHOS_VERSION=v0.5.10 echo \"export BANK_OF_ANTHOS_VERSION=${BANK_OF_ANTHOS_VERSION}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh PRIVATE_BANK_OF_ANTHOS_REGISTRY=$CONTAINER_REGISTRY_REPOSITORY/bankofanthos echo \"export PRIVATE_BANK_OF_ANTHOS_REGISTRY=${PRIVATE_BANK_OF_ANTHOS_REGISTRY}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Copy the public container images to your private registry:\nUPSTREAM_BANK_OF_ANTHOS_REGISTRY=gcr.io/bank-of-anthos-ci SERVICES=\"accounts-db balancereader contacts frontend ledger-db ledgerwriter loadgenerator transactionhistory userservice\" for s in $SERVICES; do crane copy $UPSTREAM_BANK_OF_ANTHOS_REGISTRY/$s:$BANK_OF_ANTHOS_VERSION $PRIVATE_BANK_OF_ANTHOS_REGISTRY/$s:$BANK_OF_ANTHOS_VERSION; done List the container images in your private registry:\ngcloud artifacts docker images list $CONTAINER_REGISTRY_REPOSITORY \\ --include-tags Scan the cartservice container image:\ngcloud artifacts docker images scan $PRIVATE_BANK_OF_ANTHOS_REGISTRY/cartservice:$BANK_OF_ANTHOS_VERSION \\ --project ${TENANT_PROJECT_ID} \\ --remote \\ --format='value(response.scan)' \u003e ${WORK_DIR}scan_id.txt gcloud artifacts docker images list-vulnerabilities $(cat ${WORK_DIR}scan_id.txt) \\ --project ${TENANT_PROJECT_ID} \\ --format='table(vulnerability.effectiveSeverity, vulnerability.cvssScore, noteName, vulnerability.packageIssue[0].affectedPackage, vulnerability.packageIssue[0].affectedVersion.name, vulnerability.packageIssue[0].fixedVersion.name)' Tip You could use this gcloud artifacts docker images scan command in your Continuous Integration system in order to detect as early as possible for example Critical or High vulnerabilities.\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "security-tips"
    ],
    "title": "Prepare containers",
    "uri": "/acm-workshop/bankofanthos/prepare-containers/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, you will trace your apps in order to follow a request through your Service Mesh, observe the network calls and profile your system end to end.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh In the Google Cloud console, navigate to Trace service. Click on the link displayed by the command below:\necho -e \"https://console.cloud.google.com/traces/list?project=${TENANT_PROJECT_ID}\" Select one of the Online Boutique’s frontend app’s requests: From there you will have access to a lot more details about the different calls, trace logs, etc.\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "asm",
      "apps-operator",
      "monitoring"
    ],
    "title": "Trace apps",
    "uri": "/acm-workshop/monitoring-and-audit/trace-apps/index.html"
  },
  {
    "content": " Duration: 10 min | Persona: Apps Operator\nIn this section, you will update the OnlineBoutique’s cartservice app in order to point to the Memorystore (Redis) instance previously created.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Update RepoSync to deploy the Online Boutique’s Helm chart Get Memorystore (Redis) connection information:\nexport REDIS_IP=$(gcloud redis instances describe $REDIS_NAME --region $GKE_LOCATION --project $TENANT_PROJECT_ID --format='get(host)') export REDIS_PORT=$(gcloud redis instances describe $REDIS_NAME --region $GKE_LOCATION --project $TENANT_PROJECT_ID --format='get(port)') Define the RepoSync to deploy the Online Boutique’s Helm chart with the cartservice pointing to the Memorystore (Redis) database:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs/$ONLINEBOUTIQUE_NAMESPACE/repo-sync.yaml apiVersion: configsync.gke.io/v1beta1 kind: RepoSync metadata: name: repo-sync namespace: ${ONLINEBOUTIQUE_NAMESPACE} spec: sourceFormat: unstructured sourceType: helm helm: repo: oci://${CHART_REGISTRY_REPOSITORY} chart: ${ONLINEBOUTIQUE_NAMESPACE} version: ${ONLINE_BOUTIQUE_VERSION:1} releaseName: ${ONLINEBOUTIQUE_NAMESPACE} auth: gcpserviceaccount gcpServiceAccountEmail: ${HELM_CHARTS_READER_GSA}@${TENANT_PROJECT_ID}.iam.gserviceaccount.com values: cartDatabase: inClusterRedis: create: false connectionString: ${REDIS_IP}:${REDIS_PORT} images: repository: ${PRIVATE_ONLINE_BOUTIQUE_REGISTRY} tag: ${ONLINE_BOUTIQUE_VERSION} nativeGrpcHealthCheck: true seccompProfile: enable: true loadGenerator: checkFrontendInitContainer: false frontend: externalService: false virtualService: create: true gateway: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} labelKey: asm labelValue: ingressgateway hosts: - ${ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME} serviceAccounts: create: true authorizationPolicies: create: true networkPolicies: create: true sidecars: create: true EOF Info This will change the REDIS_ADDR environment variable of the cartservice to point to the Memorystore (Redis) database as well as removing the Deployment and the Service of the default in-cluster redis database.\nDeploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Use Memorystore (Redis)\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the Online Boutique apps repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $ONLINEBOUTIQUE_NAMESPACE Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list Check the Online Boutique website Navigate to the Online Boutique website, click on the link displayed by the command below:\necho -e \"https://${ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME}\" You should still have the Online Boutique website working successfully, but now linked to an external Memorystore (Redis) database. Congrats!\n",
    "description": "Duration: 10 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "helm"
    ],
    "title": "Use Memorystore",
    "uri": "/acm-workshop/onlineboutique/memorystore/use-memorystore/index.html"
  },
  {
    "content": "Set up the GKE cluster.\nAllow GKEDuration: 5 min | Persona: Org Admin\nEnforce GKE policiesDuration: 5 min | Persona: Org Admin\nCreate GKE clusterDuration: 20 min | Persona: Platform Admin\nAllow FleetDuration: 5 min | Persona: Org Admin\nSet up GKE configs's Git repoDuration: 10 min | Persona: Platform Admin\nEnforce Kubernetes policiesDuration: 5 min | Persona: Platform Admin\nSet up NetworkPolicies loggingDuration: 5 min | Persona: Platform Admin\n",
    "description": "",
    "tags": null,
    "title": "4. GKE cluster",
    "uri": "/acm-workshop/gke-cluster/index.html"
  },
  {
    "content": "Different contents have been leveraged in order to make and inpsire this workshop, here is the list of them:\nExposing service mesh applications through GKE Ingress - Alex Mattson, Ameer Abbas and Mark Church Whereami app - Alex Mattson Online Boutique apps - Megan O’Keefe and Nim Jayawardena Bank of Anthos apps - Megan O’Keefe and Olivier Bourgeois Kubernetes Network Policies recipes - Ahmet Alp Balkan Managing microservice architectures with Anthos Service Mesh - Ameer Abbas Organizing Teams for GitOps and Cloud Native Deployments - Sandeep Parikh Build a platform with KRM - Megan O’Keefe Enforcing Service Mesh Structure using OPA Gatekeeper - Sandeep Parikh ACM @ Goldman Sachs Improve Security Posture in GKE Environment with ACM and ASM - Ameer Abbas Hybrid SME Academy Labs - Megan O’Keefe, Dan Sanche and Sandeep Parikh Anthos Multicloud Workshop - Kent Hua, Aaron Rueth and Ameer Abbas Istio Sidecar resource to reduce memory overhead - Christian Posta Securing Redis with Istio TLS origination - Sam Stoelinga Preventing Privileged pods using Pod Security Admission / Standards - Sam Stoelinga Validating Admission Policies - Joe Betz Thank you to their authors!\n",
    "description": "",
    "tags": null,
    "title": "Credits",
    "uri": "/acm-workshop/overview/credits/index.html"
  },
  {
    "content": " Duration: 10 min | Persona: Apps Operator\nIn this section, you will deploy the Whereami app.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Get upstream Kubernetes manifests Get the upstream Kubernetes manifests:\ncd ${WORK_DIR}$WHERE_AMI_DIR_NAME kpt pkg get https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/whereami/k8s rm k8s/Kptfile mv k8s upstream Create base overlay Create Kustomize base overlay files:\nmkdir ${WORK_DIR}$WHERE_AMI_DIR_NAME/base cd ${WORK_DIR}$WHERE_AMI_DIR_NAME/base kustomize create kustomize edit add resource ../upstream cat \u003c\u003cEOF \u003e\u003e ${WORK_DIR}$WHERE_AMI_DIR_NAME/base/kustomization.yaml patchesJson6902: - target: kind: Service name: whereami patch: |- - op: replace path: /spec/type value: ClusterIP EOF Info Here we are changing the Service type to ClusterIP because the Whereami app will be exposed by the Ingress Gateway.\nDefine VirtualService Define the VirtualService resource in order to establish the Ingress Gateway routing to the Whereami app:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$WHERE_AMI_DIR_NAME/base/virtualservice.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: whereami spec: hosts: - \"*\" gateways: - ${INGRESS_GATEWAY_NAMESPACE}/${INGRESS_GATEWAY_NAME} http: - route: - destination: host: whereami port: number: 80 EOF Update the Kustomize base overlay:\ncd ${WORK_DIR}$WHERE_AMI_DIR_NAME/base kustomize edit add resource virtualservice.yaml Define Staging namespace overlay cd ${WORK_DIR}$WHERE_AMI_DIR_NAME/staging kustomize edit add resource ../base kustomize edit set namespace $WHEREAMI_NAMESPACE Info The kustomization.yaml file was already existing from the GitHub repository template used when we created the Whereami app repository.\nUpdate the Staging Kustomize overlay in order to set the private container image and the proper hosts value in the VirtualService resource:\ncat \u003c\u003cEOF \u003e\u003e ${WORK_DIR}$WHERE_AMI_DIR_NAME/staging/kustomization.yaml patchesJson6902: - target: kind: Deployment name: whereami patch: |- - op: replace path: /spec/template/spec/containers/0/image value: ${PRIVATE_WHEREAMI_IMAGE_NAME} - target: kind: VirtualService name: whereami patch: |- - op: replace path: /spec/hosts value: - ${WHERE_AMI_INGRESS_GATEWAY_HOST_NAME} EOF Deploy Kubernetes manifests cd ${WORK_DIR}$WHERE_AMI_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Whereami app\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the Whereami app repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $WHEREAMI_NAMESPACE Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the Whereami app repository:\ncd ${WORK_DIR}$WHERE_AMI_DIR_NAME \u0026\u0026 gh run list Check the Whereami app Navigate to the Whereami app, click on the link displayed by the command below:\necho -e \"https://${WHERE_AMI_INGRESS_GATEWAY_HOST_NAME}\" You should see the error: RBAC: access denied. In the next section, you will see how to track this error and how to fix it.\n",
    "description": "Duration: 10 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "asm"
    ],
    "title": "Deploy app",
    "uri": "/acm-workshop/whereami/deploy-app/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, you will deploy the Bank of Anthos apps.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Get upstream Kubernetes manifests Get the upstream Kubernetes manifests:\ncd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME kpt pkg get https://github.com/GoogleCloudPlatform/bank-of-anthos/kubernetes-manifests mv kubernetes-manifests upstream cd upstream rm Kptfile curl -L https://raw.githubusercontent.com/GoogleCloudPlatform/bank-of-anthos/main/extras/jwt/jwt-secret.yaml \u003e jwt-secret.yaml kustomize create --autodetect Create base overlay Create Kustomize base overlay files:\nmkdir ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base cd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base kustomize create kustomize edit add resource ../upstream cat \u003c\u003cEOF \u003e\u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/kustomization.yaml patchesJson6902: - target: kind: Service name: frontend patch: |- - op: replace path: /spec/type value: ClusterIP EOF Info Here we are changing the Service type to ClusterIP because the frontend app will be exposed by the Ingress Gateway.\nDefine VirtualService Define the VirtualService resource in order to establish the Ingress Gateway routing to the Online Boutique apps:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/virtualservice.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: frontend spec: hosts: - \"*\" gateways: - ${INGRESS_GATEWAY_NAMESPACE}/${INGRESS_GATEWAY_NAME} http: - route: - destination: host: frontend port: number: 80 EOF Update the Kustomize base overlay:\ncd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base kustomize edit add resource virtualservice.yaml Define Staging namespace overlay cd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/staging kustomize edit add resource ../base kustomize edit set namespace $BANKOFANTHOS_NAMESPACE Info The kustomization.yaml file was already existing from the GitHub repository template used when we created the Bank of Anthos app repository.\nUpdate the Staging namespace overlay Set the proper hosts value in the VirtualService:\nmkdir ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/staging/virtualservice cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/staging/virtualservice/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1alpha1 kind: Component patchesJson6902: - target: kind: VirtualService name: frontend patch: |- - op: replace path: /spec/hosts value: - ${BANK_OF_ANTHOS_INGRESS_GATEWAY_HOST_NAME} EOF Update the StatefulSets and Deployments’s container images to point to the private Artifact Registry:\nmkdir ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/staging/container-images cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/staging/container-images/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1alpha1 kind: Component patchesJson6902: - target: kind: StatefulSet name: accounts-db patch: |- - op: replace path: /spec/template/spec/containers/0/image value: ${PRIVATE_BANK_OF_ANTHOS_REGISTRY}/accounts-db:${BANK_OF_ANTHOS_VERSION} - target: kind: Deployment name: balancereader patch: |- - op: replace path: /spec/template/spec/containers/0/image value: ${PRIVATE_BANK_OF_ANTHOS_REGISTRY}/balancereader:${BANK_OF_ANTHOS_VERSION} - target: kind: Deployment name: contacts patch: |- - op: replace path: /spec/template/spec/containers/0/image value: ${PRIVATE_BANK_OF_ANTHOS_REGISTRY}/contacts:${BANK_OF_ANTHOS_VERSION} - target: kind: Deployment name: frontend patch: |- - op: replace path: /spec/template/spec/containers/0/image value: ${PRIVATE_BANK_OF_ANTHOS_REGISTRY}/frontend:${BANK_OF_ANTHOS_VERSION} - target: kind: StatefulSet name: ledger-db patch: |- - op: replace path: /spec/template/spec/containers/0/image value: ${PRIVATE_BANK_OF_ANTHOS_REGISTRY}/ledger-db:${BANK_OF_ANTHOS_VERSION} - target: kind: Deployment name: ledgerwriter patch: |- - op: replace path: /spec/template/spec/containers/0/image value: ${PRIVATE_BANK_OF_ANTHOS_REGISTRY}/ledgerwriter:${BANK_OF_ANTHOS_VERSION} - target: kind: Deployment name: loadgenerator patch: |- - op: replace path: /spec/template/spec/containers/0/image value: ${PRIVATE_BANK_OF_ANTHOS_REGISTRY}/loadgenerator:${BANK_OF_ANTHOS_VERSION} - target: kind: Deployment name: transactionhistory patch: |- - op: replace path: /spec/template/spec/containers/0/image value: ${PRIVATE_BANK_OF_ANTHOS_REGISTRY}/transactionhistory:${BANK_OF_ANTHOS_VERSION} - target: kind: Deployment name: userservice patch: |- - op: replace path: /spec/template/spec/containers/0/image value: ${PRIVATE_BANK_OF_ANTHOS_REGISTRY}/userservice:${BANK_OF_ANTHOS_VERSION} EOF Update the StatefulSets and Deployments’s container images to point to the private Artifact Registry:\nmkdir ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/staging/disable-monitoring cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/staging/disable-monitoring/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1alpha1 kind: Component patchesStrategicMerge: - |- apiVersion: apps/v1 kind: Deployment metadata: name: balancereader spec: template: spec: containers: - name: balancereader env: - name: ENABLE_TRACING value: \"false\" - name: ENABLE_METRICS value: \"false\" - |- apiVersion: apps/v1 kind: Deployment metadata: name: contacts spec: template: spec: containers: - name: contacts env: - name: ENABLE_TRACING value: \"false\" - |- apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: template: spec: containers: - name: frontend env: - name: ENABLE_TRACING value: \"false\" - |- apiVersion: apps/v1 kind: Deployment metadata: name: ledgerwriter spec: template: spec: containers: - name: ledgerwriter env: - name: ENABLE_TRACING value: \"false\" - name: ENABLE_METRICS value: \"false\" - |- apiVersion: apps/v1 kind: Deployment metadata: name: transactionhistory spec: template: spec: containers: - name: transactionhistory env: - name: ENABLE_TRACING value: \"false\" - name: ENABLE_METRICS value: \"false\" - |- apiVersion: apps/v1 kind: Deployment metadata: name: userservice spec: template: spec: containers: - name: userservice env: - name: ENABLE_TRACING value: \"false\" EOF Update the Staging Kustomize overlay:\ncd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/staging kustomize edit add component virtualservice kustomize edit add component container-images kustomize edit add component disable-monitoring Deploy Kubernetes manifests cd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Bank of Anthos apps\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the Bank of Anthos apps repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $BANKOFANTHOS_NAMESPACE Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the Bank of Anthos apps repository:\ncd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME \u0026\u0026 gh run list Check the Bank of Anthos website Navigate to the Bank of Anthos website, click on the link displayed by the command below:\necho -e \"https://${BANK_OF_ANTHOS_INGRESS_GATEWAY_HOST_NAME}\" You should see the error: RBAC: access denied. In the next section, you will see how to track this error and how to fix it.\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "asm"
    ],
    "title": "Deploy apps",
    "uri": "/acm-workshop/bankofanthos/deploy-apps/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Platform Admin\nIn this section, you will see the Policy Controller violation regarding to the missing NetworkPolicies in the Ingress Gateway. Finally, you will fix this violation by deploying the associated resources.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh See the Policy Controller violations See the Policy Controller violations in the GKE cluster, by running this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/policy_controller/dashboard?project=${TENANT_PROJECT_ID}\" You will see that the K8sRequireNamespaceNetworkPolicies Constraint has this violation: Namespace \u003casm-ingress\u003e does not have a NetworkPolicy.\nLet’s fix it!\nDefine NetworkPolicies cat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE/networkpolicy_denyall.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: denyall namespace: ${INGRESS_GATEWAY_NAMESPACE} spec: podSelector: {} policyTypes: - Ingress - Egress EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE/networkpolicy_ingress-gateway.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} spec: podSelector: matchLabels: app: ${INGRESS_GATEWAY_NAME} policyTypes: - Ingress - Egress ingress: - {} egress: - {} EOF Deploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Ingress Gateway NetworkPolicies\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the GKE cluster configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\".\nSee the Policy Controller Constraints without any violations in the GKE cluster, by running this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/policy_controller/dashboard?project=${TENANT_PROJECT_ID}\" List the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 5 min | Persona: Platform Admin",
    "tags": [
      "platform-admin",
      "security-tips",
      "policies"
    ],
    "title": "Deploy NetworkPolicies",
    "uri": "/acm-workshop/ingress-gateway/deploy-network-policies/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Platform Admin\nIn this section, you will monitor Cloud Armor security policies logs (WAF rules).\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh In the Google Cloud console, navigate to Network Security \u003e Cloud Armor service. Click on the link displayed by the command below:\necho -e \"https://console.cloud.google.com/net-security/securitypolicies/details/${SECURITY_POLICY_NAME}?project=${TENANT_PROJECT_ID}\" Using Cloud Logging, you can view every request evaluated by a Google Cloud Armor security policy and the outcome or action taken.\nSelect the Logs tab and click on View policy logs. From here, change Last 1 hour by Last 7 days (top left) and enable the Show query toggle (top right):\nIn the Query field you could add a new line with jsonPayload.enforcedSecurityPolicy.outcome=\"DENY\" for example to see all the denied requests by the WAF rules you set up earlier in this workshop.\nYou could also leverage the gcloud command below to get such insights.\nRun this command in Cloud Shell:\nfilter=\"resource.type=\\\"http_load_balancer\\\" \"\\ \"jsonPayload.enforcedSecurityPolicy.name=\\\"${SECURITY_POLICY_NAME}\\\" \"\\ \"jsonPayload.enforcedSecurityPolicy.outcome=\\\"DENY\\\"\" gcloud logging read --project $TENANT_PROJECT_ID \"$filter\" You can also view the number of allowed and denied requests by Cloud Armor in Monitoring \u003e Metrics Explorer. Click on the link displayed by the command below:\necho -e \"https://console.cloud.google.com/monitoring/metrics-explorer?pageState=%7B%22xyChart%22:%7B%22dataSets%22:%5B%7B%22timeSeriesFilter%22:%7B%22filter%22:%22metric.type%3D%5C%22networksecurity.googleapis.com%2Fhttps%2Frequest_count%5C%22%20resource.type%3D%5C%22network_security_policy%5C%22%22,%22minAlignmentPeriod%22:%2260s%22,%22aggregations%22:%5B%7B%22perSeriesAligner%22:%22ALIGN_SUM%22,%22crossSeriesReducer%22:%22REDUCE_NONE%22,%22alignmentPeriod%22:%2260s%22,%22groupByFields%22:%5B%5D%7D,%7B%22crossSeriesReducer%22:%22REDUCE_NONE%22,%22alignmentPeriod%22:%2260s%22,%22groupByFields%22:%5B%5D%7D%5D%7D,%22targetAxis%22:%22Y1%22,%22plotType%22:%22LINE%22%7D%5D,%22options%22:%7B%22mode%22:%22COLOR%22%7D,%22constantLines%22:%5B%5D,%22timeshiftDuration%22:%220s%22,%22y1Axis%22:%7B%22label%22:%22y1Axis%22,%22scale%22:%22LINEAR%22%7D%7D,%22isAutoRefresh%22:true,%22timeSelection%22:%7B%22timeRange%22:%221h%22%7D%7D\u0026project=${TENANT_PROJECT_ID}\" ",
    "description": "Duration: 5 min | Persona: Platform Admin",
    "tags": [
      "monitoring",
      "platform-admin",
      "security-tips"
    ],
    "title": "Monitor WAF rules",
    "uri": "/acm-workshop/monitoring-and-audit/monitor-waf-rules/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, you will copy the Online Boutique apps container images and the Helm chart in your private Artifact Registry. You will also scan one container image.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh ONLINE_BOUTIQUE_VERSION=v0.5.0 echo \"export ONLINE_BOUTIQUE_VERSION=${ONLINE_BOUTIQUE_VERSION}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh PRIVATE_ONLINE_BOUTIQUE_REGISTRY=$CONTAINER_REGISTRY_REPOSITORY/onlineboutique echo \"export PRIVATE_ONLINE_BOUTIQUE_REGISTRY=${PRIVATE_ONLINE_BOUTIQUE_REGISTRY}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Prepare the container images Copy the public container images to your private registry:\nUPSTREAM_ONLINE_BOUTIQUE_CONTAINER_REGISTRY=gcr.io/google-samples/microservices-demo HTTP_SERVICES=\"frontend loadgenerator\" TAG=$ONLINE_BOUTIQUE_VERSION for s in $HTTP_SERVICES; do crane copy $UPSTREAM_ONLINE_BOUTIQUE_CONTAINER_REGISTRY/$s:$TAG $PRIVATE_ONLINE_BOUTIQUE_REGISTRY/$s:$TAG; done GRPC_SERVICES=\"adservice cartservice checkoutservice currencyservice emailservice paymentservice productcatalogservice recommendationservice shippingservice\" TAG=$ONLINE_BOUTIQUE_VERSION-native-grpc-probes for s in $GRPC_SERVICES; do crane copy $UPSTREAM_ONLINE_BOUTIQUE_CONTAINER_REGISTRY/$s:$TAG $PRIVATE_ONLINE_BOUTIQUE_REGISTRY/$s:$TAG; done crane copy redis:alpine $PRIVATE_ONLINE_BOUTIQUE_REGISTRY/redis:alpine Tip We are making the copy of the gRPC services supporting the native Kubernetes health probes in order to get the associated optimized images, learn more about this here.\nList the container images in your private registry:\ngcloud artifacts docker images list $CONTAINER_REGISTRY_REPOSITORY \\ --include-tags Scan the cartservice container image:\ngcloud artifacts docker images scan $PRIVATE_ONLINE_BOUTIQUE_REGISTRY/cartservice:$ONLINE_BOUTIQUE_VERSION-native-grpc-probes \\ --project ${TENANT_PROJECT_ID} \\ --remote \\ --format='value(response.scan)' \u003e ${WORK_DIR}scan_id.txt gcloud artifacts docker images list-vulnerabilities $(cat ${WORK_DIR}scan_id.txt) \\ --project ${TENANT_PROJECT_ID} \\ --format='table(vulnerability.effectiveSeverity, vulnerability.cvssScore, noteName, vulnerability.packageIssue[0].affectedPackage, vulnerability.packageIssue[0].affectedVersion.name, vulnerability.packageIssue[0].fixedVersion.name)' Tip You could use this gcloud artifacts docker images scan command in your Continuous Integration system in order to detect as early as possible for example Critical or High vulnerabilities.\nPrepare the Helm chart Copy the public Helm chart to your private registry:\nUPSTREAM_ONLINE_BOUTIQUE_HELM_CHART_REGISTRY=us-docker.pkg.dev/online-boutique-ci/charts/onlineboutique helm pull oci://${UPSTREAM_ONLINE_BOUTIQUE_HELM_CHART_REGISTRY} --version ${ONLINE_BOUTIQUE_VERSION:1} helm push onlineboutique-${ONLINE_BOUTIQUE_VERSION:1}.tgz oci://${CHART_REGISTRY_REPOSITORY} List the container images in your private registry:\ngcloud artifacts docker images list $CHART_REGISTRY_REPOSITORY \\ --include-tags ",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "helm",
      "security-tips"
    ],
    "title": "Prepare containers and chart",
    "uri": "/acm-workshop/onlineboutique/prepare-containers-and-chart/index.html"
  },
  {
    "content": " Duration: 10 min | Persona: Apps Operator\nIn this section, you will secure the access by TLS to the Memorystore (Redis) instance from the OnlineBoutique’s cartservice app, without updating the source code of the app, just with Istio’s capabilities.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export CART_MEMORYSTORE_HOST=${REDIS_NAME}.memorystore-redis.${ONLINEBOUTIQUE_NAMESPACE}\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Info The CART_MEMORYSTORE_HOST has been built in order to explicitly represent the Memorystore (Redis) endpoint on an Istio perspective. This name will be leveraged in 3 Istio resources: ServiceEntry, DestinationRule and Sidecar generated by the Online Boutique’s Helm chart.\nUpdate RepoSync to deploy the Online Boutique’s Helm chart Get Memorystore (Redis) connection information:\nexport REDIS_TLS_IP=$(gcloud redis instances describe $REDIS_TLS_NAME --region $GKE_LOCATION --project $TENANT_PROJECT_ID --format='get(host)') export REDIS_TLS_PORT=$(gcloud redis instances describe $REDIS_TLS_NAME --region $GKE_LOCATION --project $TENANT_PROJECT_ID --format='get(port)') export REDIS_TLS_CERT=$(gcloud redis instances describe $REDIS_TLS_NAME --region $GKE_LOCATION --project $TENANT_PROJECT_ID --format 'get(serverCaCerts[0].cert)') Define the RepoSync to deploy the Online Boutique’s Helm chart with the cartservice pointing to the Memorystore (Redis) database:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs/$ONLINEBOUTIQUE_NAMESPACE/repo-sync.yaml apiVersion: configsync.gke.io/v1beta1 kind: RepoSync metadata: name: repo-sync namespace: ${ONLINEBOUTIQUE_NAMESPACE} spec: sourceFormat: unstructured sourceType: helm helm: repo: oci://${CHART_REGISTRY_REPOSITORY} chart: ${ONLINEBOUTIQUE_NAMESPACE} version: ${ONLINE_BOUTIQUE_VERSION:1} releaseName: ${ONLINEBOUTIQUE_NAMESPACE} auth: gcpserviceaccount gcpServiceAccountEmail: ${HELM_CHARTS_READER_GSA}@${TENANT_PROJECT_ID}.iam.gserviceaccount.com values: cartDatabase: inClusterRedis: create: false connectionString: ${REDIS_TLS_IP}:${REDIS_TLS_PORT} externalRedisTlsOrigination: enable: true certificate: | $(echo -e \"${REDIS_TLS_CERT}\" | sed 's/^/ /') endpointAddress: ${REDIS_TLS_IP} endpointPort: ${REDIS_TLS_PORT} images: repository: ${PRIVATE_ONLINE_BOUTIQUE_REGISTRY} tag: ${ONLINE_BOUTIQUE_VERSION} nativeGrpcHealthCheck: true seccompProfile: enable: true loadGenerator: checkFrontendInitContainer: false frontend: externalService: false virtualService: create: true gateway: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} labelKey: asm labelValue: ingressgateway hosts: - ${ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME} serviceAccounts: create: true authorizationPolicies: create: true networkPolicies: create: true sidecars: create: true EOF Info This will change the REDIS_ADDR environment variable of the cartservice to point to the Memorystore (Redis) database as well as removing the Deployment and the Service of the default in-cluster redis database. Also, in order to properly set up the Istio origination, this Helm chart will generate multiple resources: a ServiceEntry to register the Memorystore (Redis) instance in the Mesh as an external endpoint with its private IP address and port; a DestinationRule to configure the outgoing connections to this Memorystore (Redis) instance with TLS in SIMPLE mode (not MUTUAL) with its associated Certificate Authority and finally will annotate the cartservice Deployment to mount the certificate as a Secret via its sidecar proxy.\nNote The certificate value is in clear text in this RepoSync manifest in the Git repository. It is not a good practice, you shouldn’t do that for your own scenario. In the future, this will be fixed in this workshop with for example the use of the Google Secret Manager provider for the Secret Store CSI Driver.\nDeploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Secure Memorystore (Redis) access\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the Online Boutique apps repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $ONLINEBOUTIQUE_NAMESPACE Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list Check the Online Boutique website Navigate to the Online Boutique website, click on the link displayed by the command below:\necho -e \"https://${ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME}\" You should still have the Online Boutique website working successfully, but now linked to an external redis database with encryption in-transit between this Memorystore (Redis) database and the cartservice.\n",
    "description": "Duration: 10 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "asm",
      "helm",
      "security-tips"
    ],
    "title": "Secure Memorystore access",
    "uri": "/acm-workshop/onlineboutique/memorystore/secure-memorystore-access/index.html"
  },
  {
    "content": " Duration: 10 min | Persona: Platform Admin\nIn this section, you will set up Config Sync and Policy Controller for the GKE cluster. You will also configure a main/root GitHub repository for this GKE cluster.\nDefine variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh echo \"export GKE_CONFIGS_DIR_NAME=acm-workshop-gke-configs-repo\" \u003e\u003e ${WORK_DIR}acm-workshop-variables.sh source ${WORK_DIR}acm-workshop-variables.sh Define ACM GKEHubFeature Define the ACM GKEHubFeature resource:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/gke-hub-feature-acm.yaml apiVersion: gkehub.cnrm.cloud.google.com/v1beta1 kind: GKEHubFeature metadata: name: configmanagement namespace: ${TENANT_PROJECT_ID} spec: projectRef: name: ${TENANT_PROJECT_ID} location: global resourceID: configmanagement EOF Note The resourceID must be configmanagement if you want to use Anthos Config Management feature.\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/gke-hub-membership.yaml apiVersion: gkehub.cnrm.cloud.google.com/v1beta1 kind: GKEHubMembership metadata: name: ${GKE_NAME} namespace: ${TENANT_PROJECT_ID} annotations: cnrm.cloud.google.com/project-id: ${TENANT_PROJECT_ID} config.kubernetes.io/depends-on: container.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/ContainerCluster/${GKE_NAME} spec: location: global authority: issuer: https://container.googleapis.com/v1/projects/${TENANT_PROJECT_ID}/locations/${GKE_LOCATION}/clusters/${GKE_NAME} endpoint: gkeCluster: resourceRef: name: ${GKE_NAME} EOF Create a main GitHub repository for all GKE configs Create a dedicated GitHub repository where we will commit all the configs, policies, etc. we want to deploy in this GKE cluster:\ncd ${WORK_DIR} gh repo create $GKE_CONFIGS_DIR_NAME --public --clone --template https://github.com/mathieu-benoit/config-sync-template-repo cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME git pull git checkout main GKE_CONFIGS_REPO_URL=$(gh repo view --json url --jq .url) Define RootSync with this GitHub repository cat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/gke-acm-membership.yaml apiVersion: gkehub.cnrm.cloud.google.com/v1beta1 kind: GKEHubFeatureMembership metadata: name: ${GKE_NAME}-acm-membership namespace: ${TENANT_PROJECT_ID} annotations: config.kubernetes.io/depends-on: gkehub.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/GKEHubMembership/${GKE_NAME},gkehub.cnrm.cloud.google.com/namespaces/${TENANT_PROJECT_ID}/GKEHubFeature/configmanagement spec: projectRef: name: ${TENANT_PROJECT_ID} location: global membershipRef: name: ${GKE_NAME} featureRef: name: configmanagement configmanagement: configSync: sourceFormat: unstructured preventDrift: true git: policyDir: . secretType: none syncBranch: main syncRepo: ${GKE_CONFIGS_REPO_URL} policyController: enabled: true referentialRulesEnabled: true logDeniesEnabled: true templateLibraryInstalled: true version: \"1.14.1\" EOF Tip We explicitly set the Config Management’s version field with the current version. It’s a best practice to do this, as you are responsible to manually upgrade this component as new versions are coming. So you will be able to update this file accordingly in order to trigger the upgrade of Config Management with the new version.\nInfo We explicitly set the Policy Controller’s templateLibraryInstalled field to true, in order to install the default library of ConstraintTemplates.\nDeploy Kubernetes manifests cd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"GitOps for GKE cluster configs\" \u0026\u0026 git push origin main Check deployments graph TD; GKEHubFeature-.-\u003eProject GKEHubFeatureMembership--\u003eGKEHubMembership GKEHubFeatureMembership--\u003eGKEHubFeature GKEHubFeatureMembership-.-\u003eProject GKEHubMembership--\u003eContainerCluster List the Kubernetes resources managed by Config Sync in Config Controller for the Tenant project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $TENANT_PROJECT_ID Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Tenant project configs repository:\ncd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME \u0026\u0026 gh run list List the Kubernetes resources managed by Config Sync in GKE cluster for the GKE cluster configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list List the Google Cloud resources created:\ngcloud container fleet memberships list \\ --project $TENANT_PROJECT_ID gcloud beta container fleet config-management status \\ --project $TENANT_PROJECT_ID Wait and re-run this command above until you see the resources created.\n",
    "description": "Duration: 10 min | Persona: Platform Admin",
    "tags": [
      "gitops-tips",
      "kcc",
      "platform-admin"
    ],
    "title": "Set up GKE configs's Git repo",
    "uri": "/acm-workshop/gke-cluster/set-up-gke-configs-git-repo/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Platform Admin\nIn this section, you will set up an monitoring notification channel with you email and a generic alert policy on URLs uptime checks for the Tenant project.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Define variables for this page:\nexport NOTIFICATION_CHANNEL_EMAIL_ADDRESS=FIXME export NOTIFICATION_CHANNEL_NAME=monitoringnotificationchannel-email Tip Set your own email address for the NOTIFICATION_CHANNEL_EMAIL_ADDRESS variable, this will be used when defining the monitoring notification channel below.\nDefine the monitoring notification channel with your email Define the VPC:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/monitoring-notification-channel-email.yaml apiVersion: monitoring.cnrm.cloud.google.com/v1beta1 kind: MonitoringNotificationChannel metadata: annotations: cnrm.cloud.google.com/project-id: ${TENANT_PROJECT_ID} name: ${NOTIFICATION_CHANNEL_NAME} spec: type: email labels: email_address: ${NOTIFICATION_CHANNEL_EMAIL_ADDRESS} enabled: true EOF Define the Alert policy based on the uptime checks Define the MonitoringAlertPolicy:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/monitoring-alert-policy-uptime-checks.yaml apiVersion: monitoring.cnrm.cloud.google.com/v1beta1 kind: MonitoringAlertPolicy metadata: annotations: cnrm.cloud.google.com/project-id: ${TENANT_PROJECT_ID} name: monitoring-alert-policy-uptime-checks spec: displayName: Failure of uptime checks enabled: true notificationChannels: - name: ${NOTIFICATION_CHANNEL_NAME} combiner: OR conditions: - displayName: Failure of uptime checks conditionThreshold: filter: metric.type=\"monitoring.googleapis.com/uptime_check/check_passed\" AND resource.type=\"uptime_url\" aggregations: - perSeriesAligner: ALIGN_NEXT_OLDER alignmentPeriod: 1200s crossSeriesReducer: REDUCE_COUNT_FALSE groupByFields: - resource.label.* comparison: COMPARISON_GT thresholdValue: 1 duration: 60s trigger: count: 1 EOF Deploy Kubernetes manifests cd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Monitoring features for Tenant project\" \u0026\u0026 git push origin main Check deployments graph TD; MonitoringAlertPolicy-.-\u003eProject MonitoringNotificationChannel-.-\u003eProject MonitoringAlertPolicy--\u003eMonitoringNotificationChannel List the Kubernetes resources managed by Config Sync in Config Controller for the Tenant project configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${HOST_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $HOST_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $TENANT_PROJECT_ID Wait and re-run this command above until you see \"status\": \"SYNCED\". All the managed_resources listed should have STATUS: Current too.\nList the GitHub runs for the Tenant project configs repository:\ncd ${WORK_DIR}$TENANT_PROJECT_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 5 min | Persona: Platform Admin",
    "tags": [
      "kcc",
      "monitoring",
      "platform-admin"
    ],
    "title": "Set up Monitoring",
    "uri": "/acm-workshop/tenant-project/set-up-monitoring/index.html"
  },
  {
    "content": "Set up Artifact Registry.\nAllow Artifact RegistryDuration: 5 min | Persona: Org Admin\nCreate Artifact RegistryDuration: 5 min | Persona: Platform Admin\nEnforce Artifact Registry policiesDuration: 10 min | Persona: Platform Admin\n",
    "description": "",
    "tags": null,
    "title": "5. Artifact Registry",
    "uri": "/acm-workshop/artifact-registry/index.html"
  },
  {
    "content": "Once all the services used in this workshop are provisioned, here is an estimate for a 30 days period for both projects:\nHost project: ~335.60 $USD Tenant project: ~2838.72 $USD Total: ~ 3174.32 $USD\nBreakdown for the Host project:\nBreakdown for the Tenant project:\n",
    "description": "",
    "tags": null,
    "title": "Costs",
    "uri": "/acm-workshop/overview/costs/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, you will deploy the Online Boutique apps, via Config Sync and its Helm chart.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Define RepoSync to deploy the Online Boutique’s Helm chart Define the RepoSync to deploy the Online Boutique’s Helm chart:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs/$ONLINEBOUTIQUE_NAMESPACE/repo-sync.yaml apiVersion: configsync.gke.io/v1beta1 kind: RepoSync metadata: name: repo-sync namespace: ${ONLINEBOUTIQUE_NAMESPACE} spec: sourceFormat: unstructured sourceType: helm helm: repo: oci://${CHART_REGISTRY_REPOSITORY} chart: ${ONLINEBOUTIQUE_NAMESPACE} version: ${ONLINE_BOUTIQUE_VERSION:1} releaseName: ${ONLINEBOUTIQUE_NAMESPACE} auth: gcpserviceaccount gcpServiceAccountEmail: ${HELM_CHARTS_READER_GSA}@${TENANT_PROJECT_ID}.iam.gserviceaccount.com values: cartDatabase: inClusterRedis: publicRepository: false images: repository: ${PRIVATE_ONLINE_BOUTIQUE_REGISTRY} tag: ${ONLINE_BOUTIQUE_VERSION} nativeGrpcHealthCheck: true seccompProfile: enable: true loadGenerator: checkFrontendInitContainer: false frontend: externalService: false virtualService: create: true gateway: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} labelKey: asm labelValue: ingressgateway hosts: - ${ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME} EOF Info Here we are deleting the Service frontend-external because the frontend app will be exposed by the Ingress Gateway. The associated VirtualService will be generated in order to establish the link between the Ingress Gateway and the Online Boutique’s frontend app. We are also updating the container image repository to use our own private Artifact Registry. Finally, we are also leveraging more secure feature such as nativeGrpcHealthCheck and seccompProfile. You could read more here about all the options the Online Boutique’s Helm chart exposes.\nDeploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Online Boutique apps\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the Online Boutique apps repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $ONLINEBOUTIQUE_NAMESPACE Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list Check the Online Boutique apps Navigate to the Online Boutique apps, click on the link displayed by the command below:\necho -e \"https://${ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME}\" You should see the error: RBAC: access denied. In the next section, you will see how to track this error and how to fix it.\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "asm",
      "helm"
    ],
    "title": "Deploy apps",
    "uri": "/acm-workshop/onlineboutique/deploy-apps/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, you will see how to track the AuthorizationPolicies issue and then you will deploy granular and specific ServiceAccounts and AuthorizationPolicies for the Bank of Anthos namespace to fix this issue.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh See the AuthorizationPolicies issue See the AuthorizationPolicies issue in the GKE cluster for the Bank of Anthos namespace, by running this command and click on this link:\necho -e \"https://console.cloud.google.com/anthos/security/workload-view/Deployment/${GKE_LOCATION}/${GKE_NAME}/${BANKOFANTHOS_NAMESPACE}/frontend?project=${TENANT_PROJECT_ID}\" Under the Service requests section on this page, you will see some Inbound denials. If you click on View logs you will be able to see via Cloud Logging the details of the errors. That’s where you will the logs with status: 403 and response_details: \"AuthzDenied\".\nLet’s fix it!\nDefine ServiceAccounts mkdir ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/serviceaccounts cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/serviceaccounts/serviceaccount_accounts-db.yaml apiVersion: v1 kind: ServiceAccount metadata: name: accounts-db EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/serviceaccounts/serviceaccount_balancereader.yaml apiVersion: v1 kind: ServiceAccount metadata: name: balancereader EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/serviceaccounts/serviceaccount_contacts.yaml apiVersion: v1 kind: ServiceAccount metadata: name: contacts EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/serviceaccounts/serviceaccount_frontend.yaml apiVersion: v1 kind: ServiceAccount metadata: name: frontend EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/serviceaccounts/serviceaccount_ledger-db.yaml apiVersion: v1 kind: ServiceAccount metadata: name: ledger-db EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/serviceaccounts/serviceaccount_ledgerwriter.yaml apiVersion: v1 kind: ServiceAccount metadata: name: ledgerwriter EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/serviceaccounts/serviceaccount_loadgenerator.yaml apiVersion: v1 kind: ServiceAccount metadata: name: loadgenerator EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/serviceaccounts/serviceaccount_transactionhistory.yaml apiVersion: v1 kind: ServiceAccount metadata: name: transactionhistory EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/serviceaccounts/serviceaccount_userservice.yaml apiVersion: v1 kind: ServiceAccount metadata: name: userservice EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/serviceaccounts/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1alpha1 kind: Component resources: - serviceaccount_accounts-db.yaml - serviceaccount_balancereader.yaml - serviceaccount_contacts.yaml - serviceaccount_frontend.yaml - serviceaccount_ledger-db.yaml - serviceaccount_ledgerwriter.yaml - serviceaccount_loadgenerator.yaml - serviceaccount_transactionhistory.yaml - serviceaccount_userservice.yaml EOF Update ServiceAccounts in Deployments cat \u003c\u003cEOF \u003e\u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/serviceaccounts/kustomization.yaml patchesJson6902: - target: kind: StatefulSet name: accounts-db patch: |- - op: replace path: /spec/template/spec/serviceAccountName value: accounts-db - target: kind: Deployment name: balancereader patch: |- - op: replace path: /spec/template/spec/serviceAccountName value: balancereader - target: kind: Deployment name: contacts patch: |- - op: replace path: /spec/template/spec/serviceAccountName value: contacts - target: kind: Deployment name: frontend patch: |- - op: replace path: /spec/template/spec/serviceAccountName value: frontend - target: kind: StatefulSet name: ledger-db patch: |- - op: replace path: /spec/template/spec/serviceAccountName value: ledger-db - target: kind: Deployment name: ledgerwriter patch: |- - op: replace path: /spec/template/spec/serviceAccountName value: ledgerwriter - target: kind: Deployment name: loadgenerator patch: |- - op: replace path: /spec/template/spec/serviceAccountName value: loadgenerator - target: kind: Deployment name: transactionhistory patch: |- - op: replace path: /spec/template/spec/serviceAccountName value: transactionhistory - target: kind: Deployment name: userservice patch: |- - op: replace path: /spec/template/spec/serviceAccountName value: userservice EOF Define AuthorizationPolicies mkdir ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/authorizationpolicies cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/authorizationpolicies/authorizationpolicy_accounts-db.yaml apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: accounts-db spec: rules: - from: - source: principals: - cluster.local/ns/${BANKOFANTHOS_NAMESPACE}/sa/contacts - cluster.local/ns/${BANKOFANTHOS_NAMESPACE}/sa/userservice to: - operation: ports: - \"5432\" selector: matchLabels: app: accounts-db EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/authorizationpolicies/authorizationpolicy_balancereader.yaml apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: balancereader spec: rules: - from: - source: principals: - cluster.local/ns/${BANKOFANTHOS_NAMESPACE}/sa/frontend - cluster.local/ns/${BANKOFANTHOS_NAMESPACE}/sa/ledgerwriter to: - operation: methods: - GET paths: - /balances/* ports: - \"8080\" selector: matchLabels: app: balancereader EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/authorizationpolicies/authorizationpolicy_contacts.yaml apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: contacts spec: rules: - from: - source: principals: - cluster.local/ns/${BANKOFANTHOS_NAMESPACE}/sa/frontend to: - operation: methods: - GET - POST paths: - /contacts/* ports: - \"8080\" selector: matchLabels: app: contacts EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/authorizationpolicies/authorizationpolicy_frontend.yaml apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: frontend spec: rules: - from: - source: principals: - cluster.local/ns/${BANKOFANTHOS_NAMESPACE}/sa/loadgenerator - cluster.local/ns/${INGRESS_GATEWAY_NAMESPACE}/sa/${INGRESS_GATEWAY_NAME} to: - operation: methods: - GET - POST ports: - \"8080\" selector: matchLabels: app: frontend EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/authorizationpolicies/authorizationpolicy_ledger-db.yaml apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: ledger-db spec: rules: - from: - source: principals: - cluster.local/ns/${BANKOFANTHOS_NAMESPACE}/sa/balancereader - cluster.local/ns/${BANKOFANTHOS_NAMESPACE}/sa/transactionhistory - cluster.local/ns/${BANKOFANTHOS_NAMESPACE}/sa/ledgerwriter to: - operation: ports: - \"5432\" selector: matchLabels: app: ledger-db EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/authorizationpolicies/authorizationpolicy_ledgerwriter.yaml apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: ledgerwriter spec: rules: - from: - source: principals: - cluster.local/ns/${BANKOFANTHOS_NAMESPACE}/sa/frontend to: - operation: methods: - POST paths: - /transactions - /transactions/* ports: - \"8080\" selector: matchLabels: app: ledgerwriter EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/authorizationpolicies/authorizationpolicy_transactionhistory.yaml apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: transactionhistory spec: rules: - from: - source: principals: - cluster.local/ns/${BANKOFANTHOS_NAMESPACE}/sa/frontend to: - operation: methods: - GET paths: - /transactions/* ports: - \"8080\" selector: matchLabels: app: transactionhistory EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/authorizationpolicies/authorizationpolicy_userservice.yaml apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: userservice spec: rules: - from: - source: principals: - cluster.local/ns/${BANKOFANTHOS_NAMESPACE}/sa/frontend to: - operation: methods: - GET - POST paths: - /users - /login ports: - \"8080\" selector: matchLabels: app: userservice EOF cd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/authorizationpolicies kustomize create --autodetect Update the Kustomize base overlay cd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base kustomize edit add component serviceaccounts kustomize edit add resource authorizationpolicies Deploy Kubernetes manifests cd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Bank of Anthos AuthorizationPolicies\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the Bank of Anthos apps repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $BANKOFANTHOS_NAMESPACE Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the Bank of Anthos apps repository:\ncd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME \u0026\u0026 gh run list Check the Bank of Anthos website Navigate to the Bank of Anthos website, click on the link displayed by the command below:\necho -e \"https://${BANK_OF_ANTHOS_INGRESS_GATEWAY_HOST_NAME}\" You should now have the Bank of Anthos website working successfully. Congrats!\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "asm",
      "security-tips"
    ],
    "title": "Deploy AuthorizationPolicies",
    "uri": "/acm-workshop/bankofanthos/deploy-authorization-policies/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Platform Admin\nIn this section, you will deploy granular and specific AuthorizationPolicies for the Ingress Gateway namespace.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Define AuthorizationPolicy cat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/$INGRESS_GATEWAY_NAMESPACE/authorizationpolicy_ingress-gateway.yaml apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} spec: selector: matchLabels: app: ${INGRESS_GATEWAY_NAME} rules: - to: - operation: ports: - \"8080\" EOF Deploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Ingress Gateway AuthorizationPolicy\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the GKE cluster configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 5 min | Persona: Platform Admin",
    "tags": [
      "asm",
      "platform-admin",
      "security-tips"
    ],
    "title": "Deploy AuthorizationPolicies",
    "uri": "/acm-workshop/ingress-gateway/deploy-authorization-policies/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, you will see how to track the AuthorizationPolicies issue and then you will deploy granular and specific AuthorizationPolicies for the Whereami namespace to fix this issue.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh See the AuthorizationPolicies issue See the AuthorizationPolicies issue in the GKE cluster for the Whereami app, by running this command and click on this link:\necho -e \"https://console.cloud.google.com/anthos/security/workload-view/Deployment/${GKE_LOCATION}/${GKE_NAME}/${WHEREAMI_NAMESPACE}/whereami?project=${TENANT_PROJECT_ID}\" Under the Service requests section on this page, you will see some Inbound denials depending on how many times you tried to refresh the Whereami app endpoint. If you click on View logs you will be able to see via Cloud Logging the details of the errors. That’s where you will the logs with status: 403 and response_details: \"AuthzDenied\".\nLet’s fix it!\nDefine AuthorizationPolicy Define a fine granular AuthorizationPolicy:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$WHERE_AMI_DIR_NAME/base/authorizationpolicy_whereami.yaml apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: whereami spec: selector: matchLabels: app: whereami rules: - from: - source: principals: - cluster.local/ns/${INGRESS_GATEWAY_NAMESPACE}/sa/${INGRESS_GATEWAY_NAME} to: - operation: ports: - \"8080\" methods: - GET EOF Update the Kustomize base overlay:\ncd ${WORK_DIR}$WHERE_AMI_DIR_NAME/base kustomize edit add resource authorizationpolicy_whereami.yaml Deploy Kubernetes manifests cd ${WORK_DIR}$WHERE_AMI_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Whereami AuthorizationPolicy\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the Whereami app repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $WHEREAMI_NAMESPACE Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the Whereami app repository:\ncd ${WORK_DIR}$WHERE_AMI_DIR_NAME \u0026\u0026 gh run list Check the Whereami app Navigate to the Whereami app, click on the link displayed by the command below:\necho -e \"https://${WHERE_AMI_INGRESS_GATEWAY_HOST_NAME}\" You should now have the Whereami app working successfully. Congrats!\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "asm",
      "security-tips"
    ],
    "title": "Deploy AuthorizationPolicy",
    "uri": "/acm-workshop/whereami/deploy-authorization-policy/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Platform Admin\nIn this section, you will enforce Kubernetes policies for Pod Security Admission (PSA) and NetworkPolicies.\nDefine variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Enforce Pod Security Admission (PSA) policies As best practice we will ensure that any Namespaces enables the Pod Security Admission (PSA) feature.\nDefine the namespaces-required-psa-label Constraint based on the K8sRequiredLabels ConstraintTemplate for Namespaces:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/policies/constraints/namespaces-required-psa-label.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sRequiredLabels metadata: name: namespaces-required-psa-label annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Requires Namespaces to have the \"pod-security.kubernetes.io/enforce\" label with either the value \"baseline\" or \"restricted\".', remediation: 'Any Namespaces should have the \"pod-security.kubernetes.io/enforce\" label with either the value \"baseline\" or \"restricted\".' }\" spec: enforcementAction: deny match: kinds: - apiGroups: - \"\" kinds: - Namespace excludedNamespaces: - config-management-monitoring - config-management-system - default - gatekeeper-system - istio-system - kube-node-lease - kube-public - kube-system - resource-group-system - poco-trial parameters: labels: - key: pod-security.kubernetes.io/enforce allowedRegex: (baseline|restricted) EOF Note As of now, only the asm-ingress and onlineboutique namespaces support restricted. On the other hand, the whereami and bankofanthos namespaces only support baseline. We are authorizing both here.\nEnforce NetworkPolicies policies Require labels for Namespaces and Pods As a best practice and in order to get the NetworkPolicies working in this workshop, we need to guarantee that any Pods have a label app.\nDefine the pods-required-app-label Constraint based on the K8sRequiredLabels ConstraintTemplate for Pods:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/policies/constraints/pods-required-app-label.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sRequiredLabels metadata: name: pods-required-app-label annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Requires Pods to have the name \"app\" in order to leverage the podSelector feature of NetworkPolicies.', remediation: 'Any Pods should have the \"app\" label.' }\" spec: enforcementAction: deny match: kinds: - apiGroups: - \"\" kinds: - Pod excludedNamespaces: - config-management-monitoring - config-management-system - default - gatekeeper-system - kube-node-lease - kube-public - kube-system - resource-group-system - poco-trial parameters: labels: - key: app EOF Note Complementary to this, on Namespaces the kubernetes.io/metadata.name label automatically set by Kubernetes 1.22+ will be leveraged.\nRequire NetworkPolicies in Namespaces Define the namespaces-required-networkpolicies Constraint based on the K8sRequireNamespaceNetworkPolicies ConstraintTemplate for Namespaces. This Constraint requires that any Namespaces defined in the cluster has a NetworkPolicy:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/policies/constraints/namespaces-required-networkpolicies.yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sRequireNamespaceNetworkPolicies metadata: name: namespaces-required-networkpolicies annotations: policycontroller.gke.io/constraintData: | \"{ description: 'Requires that every namespace defined in the cluster has NetworkPolicies.', remediation: 'Any namespace should have NetworkPolicies. It's highly recommended to have at least a first default deny-all and then one fine granular NetworkPolicy per app.' }\" spec: enforcementAction: dryrun match: kinds: - apiGroups: - \"\" kinds: - Namespace excludedNamespaces: - config-management-monitoring - config-management-system - default - gatekeeper-system - istio-system - kube-node-lease - kube-public - kube-system - resource-group-system EOF Because this is constraint is referential (look at NetworkPolicy in Namespace), we need to define an associated Config in the gatekeeper-system Namespace:\nCreate the gatekeeper-system folder:\nmkdir ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/gatekeeper-system Define the config-referential-constraints Config:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/gatekeeper-system/config-referential-constraints.yaml apiVersion: config.gatekeeper.sh/v1alpha1 kind: Config metadata: name: config namespace: gatekeeper-system spec: sync: syncOnly: - group: \"\" version: \"v1\" kind: \"Namespace\" - group: \"networking.k8s.io\" version: \"v1\" kind: \"NetworkPolicy\" EOF Deploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Policies for Kubernetes resources\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the GKE cluster configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\".\nSee the Policy Controller Constraints without any violations in the GKE cluster, by running this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/policy_controller/dashboard?project=${TENANT_PROJECT_ID}\" List the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 5 min | Persona: Platform Admin",
    "tags": [
      "platform-admin",
      "policies",
      "security-tips"
    ],
    "title": "Enforce Kubernetes policies",
    "uri": "/acm-workshop/gke-cluster/enforce-kubernetes-policies/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, you will monitor security scanning of your GKE workloads configurations in the Google Cloud console in order to leverage these two features:\nScan workloads for configuration issues Scan container images for known vulnerabilities Initialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh In the Google Cloud console, navigate to Kubernetes Engine \u003e Security Posture, click on the link displayed by the command below:\necho -e \"https://console.cloud.google.com/kubernetes/security/dashboard?project=${TENANT_PROJECT_ID}\" On the default Dashboard tab, find the GKE Posture settings tile. And for both features Workload configuration audit and Workload vulnerability audit, click on one of the Seclect clusters button, select your GKE cluster and click on the Turn on audit button.\nThen you have to wait a little bit while these features are being enabled on your GKE cluster and eventually the audit will start:\nAudit in progress. It can take up to 15 minutes to complete.\nWhen the audit is done, you will be able to see the report of the vulnerabilities found. The Workload configuration audit will come first, Workload vulnerability audit will be generated later.\nYou will also be able to see in details the associated concerns:\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "monitoring",
      "security-tips"
    ],
    "title": "Scan workloads",
    "uri": "/acm-workshop/monitoring-and-audit/scan-workloads/index.html"
  },
  {
    "content": "Set up Anthos Service Mesh (ASM).\nAllow ASMDuration: 5 min | Persona: Org Admin\nInstall ASMDuration: 15 min | Persona: Platform Admin\nSet up ASM configsDuration: 5 min | Persona: Platform Admin\nEnforce Service Mesh policiesDuration: 10 min | Persona: Platform Admin\n",
    "description": "",
    "tags": null,
    "title": "6. Service Mesh",
    "uri": "/acm-workshop/service-mesh/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, you will see how to track the AuthorizationPolicies issue and then you will deploy granular and specific AuthorizationPolicies for the Online Boutique namespace to fix this issue.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh See the AuthorizationPolicies issue See the AuthorizationPolicies issue in the GKE cluster for the Online Boutique apps, by running this command and click on this link:\necho -e \"https://console.cloud.google.com/anthos/security/workload-view/Deployment/${GKE_LOCATION}/${GKE_NAME}/${ONLINEBOUTIQUE_NAMESPACE}/frontend?project=${TENANT_PROJECT_ID}\" Under the Service requests section on this page, you will see some Inbound denials. If you click on View logs you will be able to see via Cloud Logging the details of the errors. That’s where you will the logs with status: 403 and response_details: \"AuthzDenied\".\nLet’s fix it!\nUpdate RepoSync to deploy the Online Boutique’s Helm chart Define the RepoSync to deploy the Online Boutique’s Helm chart with both the AuthorizationPolicies and ServiceAccounts:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs/$ONLINEBOUTIQUE_NAMESPACE/repo-sync.yaml apiVersion: configsync.gke.io/v1beta1 kind: RepoSync metadata: name: repo-sync namespace: ${ONLINEBOUTIQUE_NAMESPACE} spec: sourceFormat: unstructured sourceType: helm helm: repo: oci://${CHART_REGISTRY_REPOSITORY} chart: ${ONLINEBOUTIQUE_NAMESPACE} version: ${ONLINE_BOUTIQUE_VERSION:1} releaseName: ${ONLINEBOUTIQUE_NAMESPACE} auth: gcpserviceaccount gcpServiceAccountEmail: ${HELM_CHARTS_READER_GSA}@${TENANT_PROJECT_ID}.iam.gserviceaccount.com values: cartDatabase: inClusterRedis: publicRepository: false images: repository: ${PRIVATE_ONLINE_BOUTIQUE_REGISTRY} tag: ${ONLINE_BOUTIQUE_VERSION} nativeGrpcHealthCheck: true seccompProfile: enable: true loadGenerator: checkFrontendInitContainer: false frontend: externalService: false virtualService: create: true gateway: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} labelKey: asm labelValue: ingressgateway hosts: - ${ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME} serviceAccounts: create: true authorizationPolicies: create: true EOF Info In order to deploy the fine granular AuthorizationPolicies and ServiceAccounts, one of each per app, we just updated the list of values of the Online Boutique’s Helm chart previously configured, with serviceAccounts.create: true and authorizationPolicies.create: true.\nDeploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Online Boutique AuthorizationPolicies and ServiceAccounts\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the Online Boutique apps repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $ONLINEBOUTIQUE_NAMESPACE Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list Check the Online Boutique website Navigate to the Online Boutique website, click on the link displayed by the command below:\necho -e \"https://${ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME}\" You should now have the Online Boutique website working successfully. Congrats!\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "asm",
      "helm",
      "security-tips"
    ],
    "title": "Deploy AuthorizationPolicies",
    "uri": "/acm-workshop/onlineboutique/deploy-authorization-policies/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, you will see the Policy Controller violation regarding to the missing NetworkPolicies in the Bank of Anthos namespace. Then, you will fix this violation by deploying the associated resources.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh See the Policy Controller violations See the Policy Controller violations in the GKE cluster, by running this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/policy_controller/dashboard?project=${TENANT_PROJECT_ID}\" You will see that the K8sRequireNamespaceNetworkPolicies Constraint has this violation: Namespace \u003cbankofanthos\u003e does not have a NetworkPolicy.\nLet’s fix it!\nDefine NetworkPolicies for the Bank of Anthos apps Define a fine granular NetworkPolicy:\nmkdir ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/networkpolicies cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/networkpolicies/networkpolicy_deny-all.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all spec: podSelector: {} policyTypes: - Ingress - Egress EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/networkpolicies/networkpolicy_accounts-db.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: accounts-db spec: egress: - {} ingress: - from: - podSelector: matchLabels: app: contacts - podSelector: matchLabels: app: userservice ports: - port: 5432 protocol: TCP podSelector: matchLabels: app: accounts-db policyTypes: - Ingress - Egress EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/networkpolicies/networkpolicy_balancereader.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: balancereader spec: egress: - {} ingress: - from: - podSelector: matchLabels: app: frontend - podSelector: matchLabels: app: ledgerwriter ports: - port: 8080 protocol: TCP podSelector: matchLabels: app: balancereader policyTypes: - Ingress - Egress EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/networkpolicies/networkpolicy_contacts.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: contacts spec: egress: - {} ingress: - from: - podSelector: matchLabels: app: frontend ports: - port: 8080 protocol: TCP podSelector: matchLabels: app: contacts policyTypes: - Ingress - Egress EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/networkpolicies/networkpolicy_frontend.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: frontend spec: egress: - {} ingress: - from: - podSelector: matchLabels: app: loadgenerator - namespaceSelector: matchLabels: kubernetes.io/metadata.name: ${INGRESS_GATEWAY_NAMESPACE} podSelector: matchLabels: app: ${INGRESS_GATEWAY_NAME} ports: - port: 8080 protocol: TCP podSelector: matchLabels: app: frontend policyTypes: - Ingress - Egress EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/networkpolicies/networkpolicy_ledger-db.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ledger-db spec: egress: - {} ingress: - from: - podSelector: matchLabels: app: ledgerwriter - podSelector: matchLabels: app: balancereader - podSelector: matchLabels: app: transactionhistory ports: - port: 5432 protocol: TCP podSelector: matchLabels: app: ledger-db policyTypes: - Ingress - Egress EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/networkpolicies/networkpolicy_ledgerwriter.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ledgerwriter spec: egress: - {} ingress: - from: - podSelector: matchLabels: app: frontend ports: - port: 8080 protocol: TCP podSelector: matchLabels: app: ledgerwriter policyTypes: - Ingress - Egress EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/networkpolicies/networkpolicy_loadgenerator.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: loadgenerator spec: egress: - {} podSelector: matchLabels: app: loadgenerator policyTypes: - Egress EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/networkpolicies/networkpolicy_transactionhistory.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: transactionhistory spec: egress: - {} ingress: - from: - podSelector: matchLabels: app: frontend ports: - port: 8080 protocol: TCP podSelector: matchLabels: app: transactionhistory policyTypes: - Ingress - Egress EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/networkpolicies/networkpolicy_userservice.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: userservice spec: egress: - {} ingress: - from: - podSelector: matchLabels: app: frontend ports: - port: 8080 protocol: TCP podSelector: matchLabels: app: userservice policyTypes: - Ingress - Egress EOF Update the Kustomize base overlay:\ncd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/networkpolicies kustomize create --autodetect cd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base kustomize edit add resource networkpolicies Deploy Kubernetes manifests cd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Bank of Anthos NetworkPolicies\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the Bank of Anthos apps repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $BANKOFANTHOS_NAMESPACE Wait and re-run this command above until you see \"status\": \"SYNCED\".\nSee the Policy Controller Constraints without any violations in the GKE cluster, by running this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/policy_controller/dashboard?project=${TENANT_PROJECT_ID}\" List the GitHub runs for the Bank of Anthos apps repository:\ncd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME \u0026\u0026 gh run list Check the Bank of Anthos website Navigate to the Bank of Anthos website, click on the link displayed by the command below:\necho -e \"https://${BANK_OF_ANTHOS_INGRESS_GATEWAY_HOST_NAME}\" You should still have the Bank of Anthos website working successfully.\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "policies",
      "security-tips"
    ],
    "title": "Deploy NetworkPolicies",
    "uri": "/acm-workshop/bankofanthos/deploy-network-policies/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, you will see the Policy Controller violation regarding to the missing NetworkPolicies in the Whereami namespace. Then, you will fix this violation by deploying the associated resources.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh See the Policy Controller violations See the Policy Controller violations in the GKE cluster, by running this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/policy_controller/dashboard?project=${TENANT_PROJECT_ID}\" You will see that the K8sRequireNamespaceNetworkPolicies Constraint has this violation: Namespace \u003cwhereami\u003e does not have a NetworkPolicy.\nLet’s fix it!\nDefine a default deny-all NetworkPolicy Define a default deny-all NetworkPolicy:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$WHERE_AMI_DIR_NAME/base/networkpolicy_deny-all.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all spec: podSelector: {} policyTypes: - Ingress - Egress EOF Update the Kustomize base overlay:\ncd ${WORK_DIR}$WHERE_AMI_DIR_NAME/base kustomize edit add resource networkpolicy_deny-all.yaml Define NetworkPolicy for the Whereami app Define a fine granular NetworkPolicy:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$WHERE_AMI_DIR_NAME/base/networkpolicy_whereami.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: whereami spec: podSelector: matchLabels: app: whereami policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: ${INGRESS_GATEWAY_NAMESPACE} podSelector: matchLabels: app: ${INGRESS_GATEWAY_NAME} ports: - port: 8080 protocol: TCP egress: - {} EOF Update the Kustomize base overlay:\ncd ${WORK_DIR}$WHERE_AMI_DIR_NAME/base kustomize edit add resource networkpolicy_whereami.yaml Deploy Kubernetes manifests cd ${WORK_DIR}$WHERE_AMI_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Whereami NetworkPolicies\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the Whereami app repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $WHEREAMI_NAMESPACE Wait and re-run this command above until you see \"status\": \"SYNCED\".\nSee the Policy Controller Constraints without any violations in the GKE cluster, by running this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/policy_controller/dashboard?project=${TENANT_PROJECT_ID}\" List the GitHub runs for the Whereami app repository:\ncd ${WORK_DIR}$WHERE_AMI_DIR_NAME \u0026\u0026 gh run list Check the Whereami app Navigate to the Whereami app, click on the link displayed by the command below:\necho -e \"https://${WHERE_AMI_INGRESS_GATEWAY_HOST_NAME}\" You should still have the Whereami app working successfully.\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "policies",
      "security-tips"
    ],
    "title": "Deploy NetworkPolicies",
    "uri": "/acm-workshop/whereami/deploy-network-policies/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, in the Google Cloud Console you will monitor the resources synced by Config Sync for both the Config Controller instance in the Host project and the GKE cluster in the Tenant project.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Monitor the resources synced by Config Sync in the Config Controller instance in the Host project Click on the link displayed by the command below:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/dashboard?project=${HOST_PROJECT_ID}\" On the default Dashboard tab, you will find something similar to:\nThen if you go on the Packages tab, you will find something similar to:\nMonitor the resources synced by Config Sync in the GKE cluster in the Tenant project Click on the link displayed by the command below:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/dashboard?project=${TENANT_PROJECT_ID}\" On the default Dashboard tab, you will find something similar to:\nThen if you go on the Packages tab, you will find something similar to:\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "gitops-tips",
      "monitoring"
    ],
    "title": "Monitor resources synced",
    "uri": "/acm-workshop/monitoring-and-audit/monitor-resources-synced/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Platform Admin\nIn this section, you will set up the NetworkPolicies logging in order to get more insights about the logs generated by the denied or allowed requests controlled by NetworkPolicies thanks to the GKE Dataplane V2 feature.\nDefine variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Define Network Policy logging cat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/networkpolicies-logging.yaml kind: NetworkLogging apiVersion: networking.gke.io/v1alpha1 metadata: name: default spec: cluster: allow: log: false delegate: false deny: log: true delegate: false EOF Deploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"NetworkPolicies logging\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the GKE cluster configs repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name root-sync \\ --sync-namespace config-management-system Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list ",
    "description": "Duration: 5 min | Persona: Platform Admin",
    "tags": [
      "gke",
      "platform-admin",
      "security-tips"
    ],
    "title": "Set up NetworkPolicies logging",
    "uri": "/acm-workshop/gke-cluster/set-up-network-policies-logging/index.html"
  },
  {
    "content": "Set up Anthos Service Mesh (ASM) Ingress Gateway.\nCreate IP addressDuration: 5 min | Persona: Platform Admin\nAllow Cloud ArmorDuration: 2 min | Persona: Org Admin\nSet up Cloud ArmorDuration: 10 min | Persona: Platform Admin\nDeploy Ingress GatewayDuration: 15 min | Persona: Platform Admin\nDeploy NetworkPoliciesDuration: 5 min | Persona: Platform Admin\nDeploy AuthorizationPoliciesDuration: 5 min | Persona: Platform Admin\n",
    "description": "",
    "tags": null,
    "title": "7. Ingress Gateway",
    "uri": "/acm-workshop/ingress-gateway/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, you will see the Policy Controller violation regarding to the missing NetworkPolicies in the Online Boutique namespace. Then, you will fix this violation by deploying the associated resources.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh See the Policy Controller violations See the Policy Controller violations in the GKE cluster, by running this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/policy_controller/dashboard?project=${TENANT_PROJECT_ID}\" You will see that the K8sRequireNamespaceNetworkPolicies Constraint has this violation: Namespace \u003conlineboutique\u003e does not have a NetworkPolicy.\nLet’s fix it!\nUpdate RepoSync to deploy the Online Boutique’s Helm chart Define the RepoSync to deploy the Online Boutique’s Helm chart with the NetworkPolicies:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs/$ONLINEBOUTIQUE_NAMESPACE/repo-sync.yaml apiVersion: configsync.gke.io/v1beta1 kind: RepoSync metadata: name: repo-sync namespace: ${ONLINEBOUTIQUE_NAMESPACE} spec: sourceFormat: unstructured sourceType: helm helm: repo: oci://${CHART_REGISTRY_REPOSITORY} chart: ${ONLINEBOUTIQUE_NAMESPACE} version: ${ONLINE_BOUTIQUE_VERSION:1} releaseName: ${ONLINEBOUTIQUE_NAMESPACE} auth: gcpserviceaccount gcpServiceAccountEmail: ${HELM_CHARTS_READER_GSA}@${TENANT_PROJECT_ID}.iam.gserviceaccount.com values: cartDatabase: inClusterRedis: publicRepository: false images: repository: ${PRIVATE_ONLINE_BOUTIQUE_REGISTRY} tag: ${ONLINE_BOUTIQUE_VERSION} nativeGrpcHealthCheck: true seccompProfile: enable: true loadGenerator: checkFrontendInitContainer: false frontend: externalService: false virtualService: create: true gateway: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} labelKey: asm labelValue: ingressgateway hosts: - ${ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME} serviceAccounts: create: true authorizationPolicies: create: true networkPolicies: create: true EOF Info In order to deploy the fine granular NetworkPolicies, one per app, we just updated the list of values of the Online Boutique’s Helm chart previously configured, with networkPolicies.create: true.\nDeploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Online Boutique NetworkPolicies\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the Online Boutique apps repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $ONLINEBOUTIQUE_NAMESPACE Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list See the Policy Controller Constraints without any violations in the GKE cluster, by running this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/policy_controller/dashboard?project=${TENANT_PROJECT_ID}\" Check the Online Boutique website Navigate to the Online Boutique website, click on the link displayed by the command below:\necho -e \"https://${ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME}\" You should still have the Online Boutique website working successfully.\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "helm",
      "policies",
      "security-tips"
    ],
    "title": "Deploy NetworkPolicies",
    "uri": "/acm-workshop/onlineboutique/deploy-network-policies/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, you will deploy a fine granular Sidecar in order to optimize the resources (CPU/Memory) usage of the Whereami app’s sidecar proxy. By default, each application in the whereami Namespace can reach to all the endpoints in the mesh. The Sidecar resource allows to reduce that list to the strict minimum of which endpoints it needs to communicate with.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Define Sidecar cat \u003c\u003cEOF \u003e ${WORK_DIR}$WHERE_AMI_DIR_NAME/base/sidecar.yaml apiVersion: networking.istio.io/v1beta1 kind: Sidecar metadata: name: whereami spec: workloadSelector: labels: app: whereami egress: - hosts: - istio-system/* EOF Update the Kustomize base overlay:\ncd ${WORK_DIR}$WHERE_AMI_DIR_NAME/base kustomize edit add resource sidecar.yaml Deploy Kubernetes manifests cd ${WORK_DIR}$WHERE_AMI_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Whereami Sidecar\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the Whereami app repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $WHEREAMI_NAMESPACE Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the Whereami app repository:\ncd ${WORK_DIR}$WHERE_AMI_DIR_NAME \u0026\u0026 gh run list Check the Whereami app Navigate to the Whereami app, click on the link displayed by the command below:\necho -e \"https://${WHERE_AMI_INGRESS_GATEWAY_HOST_NAME}\" You should still have the Whereami app working successfully.\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "asm"
    ],
    "title": "Deploy Sidecar",
    "uri": "/acm-workshop/whereami/deploy-sidecar/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, you will deploy fine granular Sidecars in order to optimize the resources (CPU/Memory) usage of the Bank of Anthos apps’s sidecar proxies. By default, each application in the bankofanthos Namespace can reach to all the endpoints in the mesh. The Sidecar resource allows to reduce that list to the strict minimum of which endpoints it needs to communicate with.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Define Sidecars mkdir ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/sidecars cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/sidecars/sidecar_accounts-db.yaml apiVersion: networking.istio.io/v1beta1 kind: Sidecar metadata: name: accounts-db spec: egress: - hosts: - istio-system/* workloadSelector: labels: app: accounts-db EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/sidecars/sidecar_balancereader.yaml apiVersion: networking.istio.io/v1beta1 kind: Sidecar metadata: name: balancereader spec: egress: - hosts: - istio-system/* - ./ledger-db.${BANKOFANTHOS_NAMESPACE}.svc.cluster.local workloadSelector: labels: app: balancereader EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/sidecars/sidecar_contacts.yaml apiVersion: networking.istio.io/v1beta1 kind: Sidecar metadata: name: contacts spec: egress: - hosts: - istio-system/* - ./accounts-db.${BANKOFANTHOS_NAMESPACE}.svc.cluster.local workloadSelector: labels: app: contacts EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/sidecars/sidecar_frontend.yaml apiVersion: networking.istio.io/v1beta1 kind: Sidecar metadata: name: frontend spec: egress: - hosts: - istio-system/* - ./balancereader.${BANKOFANTHOS_NAMESPACE}.svc.cluster.local - ./contacts.${BANKOFANTHOS_NAMESPACE}.svc.cluster.local - ./ledgerwriter.${BANKOFANTHOS_NAMESPACE}.svc.cluster.local - ./transactionhistory.${BANKOFANTHOS_NAMESPACE}.svc.cluster.local - ./userservice.${BANKOFANTHOS_NAMESPACE}.svc.cluster.local workloadSelector: labels: app: frontend EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/sidecars/sidecar_ledger-db.yaml apiVersion: networking.istio.io/v1beta1 kind: Sidecar metadata: name: ledger-db spec: egress: - hosts: - istio-system/* workloadSelector: labels: app: ledger-db EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/sidecars/sidecar_ledgerwriter.yaml apiVersion: networking.istio.io/v1beta1 kind: Sidecar metadata: name: ledgerwriter spec: egress: - hosts: - istio-system/* - ./balancereader.${BANKOFANTHOS_NAMESPACE}.svc.cluster.local - ./ledger-db.${BANKOFANTHOS_NAMESPACE}.svc.cluster.local workloadSelector: labels: app: ledgerwriter EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/sidecars/sidecar_loadgenerator.yaml apiVersion: networking.istio.io/v1beta1 kind: Sidecar metadata: name: loadgenerator spec: egress: - hosts: - istio-system/* - ./frontend.${BANKOFANTHOS_NAMESPACE}.svc.cluster.local workloadSelector: labels: app: loadgenerator EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/sidecars/sidecar_transactionhistory.yaml apiVersion: networking.istio.io/v1beta1 kind: Sidecar metadata: name: transactionhistory spec: egress: - hosts: - istio-system/* - ./ledger-db.${BANKOFANTHOS_NAMESPACE}.svc.cluster.local workloadSelector: labels: app: transactionhistory EOF cat \u003c\u003cEOF \u003e ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/sidecars/sidecar_userservice.yaml apiVersion: networking.istio.io/v1beta1 kind: Sidecar metadata: name: userservice spec: egress: - hosts: - istio-system/* - ./accounts-db.${BANKOFANTHOS_NAMESPACE}.svc.cluster.local workloadSelector: labels: app: userservice EOF cd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base/sidecars kustomize create --autodetect Update the Kustomize base overlay cd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/base kustomize edit add resource sidecars Deploy Kubernetes manifests cd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Bank of Anthos Sidecars\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the Bank of Anthos apps repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $BANKOFANTHOS_NAMESPACE Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the Bank of Anthos apps repository:\ncd ${WORK_DIR}$BANK_OF_ANTHOS_DIR_NAME \u0026\u0026 gh run list Check the Bank of Anthos apps Navigate to the Bank of Anthos website, click on the link displayed by the command below:\necho -e \"https://${BANK_OF_ANTHOS_INGRESS_GATEWAY_HOST_NAME}\" You should still have the Bank of Anthos website working successfully.\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "asm",
      "apps-operator"
    ],
    "title": "Deploy Sidecars",
    "uri": "/acm-workshop/bankofanthos/deploy-sidecars/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, in the Google Cloud Console you will monitor the Policy Controller’s policies violations for the GKE cluster in the Tenant project.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Click on the link displayed by the command below:\necho -e \"https://console.cloud.google.com/kubernetes/policy_controller/dashboard?project=${TENANT_PROJECT_ID}\" On the default Dashboard tab, you will find something similar to:\nThen if you go on the Violations tab, you will find something similar to:\nAt the end of the workshop, we have fixed all the violations, but here below is an example of how a violation shows up:\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "apps-operator",
      "monitoring",
      "security-tips"
    ],
    "title": "Monitor policies violations",
    "uri": "/acm-workshop/monitoring-and-audit/monitor-policies-violations/index.html"
  },
  {
    "content": "Set up the Whereami app.\nSet up DNSDuration: 5 min | Persona: Platform Admin\nSet up URL uptime checkDuration: 10 min | Persona: Platform Admin\nConfigure Config SyncDuration: 10 min | Persona: Platform Admin\nPrepare containerDuration: 5 min | Persona: Apps Operator\nDeploy appDuration: 10 min | Persona: Apps Operator\nDeploy AuthorizationPolicyDuration: 5 min | Persona: Apps Operator\nDeploy NetworkPoliciesDuration: 5 min | Persona: Apps Operator\nDeploy SidecarDuration: 5 min | Persona: Apps Operator\n",
    "description": "",
    "tags": null,
    "title": "8. Whereami app",
    "uri": "/acm-workshop/whereami/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Apps Operator\nIn this section, you will deploy fine granular Sidecars in order to optimize the resources (CPU/Memory) usage of the Online Boutique apps’s sidecar proxies. By default, each application in the Online Boutique namespace can reach to all the endpoints in the mesh. The Sidecar resource allows to reduce that list to the strict minimum of which endpoints it needs to communicate with.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh Update RepoSync to deploy the Online Boutique’s Helm chart Define the RepoSync to deploy the Online Boutique’s Helm chart with the Sidecars:\ncat \u003c\u003cEOF \u003e ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/repo-syncs/$ONLINEBOUTIQUE_NAMESPACE/repo-sync.yaml apiVersion: configsync.gke.io/v1beta1 kind: RepoSync metadata: name: repo-sync namespace: ${ONLINEBOUTIQUE_NAMESPACE} spec: sourceFormat: unstructured sourceType: helm helm: repo: oci://${CHART_REGISTRY_REPOSITORY} chart: ${ONLINEBOUTIQUE_NAMESPACE} version: ${ONLINE_BOUTIQUE_VERSION:1} releaseName: ${ONLINEBOUTIQUE_NAMESPACE} auth: gcpserviceaccount gcpServiceAccountEmail: ${HELM_CHARTS_READER_GSA}@${TENANT_PROJECT_ID}.iam.gserviceaccount.com values: cartDatabase: inClusterRedis: publicRepository: false images: repository: ${PRIVATE_ONLINE_BOUTIQUE_REGISTRY} tag: ${ONLINE_BOUTIQUE_VERSION} nativeGrpcHealthCheck: true seccompProfile: enable: true loadGenerator: checkFrontendInitContainer: false frontend: externalService: false virtualService: create: true gateway: name: ${INGRESS_GATEWAY_NAME} namespace: ${INGRESS_GATEWAY_NAMESPACE} labelKey: asm labelValue: ingressgateway hosts: - ${ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME} serviceAccounts: create: true authorizationPolicies: create: true networkPolicies: create: true sidecars: create: true EOF Info In order to deploy the fine granular Sidecars, one per app, we just updated the list of values of the Online Boutique’s Helm chart previously configured, with sidecars.create: true.\nDeploy Kubernetes manifests cd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME/ git add . \u0026\u0026 git commit -m \"Online Boutique Sidecars\" \u0026\u0026 git push origin main Check deployments List the Kubernetes resources managed by Config Sync in GKE cluster for the Online Boutique apps repository: ​ UI UI gcloud gcloud Run this command and click on this link:\necho -e \"https://console.cloud.google.com/kubernetes/config_management/packages?project=${TENANT_PROJECT_ID}\" Wait until you see the Sync status column as Synced and the Reconcile status column as Current.\nRun this command:\ngcloud alpha anthos config sync repo describe \\ --project $TENANT_PROJECT_ID \\ --managed-resources all \\ --sync-name repo-sync \\ --sync-namespace $ONLINEBOUTIQUE_NAMESPACE Wait and re-run this command above until you see \"status\": \"SYNCED\".\nList the GitHub runs for the GKE cluster configs repository:\ncd ${WORK_DIR}$GKE_CONFIGS_DIR_NAME \u0026\u0026 gh run list Check the Online Boutique website Navigate to the Online Boutique website, click on the link displayed by the command below:\necho -e \"https://${ONLINE_BOUTIQUE_INGRESS_GATEWAY_HOST_NAME}\" You should still have the Online Boutique website working successfully.\n",
    "description": "Duration: 5 min | Persona: Apps Operator",
    "tags": [
      "asm",
      "apps-operator",
      "helm"
    ],
    "title": "Deploy Sidecars",
    "uri": "/acm-workshop/onlineboutique/deploy-sidecars/index.html"
  },
  {
    "content": " Duration: 5 min | Persona: Platform Admin\nIn this section, you will monitor the uptime checks defined earlier in this workshop.\nInitialize variables:\nWORK_DIR=~/ source ${WORK_DIR}acm-workshop-variables.sh In the Google Cloud console, navigate to Monitoring \u003e Uptime checks service. Click on the link displayed by the command below:\necho -e \"https://console.cloud.google.com/monitoring/uptime?project=${TENANT_PROJECT_ID}\" Then, you could select one of the uptime checks config to get more insights:\nWith the email notification on the uptime checks alerting we set earlier in this workshop, if there is any alert you will receive an email similar to this:\n",
    "description": "Duration: 5 min | Persona: Platform Admin",
    "tags": [
      "monitoring",
      "platform-admin"
    ],
    "title": "Monitor uptime checks",
    "uri": "/acm-workshop/monitoring-and-audit/monitor-uptime-checks/index.html"
  },
  {
    "content": "Set up the Online Boutique apps.\nSet up DNSDuration: 5 min | Persona: Platform Admin\nSet up URL uptime checkDuration: 10 min | Persona: Platform Admin\nAllow Config SyncDuration: 5 min | Persona: Platform Admin\nConfigure Config SyncDuration: 5 min | Persona: Platform Admin\nPrepare containers and chartDuration: 5 min | Persona: Apps Operator\nDeploy appsDuration: 5 min | Persona: Apps Operator\nDeploy AuthorizationPoliciesDuration: 5 min | Persona: Apps Operator\nDeploy NetworkPoliciesDuration: 5 min | Persona: Apps Operator\nDeploy SidecarsDuration: 5 min | Persona: Apps Operator\nMemorystoreUse Memorystore (Redis)\nAllow MemorystoreDuration: 5 min | Persona: Org Admin\nEnforce Memorystore policiesDuration: 5 min | Persona: Org Admin\nCreate MemorystoreDuration: 10 min | Persona: Platform Admin\nUse MemorystoreDuration: 10 min | Persona: Apps Operator\nSecure Memorystore accessDuration: 10 min | Persona: Apps Operator\nSpannerUse Spanner\nAllow SpannerDuration: 5 min | Persona: Org Admin\nCreate SpannerDuration: 10 min | Persona: Platform Admin\nUse SpannerDuration: 10 min | Persona: Apps Operator\n",
    "description": "",
    "tags": null,
    "title": "9. Online Boutique apps",
    "uri": "/acm-workshop/onlineboutique/index.html"
  },
  {
    "content": "Use Memorystore (Redis).\nAllow MemorystoreDuration: 5 min | Persona: Org Admin\nEnforce Memorystore policiesDuration: 5 min | Persona: Org Admin\nCreate MemorystoreDuration: 10 min | Persona: Platform Admin\nUse MemorystoreDuration: 10 min | Persona: Apps Operator\nSecure Memorystore accessDuration: 10 min | Persona: Apps Operator\n",
    "description": "Use Memorystore (Redis)",
    "tags": null,
    "title": "Memorystore",
    "uri": "/acm-workshop/onlineboutique/memorystore/index.html"
  },
  {
    "content": "Set up the Bank of Anthos apps.\nSet up DNSDuration: 5 min | Persona: Platform Admin\nSet up URL uptime checkDuration: 10 min | Persona: Platform Admin\nConfigure Config SyncDuration: 10 min | Persona: Platform Admin\nPrepare containersDuration: 5 min | Persona: Apps Operator\nDeploy appsDuration: 5 min | Persona: Apps Operator\nDeploy AuthorizationPoliciesDuration: 5 min | Persona: Apps Operator\nDeploy NetworkPoliciesDuration: 5 min | Persona: Apps Operator\nDeploy SidecarsDuration: 5 min | Persona: Apps Operator\n",
    "description": "",
    "tags": null,
    "title": "10. Bank of Anthos apps",
    "uri": "/acm-workshop/bankofanthos/index.html"
  },
  {
    "content": "Use Spanner.\nAllow SpannerDuration: 5 min | Persona: Org Admin\nCreate SpannerDuration: 10 min | Persona: Platform Admin\nUse SpannerDuration: 10 min | Persona: Apps Operator\n",
    "description": "Use Spanner",
    "tags": null,
    "title": "Spanner",
    "uri": "/acm-workshop/onlineboutique/spanner/index.html"
  },
  {
    "content": "Leverage the different Monitoring and Audit features in the Tenant project.\nVerify ASM versionDuration: 5 min | Persona: Platform Admin\nMonitor apps securityDuration: 5 min | Persona: Apps Operator\nMonitor apps healthDuration: 5 min | Persona: Apps Operator\nTrace appsDuration: 5 min | Persona: Apps Operator\nMonitor WAF rulesDuration: 5 min | Persona: Platform Admin\nScan workloadsDuration: 5 min | Persona: Apps Operator\nMonitor resources syncedDuration: 5 min | Persona: Apps Operator\nMonitor policies violationsDuration: 5 min | Persona: Apps Operator\nMonitor uptime checksDuration: 5 min | Persona: Platform Admin\n",
    "description": "",
    "tags": null,
    "title": "11. Monitoring \u0026 Audit",
    "uri": "/acm-workshop/monitoring-and-audit/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tag :: apps-operator",
    "uri": "/acm-workshop/tags/apps-operator/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tag :: asm",
    "uri": "/acm-workshop/tags/asm/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/acm-workshop/categories/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tag :: gitops-tips",
    "uri": "/acm-workshop/tags/gitops-tips/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tag :: gke",
    "uri": "/acm-workshop/tags/gke/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tag :: helm",
    "uri": "/acm-workshop/tags/helm/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tag :: kcc",
    "uri": "/acm-workshop/tags/kcc/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tag :: monitoring",
    "uri": "/acm-workshop/tags/monitoring/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tag :: org-admin",
    "uri": "/acm-workshop/tags/org-admin/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tag :: platform-admin",
    "uri": "/acm-workshop/tags/platform-admin/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tag :: policies",
    "uri": "/acm-workshop/tags/policies/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tag :: security-tips",
    "uri": "/acm-workshop/tags/security-tips/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/acm-workshop/tags/index.html"
  }
]
